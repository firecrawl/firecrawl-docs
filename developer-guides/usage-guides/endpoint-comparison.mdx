---
title: "Choosing the Data Extractor"
description: "Compare /agent, /extract, and /scrape (JSON mode) to pick the right tool for structured data extraction"
og:title: "Choosing the Data Extractor | Firecrawl"
og:description: "Compare /agent, /extract, and /scrape (JSON mode) to pick the right tool for structured data extraction"
sidebarTitle: "Choosing the Data Extractor"
---

import AgentWithSchemaPython from "/snippets/v2/agent/with-schema/python.mdx";
import AgentWithSchemaJS from "/snippets/v2/agent/with-schema/js.mdx";
import AgentWithSchemaCURL from "/snippets/v2/agent/with-schema/curl.mdx";
import ExtractPython from "/snippets/v2/extract/base/python.mdx";
import ExtractNode from "/snippets/v2/extract/base/js.mdx";
import ExtractCURL from "/snippets/v2/extract/base/curl.mdx";
import ScrapeJsonPython from "/snippets/v2/scrape/json/base/python.mdx";
import ScrapeJsonNode from "/snippets/v2/scrape/json/base/js.mdx";
import ScrapeJsonCURL from "/snippets/v2/scrape/json/base/curl.mdx";

Firecrawl offers three approaches for extracting structured data from web pages. Each serves different use cases with varying levels of automation and control.

## Quick Comparison

| Feature | `/agent` | `/extract` | `/scrape` (JSON mode) |
|---------|----------|------------|----------------------|
| **Status** | Active | Use `/agent` instead | Active |
| **URL Required** | No (optional) | Yes (wildcards supported) | Yes (single URL) |
| **Scope** | Web-wide discovery | Multiple pages/domains | Single page |
| **URL Discovery** | Autonomous web search | Crawls from given URLs | None |
| **Processing** | Asynchronous | Asynchronous | Synchronous |
| **Schema Required** | No (prompt or schema) | No (prompt or schema) | No (prompt or schema) |
| **Best For** | Research, discovery, complex gathering | Multi-page extraction (when you know URLs) | Known single-page extraction |

## 1. `/agent` Endpoint (Recommended)

The `/agent` endpoint is Firecrawl's most advanced offering—the successor to `/extract`. It uses AI agents to autonomously search, navigate, and gather data from across the web.

<Info>
**Research Preview**: Agent is in early access. It will continue to improve significantly over time. [Share feedback →](mailto:product@firecrawl.com)
</Info>

### Key Characteristics

- **URLs Optional**: Just describe what you need via `prompt`; URLs are completely optional
- **Autonomous Navigation**: The agent searches and navigates deep into sites to find your data
- **Deep Web Search**: Autonomously discovers information across multiple domains and pages
- **Parallel Processing**: Processes multiple sources simultaneously for faster results
- **Models Available**: `spark-1-mini` (default, 60% cheaper) and `spark-1-pro` (higher accuracy)

### Example

<CodeGroup>

<AgentWithSchemaPython />
<AgentWithSchemaJS />
<AgentWithSchemaCURL />

</CodeGroup>

### Best Use Case: Autonomous Research & Discovery

**Scenario**: You need to find information about AI startups that raised Series A funding, including their founders and funding amounts.

**Why `/agent`**: You don't know which websites contain this information. The agent will autonomously search the web, navigate to relevant sources (Crunchbase, news sites, company pages), and compile the structured data for you.

For more details, see the [Agent documentation](/features/agent).

---

## 2. `/extract` Endpoint

<Note>
**Use `/agent` instead**: We recommend migrating to [`/agent`](/features/agent)—it's faster, more reliable, doesn't require URLs, and handles all `/extract` use cases plus more.
</Note>

The `/extract` endpoint collects structured data from specified URLs or entire domains using LLM-powered extraction.

### Key Characteristics

- **URLs Typically Required**: Provide at least one URL (supports wildcards like `example.com/*`)
- **Domain Crawling**: Can crawl and parse all URLs discovered in a domain
- **Web Search Enhancement**: Optional `enableWebSearch` to follow links outside specified domains
- **Schema Optional**: Supports strict JSON schema OR natural language prompts
- **Async Processing**: Returns job ID for status checking

<Tip>
`/extract` now supports extracting without URLs—just provide a prompt. However, for this use case, `/agent` delivers better results.
</Tip>

### The URL Limitation

The fundamental challenge with `/extract` is that you typically need to know URLs upfront:

1. **Discovery gap**: For tasks like "find YC W24 companies," you don't know which URLs contain the data. You'd need a separate search step before calling `/extract`.
2. **Awkward web search**: While `enableWebSearch` exists, it's constrained to start from URLs you provide—an awkward workflow for discovery tasks.
3. **Why `/agent` was created**: `/extract` is good at extracting from known locations, but less effective at discovering where data lives.

### Example

<CodeGroup>

<ExtractPython />
<ExtractNode />
<ExtractCURL />

</CodeGroup>

### Best Use Case: Targeted Multi-Page Extraction

**Scenario**: You have your competitor's documentation URL and want to extract all their API endpoints from `docs.competitor.com/*`.

**Why `/extract` worked here**: You knew the exact domain. But even then, `/agent` with URLs provided would typically give better results today.

For more details, see the [Extract documentation](/features/extract).

---

## 3. `/scrape` Endpoint with JSON Mode

The `/scrape` endpoint with JSON mode is the most controlled approach—it extracts structured data from a single known URL using an LLM to parse the page content into your specified schema.

### Key Characteristics

- **Single URL Only**: Designed for extracting data from one specific page at a time
- **Exact URL Required**: You must know the precise URL containing the data
- **Schema Optional**: Can use JSON schema OR just a prompt (LLM chooses structure)
- **Synchronous**: Returns data immediately (no job polling needed)
- **Additional Formats**: Can combine JSON extraction with markdown, HTML, screenshots in one request

### Example

<CodeGroup>

<ScrapeJsonPython />
<ScrapeJsonNode />
<ScrapeJsonCURL />

</CodeGroup>

### Best Use Case: Single-Page Precision Extraction

**Scenario**: You're building a price monitoring tool and need to extract the price, stock status, and product details from a specific product page you already have the URL for.

**Why `/scrape` with JSON mode**: You know exactly which page contains the data, need precise single-page extraction, and want synchronous results without job management overhead.

For more details, see the [JSON mode documentation](/features/llm-extract).

---

## Decision Guide

Use this flowchart to choose the right endpoint:

```
Do you know the exact URL(s) containing your data?
│
├─ NO → Use /agent (autonomous web discovery)
│
└─ YES
    │
    ├─ Single page? → Use /scrape with JSON mode
    │
    └─ Multiple pages? → Use /agent with URLs
                         (or /scrape with batch processing)
```

### Recommendations by Scenario

| Scenario | Recommended Endpoint |
|----------|---------------------|
| "Find all AI startups and their funding" | `/agent` |
| "Extract data from this specific product page" | `/scrape` (JSON mode) |
| "Get all blog posts from competitor.com" | `/agent` with URL |
| "Monitor prices across multiple known URLs" | `/scrape` with batch processing |
| "Research companies in a specific industry" | `/agent` |
| "Extract contact info from 50 known company pages" | `/scrape` with batch processing |

---

## Migration: `/extract` → `/agent`

If you're currently using `/extract`, migration is straightforward:

**Before (extract):**

```python
result = app.extract(
    urls=["https://example.com/*"],
    prompt="Extract product information",
    schema=schema
)
```

**After (agent):**

```python
result = app.agent(
    urls=["https://example.com"],  # Optional - can omit entirely
    prompt="Extract product information from example.com",
    schema=schema,
    model="spark-1-mini"  # or "spark-1-pro" for higher accuracy
)
```

The key advantage: with `/agent`, you can drop the URLs entirely and just describe what you need.

---

## Key Takeaways

1. **Default to `/agent`** for most extraction tasks—it's the newest, most capable, and handles both known and unknown URL scenarios.

2. **Use `/scrape` with JSON mode** when you have a specific URL and need fast, synchronous, single-page extraction.

3. **Migrate from `/extract`** to `/agent` for new projects—`/agent` is the successor with better capabilities.

4. **Consider costs**: `/agent` uses dynamic credit billing based on complexity; `/scrape` has more predictable per-page pricing. See [Agent pricing](/features/agent#pricing) for details.

---

## Further Reading

- [Agent documentation](/features/agent)
- [Agent models](/features/models)
- [JSON mode documentation](/features/llm-extract)
- [Extract documentation](/features/extract)
- [Batch scraping](/features/batch-scrape)
