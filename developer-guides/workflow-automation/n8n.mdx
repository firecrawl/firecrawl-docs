---
title: "Firecrawl + n8n"
description: "Official n8n node with 20+ operations for web scraping, crawling, and data extraction"
---

<Note>
**Official n8n Integration:** [n8n.io/integrations/firecrawl](https://n8n.io/integrations/firecrawl/)

Community-built node with 400+ integrations • Self-hosted & cloud options • Visual workflow builder • 20+ Firecrawl operations
</Note>

## n8n Integration Overview

n8n is an open-source workflow automation platform that lets you connect Firecrawl with 400+ other tools. The official Firecrawl node includes **20+ operations** covering everything from basic scraping to advanced analytics.

<CardGroup cols={2}>
  <Card title="Explore n8n" icon="compass" href="https://n8n.io/?utm_source=firecrawl&utm_medium=docs">
    Join thousands using the fastest-growing open-source automation platform to build powerful Firecrawl workflows
  </Card>

  <Card title="Learn n8n" icon="book" href="https://docs.n8n.io/?utm_source=firecrawl&utm_medium=docs">
    Complete guides and tutorials on building workflows, using nodes, and deploying n8n
  </Card>
</CardGroup>

## Official Blog Posts

<CardGroup cols={2}>
  <Card title="Getting Started with n8n" icon="rocket" href="https://www.firecrawl.dev/blog/firecrawl-n8n-web-automation">
    Complete setup guide for n8n Cloud & self-hosted. Build an AI chatbot with live web data.
  </Card>

  <Card title="8 Workflow Templates" icon="layer-group" href="https://www.firecrawl.dev/blog/n8n-web-scraping-workflow-templates">
    Production-ready templates for competitor monitoring, lead generation, and more.
  </Card>
</CardGroup>

## Getting Started

<Steps>
  <Step title="Install Firecrawl Node">
    **n8n Cloud:** Search for "Firecrawl" in the node browser and add to your workflow

    **Self-Hosted:** The Firecrawl node is included in n8n by default (v1.0+)
  </Step>

  <Step title="Get Firecrawl API Key">
    Visit [Firecrawl API Keys](https://www.firecrawl.dev/app/api-keys) and create a new API key
  </Step>

  <Step title="Add Credentials">
    In your n8n workflow:
    1. Add a Firecrawl node
    2. Click "Create New Credentials"
    3. Enter your API key
    4. (Optional) Add custom base URL for self-hosted Firecrawl
  </Step>

  <Step title="Select Operation">
    Choose from 20+ operations organized by category:
    - **Core Operations:** Scrape, Crawl, Map, Search, Extract, Batch Scrape
    - **Status & Management:** Get status, cancel jobs, list active crawls
    - **Analytics:** Monitor credit usage, token consumption, and queue status
  </Step>

  <Step title="Configure & Execute">
    Fill in the required parameters and test your node
  </Step>
</Steps>

## Core Operations

Core operations for extracting and processing web data.

<Tabs>
  <Tab title="Scrape a url and get its content">

Convert a single URL into clean, structured data in multiple formats.

**Use Cases:**
- Extract article content from news sites
- Scrape product details from e-commerce pages
- Get structured data with AI-powered JSON extraction
- Capture full-page screenshots

**Parameters:**

**Url** (required)
The URL to scrape

**Parsers**
Controls how PDF files are processed. When PDF parser is enabled, content is extracted and converted to markdown (1 credit per page). When disabled, PDF is returned in base64 encoding (1 credit total).

**Scrape Options:**

- **Formats** - Output format(s): Markdown, HTML, Raw HTML, JSON, Links, Screenshot, Summary, Change Tracking
- **Only Main Content** - Only return the main content excluding headers, navs, footers, etc. (default: true)
- **Include Tags** - Specific HTML tags to include in the output
- **Exclude Tags** - Specific HTML tags to exclude from the output
- **Headers** - Custom headers to send with the request
- **Wait For (Ms)** - Milliseconds to wait for the page to load before fetching content
- **Mobile** - Emulate scraping from a mobile device
- **Skip TLS Verification** - Skip TLS certificate verification when making requests
- **Timeout (Ms)** - Timeout in milliseconds for the request (default: 30000)
- **Actions** - Interact with page before scraping (click, wait, scroll, type, screenshot, etc.)
- **Location** - Set country and language preferences for the request
- **Remove Base64 Images** - Remove base64 encoded images from the output (default: true)
- **Block Ads** - Enable ad-blocking and cookie popup blocking (default: true)
- **Store In Cache** - Store the page in Firecrawl cache. Disable for data protection concerns (default: true)
- **Proxy** - Type of proxy to use: Basic or Stealth (default: Basic)

**Response Schema**

<Note>
The response structure varies based on your selected formats and options. Only requested formats and enabled features will appear in the response.
</Note>

```json
{
  "success": true,
  "data": {
    "markdown": "# Page Content...",        // When 'Markdown' format is selected
    "html": "<html>...</html>",             // When 'HTML' format is selected
    "rawHtml": "<!DOCTYPE html>...",        // When 'Raw HTML' format is selected
    "metadata": {                           // Always included
      "title": "Page Title",
      "description": "Meta description",
      "language": "en",
      "sourceURL": "https://example.com",
      "statusCode": 200
    },
    "links": ["https://example.com/page1", ...],  // When 'Links' format is selected
    "screenshot": "base64_encoded_image...",      // When 'Screenshot' format is selected
    "actions": {                                  // When 'Actions' are configured
      "screenshots": ["base64_encoded_image..."]
    }
  }
}
```

**Format-Specific Fields:**
- **JSON format**: Returns structured data based on your schema in a `json` field
- **Screenshot format**: Returns base64 image in `screenshot` field
- **Change Tracking format**: Returns git-diff style changes or JSON diff in `changeTracking` field
- **Summary format**: Returns AI-generated page summary in `summary` field

**API Reference:** [Scrape Endpoint](/api-reference/endpoint/scrape)

---

  </Tab>

  <Tab title="Crawl a website">

Recursively crawl websites to gather content from multiple pages.

**Use Cases:**
- Full site content extraction
- Documentation scraping
- Multi-page data collection with filtering
- Automated site archiving

**Parameters:**

**Url** (required)
Starting URL for the crawl

**Prompt**
Natural language instruction to guide the LLM-powered crawl

**Limit**
Maximum number of pages to crawl (default: 500)

**Delay**
Delay between requests in milliseconds for rate limiting (default: 0)

**Max Concurrency**
Maximum number of concurrent scrapes. If not specified, adheres to your team's concurrency limit (default: 100)

**Exclude Paths**
URL patterns to exclude from the crawl (e.g., `blog/*` will exclude paths like `/blog/article-1`)

**Include Paths**
URL patterns to include in the crawl (e.g., `blog/*` will only include paths like `/blog/article-1`)

**Crawl Options:**
- **Ignore Sitemap** - Whether to ignore the website sitemap when crawling
- **Ignore Query Params** - Whether to ignore query parameters (not re-scrape the same path with different or no parameters)
- **Allow External Links** - Whether to allow the crawler to follow links to external websites
- **Allow Subdomains** - Whether to allow the crawler to follow links to subdomains of the main domain

**Scrape Options:**
All scrape options from the Scrape operation are available (Formats, Only Main Content, Actions, etc.)

**Response Schema**

<Note>
Crawl operations return immediately with a job ID. Use "Get crawl status" to retrieve results as the crawl progresses.
</Note>

```json
{
  "success": true,
  "id": "crawl_1234567890abcdef",    // Crawl job ID for status checks
  "url": "https://api.firecrawl.dev/v2/crawl/crawl_1234567890abcdef"
}
```

**Status Check Response** (use "Get crawl status" operation):
```json
{
  "success": true,
  "status": "completed",              // Status: scraping, completed, failed, cancelled
  "total": 48,                        // Total pages discovered
  "completed": 48,                    // Pages successfully scraped
  "creditsUsed": 48,
  "expiresAt": "2024-01-20T12:00:00.000Z",
  "next": "https://api.firecrawl.dev/v2/crawl/crawl_123?skip=10",  // Pagination URL if applicable
  "data": [                           // Array of scraped pages
    {
      "markdown": "# Page content...",  // Format based on Scrape Options
      "metadata": { ... }                // Always included
    }
  ]
}
```

**API Reference:** [Crawl Endpoint](/api-reference/endpoint/crawl-post)

  </Tab>

  <Tab title="Map a website and get urls">

Discover all URLs on a website without scraping content.

**Use Cases:**
- Site structure analysis
- SEO auditing and URL discovery
- URL collection for batch scraping
- Sitemap generation

**Parameters:**

**Url** (required)
Website URL to map

**Sitemap**
How to handle the website sitemap during crawling:
- **Include** - Use sitemap + crawl (default)
- **Only** - Only return links found in the website sitemap
- **Skip** - Ignore the website sitemap when crawling

**Include Subdomains**
Whether to include subdomains of the website (default: false)

**Limit**
Maximum number of URLs to return (default: 5000)

**Timeout (Ms)**
Timeout in milliseconds for the request (default: 10000)

**Response Schema**

<Note>
Map operations return immediately with all discovered URLs. No status check needed.
</Note>

```json
{
  "success": true,
  "links": [                          // Array of all discovered URLs
    "https://example.com",
    "https://example.com/about",
    "https://example.com/products",
    "https://example.com/contact"
  ]
}
```

The response is a flat array of discovered URLs, perfect for:
- Feeding into batch scrape operations
- Analyzing site structure
- Building custom sitemaps
- URL filtering and categorization

**API Reference:** [Map Endpoint](/api-reference/endpoint/map)

---

  </Tab>

  <Tab title="Search and optionally scrape search results">

Search the web and get structured results, with optional content scraping.

**Use Cases:**
- Market research and trend monitoring
- Competitor analysis
- News aggregation
- Real-time web data collection

**Parameters:**

**Query** (required)
The search query string

**Sources**
Search sources to use. At least one source must be selected (default: Web):
- **Web** - Traditional web search results
- **Images** - Image search results with URLs
- **News** - Recent news articles

**Timeout (Ms)**
Timeout in milliseconds for the request (default: 60000)

**Limit**
Maximum number of results to return (default: 5)

**Time Based Search**
Time-based search parameter for filtering results by date

**Response Schema**

<Note>
Search operations return immediately with results. The response structure depends on selected sources (Web, Images, News).
</Note>

```json
{
  "success": true,
  "data": [
    {
      "url": "https://example.com/article",       // Always included
      "title": "Article Title",                   // Always included
      "description": "Article description...",    // Always included
      "markdown": "# Full content..."             // When scraping is enabled
    }
  ]
}
```

**Time-Based Search Examples:**
- `qdr:h` - Past hour
- `qdr:d` - Past 24 hours
- `qdr:w` - Past week
- `qdr:m` - Past month
- `qdr:y` - Past year

**API Reference:** [Search Endpoint](/api-reference/endpoint/search)

---

  </Tab>

  <Tab title="Extract Data">

AI-powered structured data extraction from multiple URLs using prompts and schemas.

**Use Cases:**
- Extract product catalogs with pricing
- Collect contact information from company pages
- Build structured datasets from unstructured web pages
- Create knowledge bases from documentation sites

**Parameters:**

**URLs** (required)
Array of URLs to extract data from. Supports glob patterns (e.g., `https://example.com/*`)

**Prompt**
Natural language instruction to guide the extraction process

**Schema**
JSON schema defining the structure of the extracted data

**Ignore Sitemap**
Whether to ignore the website sitemap when crawling (default: true)

**Include Subdomains**
Whether to include subdomains of the website (default: false)

**Enable Web Search**
Whether to enable web search to find additional data (default: false)

**Show Sources**
Whether to show the sources used to extract the data (default: false)

**Scrape Options:**
All scrape options from the Scrape operation are available (Formats, Only Main Content, Actions, etc.)

**Example Schema:**
```json
{
  "name": "string",
  "price": "number",
  "inStock": "boolean",
  "features": ["string"]
}
```

**Response Schema**

<Note>
Extract operations return immediately with a job ID. Use "Get Extract Status" to retrieve the extracted data when processing is complete.
</Note>

```json
{
  "success": true,
  "id": "extract_1234567890abcdef",    // Extract job ID for status checks
  "url": "https://api.firecrawl.dev/v2/extract/extract_1234567890abcdef"
}
```

**Status Check Response** (use "Get Extract Status" operation):
```json
{
  "success": true,
  "status": "completed",                // Status: processing, completed, failed
  "data": {
    "name": "Product Name",             // Fields match your schema
    "price": 29.99,
    "inStock": true,
    "features": ["Feature 1", "Feature 2"],
    "sources": ["https://example.com/product1"]  // When Show Sources is enabled
  }
}
```

The extracted data conforms to your schema. Sources are included when "Show Sources" option is enabled.

**API Reference:** [Extract Endpoint](/api-reference/endpoint/extract)

  </Tab>

  <Tab title="Batch scrape multiple URLs">

Efficiently scrape multiple URLs in parallel with webhook notifications.

**Use Cases:**
- Scrape product listings at scale
- Monitor competitor pricing across multiple pages
- Extract data from URL lists
- Process large datasets with webhooks

**Parameters:**

**URLs** (required)
Array of URLs to scrape in batch

**Parsers**
Controls how PDF files are processed. When PDF parser is enabled, content is extracted and converted to markdown (1 credit per page). When disabled, PDF is returned in base64 encoding (1 credit total).

**Scrape Options:**

Standard options:
- **Formats** - Output format(s): Markdown, HTML, Raw HTML, JSON, Links, Screenshot, Summary, Change Tracking
- **Only Main Content** - Only return the main content excluding headers, navs, footers, etc. (default: true)
- **Include Tags** / **Exclude Tags** - Filter specific HTML tags
- **Headers** - Custom headers to send with the request
- **Wait For (Ms)** - Milliseconds to wait for the page to load
- **Mobile** - Emulate scraping from a mobile device
- **Skip TLS Verification** - Skip TLS certificate verification
- **Timeout (Ms)** - Request timeout (default: 30000)
- **Actions** - Interact with page before scraping
- **Location** - Set country and language preferences
- **Remove Base64 Images** - Remove base64 images from output (default: true)
- **Block Ads** - Enable ad-blocking and cookie popup blocking (default: true)
- **Store In Cache** - Store page in Firecrawl cache (default: true)
- **Proxy** - Proxy type: Basic or Stealth (default: Basic)

Batch-specific options:
- **Webhook** - Webhook settings for real-time notifications
  - **URL** - Webhook endpoint to receive notifications
  - **Headers** - Custom headers to send to webhook
  - **Metadata** - Custom metadata included in webhook payloads
  - **Events** - Events to trigger webhook: Started, Page, Completed, Failed
- **Max Concurrency** - Maximum concurrent scrapes. Adheres to team limit if not specified (default: 100)
- **Ignore Invalid URLs** - Skip invalid URLs instead of failing the entire request (default: true)
- **Zero Data Retention** - Ensure no data is retained on server after completion (default: false)

**Response Schema**

<Note>
Batch scrape operations return immediately with a job ID. Use "Get batch scrape status" or configure webhooks for real-time updates.
</Note>

```json
{
  "success": true,
  "id": "batch_1234567890abcdef",    // Batch job ID for status checks
  "url": "https://api.firecrawl.dev/v2/batch/scrape/batch_1234567890abcdef"
}
```

**Status Check Response** (use "Get batch scrape status" operation):
```json
{
  "success": true,
  "status": "completed",              // Status: scraping, completed, failed
  "total": 100,                       // Total URLs to scrape
  "completed": 98,                    // Successfully scraped
  "failed": 2,                        // Failed URLs
  "creditsUsed": 98,
  "expiresAt": "2024-01-20T12:00:00.000Z",
  "data": [                           // Array of scraped results
    {
      "markdown": "# Page content...",  // Format based on Scrape Options
      "metadata": { ... }                // Always included
    }
  ]
}
```

**Webhook Payload Example** (when webhook is configured):
```json
{
  "event": "batchScrape.page",        // Event type: started, page, completed, failed
  "batchId": "batch_123",
  "data": {
    "markdown": "...",                // Format based on Scrape Options
    "metadata": { ... }
  },
  "metadata": { "custom": "data" }   // Your custom metadata
}
```

**API Reference:** [Batch Scrape Endpoint](/api-reference/endpoint/batch-scrape)

  </Tab>
</Tabs>

## Status & Management Operations

Monitor and control your scraping jobs.

<Tabs>
  <Tab title="Get crawl status">

Check the progress and retrieve results of a crawl job.

**Parameters:**

**Crawl ID** (required)
ID of the crawl job to get status for

**Returns:** Status, progress, completed pages, and data (paginated)

**Response Schema**

```json
{
  "success": true,
  "status": "completed",
  "total": 48,
  "completed": 48,
  "creditsUsed": 48,
  "expiresAt": "2024-01-20T12:00:00.000Z",
  "next": "https://api.firecrawl.dev/v2/crawl/crawl_123?skip=10",
  "data": [...]
}
```

**API Reference:** [Get Crawl Status](/api-reference/endpoint/crawl-get)

  </Tab>

  <Tab title="Get batch scrape status">

Check the progress and retrieve results of a batch scrape job.

**Parameters:**

**Batch ID** (required)
ID of the batch scrape job

**Returns:** Status, progress, success/failure counts, and data

**Response Schema**

```json
{
  "success": true,
  "status": "scraping",
  "total": 100,
  "completed": 75,
  "failed": 2,
  "creditsUsed": 75,
  "expiresAt": "2024-01-20T12:00:00.000Z",
  "data": [...]
}
```

**API Reference:** [Get Batch Scrape Status](/api-reference/endpoint/batch-scrape-get)

  </Tab>

  <Tab title="Get Extract Status">

Check the progress and retrieve results of an extract job.

**Parameters:**

**Extract ID** (required)
ID of the extract job to get status for

**Returns:** Status and extracted structured data

**Response Schema**

```json
{
  "success": true,
  "status": "completed",
  "data": {
    "field1": "value1",
    "field2": 123
  }
}
```

**API Reference:** [Get Extract Status](/api-reference/endpoint/extract-get)

  </Tab>

  <Tab title="Get crawl errors">

Retrieve error details for failed pages in a crawl.

**Parameters:**

**Crawl ID** (required)
ID of the crawl job

**Returns:** Array of errors with URLs and error messages

**Response Schema**

```json
{
  "success": true,
  "errors": [
    {
      "url": "https://example.com/page",
      "error": "Timeout after 30000ms"
    }
  ]
}
```

**API Reference:** [Get Crawl Errors](/api-reference/endpoint/crawl-get-errors)

  </Tab>

  <Tab title="Get batch scrape errors">

Retrieve error details for failed URLs in a batch scrape.

**Parameters:**

**Batch ID** (required)
ID of the batch scrape job

**Returns:** Array of errors with URLs and error messages

**API Reference:** [Get Batch Scrape Errors](/api-reference/endpoint/batch-scrape-get-errors)

  </Tab>

  <Tab title="List active crawls">

Get a list of all currently running crawl jobs for your account.

**Parameters:** None required

**Returns:** Array of active crawl IDs and their status

**Response Schema**

```json
{
  "success": true,
  "data": [
    {
      "id": "crawl_123",
      "url": "https://example.com",
      "status": "scraping",
      "completed": 15,
      "total": 50
    }
  ]
}
```

**API Reference:** [List Active Crawls](/api-reference/endpoint/crawl-active)

  </Tab>

  <Tab title="Cancel a crawl job">

Stop a running crawl job.

**Parameters:**

**Crawl ID** (required)
ID of the crawl job to cancel

**Returns:** Confirmation of cancellation

**Response Schema**

```json
{
  "success": true,
  "status": "cancelled"
}
```

**API Reference:** [Cancel Crawl Job](/api-reference/endpoint/crawl-delete)

  </Tab>

  <Tab title="Cancel batch scrape job">

Stop a running batch scrape job.

**Parameters:**

**Batch ID** (required)
ID of the batch scrape job to cancel

**Returns:** Confirmation of cancellation

**API Reference:** [Cancel Batch Scrape Job](/api-reference/endpoint/batch-scrape-delete)

  </Tab>

  <Tab title="Preview crawl params from prompt">

Preview which URLs will be crawled with given parameters without starting a crawl.

**Parameters:** Same as crawl operation

**Returns:** Preview of URLs that would be crawled

**Response Schema**

```json
{
  "success": true,
  "preview": [
    "https://example.com",
    "https://example.com/about",
    "https://example.com/products"
  ]
}
```

**API Reference:** [Preview Crawl Params](/api-reference/endpoint/crawl-params-preview)

  </Tab>
</Tabs>

## Analytics Operations

Monitor your team's usage and resource consumption.

<Tabs>
  <Tab title="Get team credit usage">

Get current credit balance and usage statistics.

**Parameters:** None required

**Returns:** Current credits, used credits, and remaining balance

**Response Schema**

```json
{
  "success": true,
  "data": {
    "credits": 10000,
    "used": 2450,
    "remaining": 7550,
    "resetDate": "2024-02-01T00:00:00.000Z"
  }
}
```

**API Reference:** [Get Team Credit Usage](/api-reference/endpoint/credit-usage)

  </Tab>

  <Tab title="Get team token usage">

Get current LLM token consumption statistics.

**Parameters:** None required

**Returns:** Token usage for extract and LLM-powered features

**Response Schema**

```json
{
  "success": true,
  "data": {
    "tokensUsed": 125000,
    "tokensRemaining": 875000,
    "resetDate": "2024-02-01T00:00:00.000Z"
  }
}
```

**API Reference:** [Get Team Token Usage](/api-reference/endpoint/token-usage)

  </Tab>

  <Tab title="Get historical credit usage">

Get historical credit usage data over time.

**Parameters:**

**Start Date** (optional)
Start date for historical data

**End Date** (optional)
End date for historical data

**Returns:** Time-series credit usage data

**Response Schema**

```json
{
  "success": true,
  "data": [
    {
      "date": "2024-01-15",
      "creditsUsed": 450,
      "operations": {
        "scrape": 200,
        "crawl": 150,
        "extract": 100
      }
    }
  ]
}
```

**API Reference:** [Get Historical Credit Usage](/api-reference/endpoint/credit-usage-historical)

  </Tab>

  <Tab title="Get historical token usage">

Get historical LLM token usage data over time.

**Parameters:**

**Start Date** (optional)
Start date for historical data

**End Date** (optional)
End date for historical data

**Returns:** Time-series token usage data

**API Reference:** [Get Historical Token Usage](/api-reference/endpoint/token-usage-historical)

  </Tab>

  <Tab title="Get team queue status">

Get information about jobs in your team's queue.

**Parameters:** None required

**Returns:** Queue depth, estimated wait time, and job counts

**Response Schema**

```json
{
  "success": true,
  "data": {
    "queueDepth": 15,
    "estimatedWaitTime": 120,
    "activeJobs": 5,
    "pendingJobs": 10
  }
}
```

**API Reference:** [Get Team Queue Status](/api-reference/endpoint/queue-status)

  </Tab>
</Tabs>

## Community Workflow Examples

<Info>All workflows below are **free** to use and open-source</Info>

| Use Case | Workflow Link | Best For |
|----------|----------|----------|
| **Market Research** | → [View Workflow](https://n8n.io/workflows/4588) | Research teams tracking news & trends |
| **Compliance** | → [View Workflow](https://n8n.io/workflows/5591) | Daily monitoring with alerts |
| **Price Tracking** | → [View Workflow](https://n8n.io/workflows/3101) | E-commerce competitor analysis |
| **Sales Leads** | → [View Workflow](https://n8n.io/workflows/5786) | B2B prospecting & outreach |
| **Lead Outreach** | → [View Workflow](https://n8n.io/workflows/8520) | Personalized email automation |
| **Product Updates** | → [View Workflow](https://n8n.io/workflows/5510) | React/JS app monitoring |
| **AI Research** | → [View Workflow](https://n8n.io/workflows/6343) | GPT-powered web research |
| **Local Businesses** | → [View Workflow](https://n8n.io/workflows/4573) | Google Maps lead generation |
| **Developer Tools** | → [View Workflow](https://n8n.io/workflows/7394) | GitHub trending tracking |
| **SEO Content** | → [View Workflow](https://n8n.io/workflows/8289) | Research-backed content briefs |
| **SEO Analysis** | → [View Workflow](https://n8n.io/workflows/7379) | Website structure analysis |
| **Content Strategy** | → [View Workflow](https://n8n.io/workflows/7372) | Google Trends automation |

## Common Workflow Patterns

<Tabs>
  <Tab title="Scheduled Monitoring">

**Pattern:** Trigger → Map → Crawl → Process → Notify

Monitor websites for changes on a schedule.

```
Schedule Trigger (daily)
  → Map website URLs
  → Crawl changed pages
  → Compare with previous data
  → Send Slack notification if changes detected
```

**Use Cases:** Competitor monitoring, compliance tracking, content updates

  </Tab>

  <Tab title="Data Enrichment">

**Pattern:** Input → Scrape → Extract → Enrich → Store

Enrich data by scraping additional information.

```
Google Sheets trigger (new row)
  → Scrape company website
  → Extract structured data (address, phone, email)
  → Enrich row with new data
  → Update Google Sheets
```

**Use Cases:** Lead enrichment, contact discovery, data validation

  </Tab>

  <Tab title="Research Pipeline">

**Pattern:** Search → Scrape → Extract → Summarize → Store

Automated research workflows with AI.

```
Manual trigger with query
  → Search web for topic
  → Scrape top results
  → Extract key information with schema
  → Summarize with OpenAI
  → Save to Notion database
```

**Use Cases:** Market research, competitive analysis, content research

  </Tab>

  <Tab title="Batch Processing">

**Pattern:** Load → Batch Scrape → Process → Export

Process large URL lists efficiently.

```
Airtable trigger (button click)
  → Load URLs from Airtable
  → Batch scrape all URLs with webhook
  → Process results as they arrive
  → Export to CSV / Database
```

**Use Cases:** Product catalogs, directory scraping, mass data collection

  </Tab>
</Tabs>

## Best Practices

### Error Handling

- Always add error handling nodes
- Use "Get Errors" operations to debug failures
- Set appropriate timeouts for long-running operations
- Monitor queue status before submitting large jobs

### Performance

- Use Map before Crawl to preview URL counts
- Set realistic limits to avoid hitting rate limits
- Use batch scrape for multiple URLs (more efficient than loops)
- Leverage webhooks for long-running jobs instead of polling

### Cost Optimization

- Use `onlyMainContent: true` to reduce token usage
- Set exclude paths to skip unnecessary pages
- Monitor credit usage with analytics operations
- Cache results in n8n database nodes for reuse

### Data Quality

- Use Extract operation for structured data instead of parsing markdown
- Enable `blockAds` to remove ad content
- Set `removeBase64Images` for cleaner output
- Use `includeTags`/`excludeTags` to focus on relevant content

## n8n vs Other Automation Platforms

| Feature | n8n | Make | Zapier | Dify |
|---------|-----|------|--------|------|
| **Deployment** | Self-hosted + Cloud | Cloud only | Cloud only | Self-hosted + Cloud |
| **Pricing Model** | Execution-based | Operations-based | Task-based | Open-source + Cloud |
| **Visual Builder** | Yes | Yes | Yes | Yes |
| **Code Support** | JavaScript & Python | Limited | Limited | Limited |
| **Firecrawl Operations** | 20+ operations | Limited | Limited | 4 tools |
| **Best For** | Developers, power users | Visual automation | Quick integrations | AI applications |
| **Open Source** | Yes | No | No | Yes |

<Tip>
**Pro Tip:** All workflows are open-source and customizable. Clone and modify them for your specific needs!
</Tip>
