---
title: 'Rastrear'
description: 'Firecrawl puede explorar recursivamente los subdominios de una URL y recopilar su contenido'
og:title: 'Rastrear | Firecrawl'
og:description: 'Firecrawl puede explorar recursivamente los subdominios de una URL y recopilar su contenido'
icon: 'spider'
---

import InstallationPython from '/snippets/es/v2/installation/python.mdx';
import InstallationNode from '/snippets/es/v2/installation/js.mdx';
import CrawlPython from '/snippets/es/v2/crawl/base/python.mdx';
import CrawlNode from '/snippets/es/v2/crawl/base/js.mdx';
import CrawlCURL from '/snippets/es/v2/crawl/base/curl.mdx';
import CheckCrawlJobPython from '/snippets/es/v2/crawl-status/short/python.mdx';
import CheckCrawlJobNode from '/snippets/es/v2/crawl-status/short/js.mdx';
import CheckCrawlJobCURL from '/snippets/es/v2/crawl-status/short/curl.mdx';
import CheckCrawlJobOutputScraping from '/snippets/es/v2/crawl-status/base/output-scraping.mdx';
import CheckCrawlJobOutputCompleted from '/snippets/es/v2/crawl-status/base/output-completed.mdx';
import CrawlWebSocketPython from '/snippets/es/v2/crawl-websocket/base/python.mdx';
import CrawlWebSocketNode from '/snippets/es/v2/crawl-websocket/base/js.mdx';
import CrawlWebhookCURL from '/snippets/es/v2/crawl-webhook/base/curl.mdx';
import PythonCrawlExample from '/snippets/es/v2/crawl/sdk-example/python.mdx';
import NodeCrawlExample from '/snippets/es/v2/crawl/sdk-example/js.mdx';
import PythonCrawlExampleResponse from '/snippets/es/v2/crawl/sdk-example/python-response.mdx';
import NodeCrawlExampleResponse from '/snippets/es/v2/crawl/sdk-example/js-response.mdx';
import StartCrawlPython from '/snippets/es/v2/start-crawl/base/python.mdx';
import StartCrawlNode from '/snippets/es/v2/start-crawl/base/js.mdx';
import StartCrawlCURL from '/snippets/es/v2/start-crawl/base/curl.mdx';
import StartCrawlOutput from '/snippets/es/v2/start-crawl/base/output.mdx';

Firecrawl rastrea sitios web de forma eficiente para extraer datos completos mientras gestiona infraestructuras web complejas. El proceso:

1. **Análisis de URL:** Examina el sitemap y rastrea el sitio web para identificar enlaces
2. **Recorrido:** Sigue enlaces de manera recursiva para encontrar todas las subpáginas
3. **Extracción (scraping):** Extrae contenido de cada página, gestionando JS y límites de tasa
4. **Salida:** Convierte los datos a Markdown limpio o a un formato estructurado

Esto garantiza una recopilación exhaustiva de datos desde cualquier URL de partida.


<div id="crawling">
  ## Rastreo
</div>

<div id="crawl-endpoint">
  ### punto de conexión /crawl
</div>

Se utiliza para rastrear una URL y todas las subpáginas accesibles. Esto envía un trabajo de rastreo y devuelve un ID de trabajo para verificar el estado del rastreo.

<Warning>
  De forma predeterminada, el rastreador ignorará los enlaces de una página si no son
  descendientes directos de la URL que proporcionas. Así, website.com/other-parent/blog-1 no se
  devolvería si rastreas website.com/blogs/. Si quieres incluir
  website.com/other-parent/blog-1, usa el parámetro `crawlEntireDomain`. Para
  rastrear subdominios como blog.website.com al rastrear website.com, usa el
  parámetro `allowSubdomains`.
</Warning>

<div id="installation">
  ### Instalación
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />
</CodeGroup>

<div id="usage">
  ### Uso
</div>

<CodeGroup>
  <CrawlPython />

  <CrawlNode />

  <CrawlCURL />
</CodeGroup>

<div id="scrape-options-in-crawl">
  ### Opciones de scraping en crawl
</div>

Todas las opciones del endpoint Scrape están disponibles en Crawl mediante `scrapeOptions` (JS) / `scrape_options` (Python). Se aplican a cada página que el crawler raspa: formatos, proxy, caché, acciones, ubicación, etiquetas, etc. Consulta la lista completa en la [referencia de la API de Scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<CodeGroup>
  ```js Node
  import Firecrawl from '@mendable/firecrawl-js';

  const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR_API_KEY' });

  // Crawl con opciones de scraping
  const crawlResponse = await firecrawl.crawl('https://example.com', {
    limit: 100,
    scrapeOptions: {
      formats: [
        'markdown',
        {
          type: 'json',
          schema: { type: 'object', properties: { title: { type: 'string' } } },
        },
      ],
      proxy: 'auto',
      maxAge: 600000,
      onlyMainContent: true,
    },
  });
  ```

  ```python Python
  from firecrawl import Firecrawl

  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  # Crawl con opciones de scraping
  response = firecrawl.crawl('https://example.com',
      limit=100,
      scrape_options={
          'formats': [
              'markdown',
              { 'type': 'json', 'schema': { 'type': 'object', 'properties': { 'title': { 'type': 'string' } } } }
          ],
          'proxy': 'auto',
          'maxAge': 600000,
          'onlyMainContent': True
      }
  )
  ```
</CodeGroup>

<div id="api-response">
  ### Respuesta de la API
</div>

Si usas cURL o el método starter, se devolverá un `ID` para verificar el estado del rastreo.

<Note>
  Si usas el SDK, consulta los métodos a continuación para conocer el comportamiento de waiter vs starter.
</Note>

<StartCrawlOutput />

<div id="check-crawl-job">
  ### Consultar trabajo de rastreo
</div>

Se usa para verificar el estado de un trabajo de rastreo y obtener su resultado.

<Note>
  Este endpoint solo funciona para rastreos que están en progreso o que se han
  completado recientemente.{' '}
</Note>

<CodeGroup>
  <CheckCrawlJobPython />

  <CheckCrawlJobNode />

  <CheckCrawlJobCURL />
</CodeGroup>

<div id="response-handling">
  #### Manejo de respuestas
</div>

La respuesta varía según el estado del rastreo.

Para respuestas incompletas o de gran tamaño que superen los 10 MB, se proporciona un parámetro de URL `next`. Debes solicitar esta URL para obtener los siguientes 10 MB de datos. Si el parámetro `next` no está presente, indica el final de los datos del rastreo.

El parámetro `skip` define el número máximo de resultados incluidos en cada bloque de resultados devueltos.

<Info>
  Los parámetros `skip` y `next` solo son relevantes cuando se consume la API directamente.
  Si usas el SDK, nos encargamos de esto por ti y devolveremos
  todos los resultados de una vez.
</Info>

<CodeGroup>
  <CheckCrawlJobOutputScraping />

  <CheckCrawlJobOutputCompleted />
</CodeGroup>

<div id="sdk-methods">
  ### Métodos del SDK
</div>

Hay dos maneras de usar el SDK:

1. **Rastrear y esperar** (`crawl`):
   * Espera a que el rastreo termine y devuelve la respuesta completa
   * Gestiona la paginación automáticamente
   * Recomendado para la mayoría de los casos de uso

<CodeGroup>
  <PythonCrawlExample />

  <NodeCrawlExample />
</CodeGroup>

La respuesta incluye el estado del rastreo y todos los datos extraídos:

<CodeGroup>
  <PythonCrawlExampleResponse />

  <NodeCrawlExampleResponse />
</CodeGroup>

2. **Iniciar y luego verificar el estado** (`startCrawl`/`start_crawl`):
   * Devuelve de inmediato un ID de rastreo
   * Permite verificar el estado manualmente
   * Útil para rastreos de larga duración o lógica de sondeo personalizada

<CodeGroup>
  <StartCrawlPython />

  <StartCrawlNode />

  <StartCrawlCURL />
</CodeGroup>

<div id="crawl-websocket">
  ## WebSocket de rastreo
</div>

El método de Firecrawl basado en WebSocket, `Crawl URL and Watch`, permite la extracción y el monitoreo de datos en tiempo real. Inicia un rastreo con una URL y personalízalo con opciones como límites de páginas, dominios permitidos y formatos de salida; ideal para necesidades de procesamiento de datos inmediatas.

<CodeGroup>
  <CrawlWebSocketPython />

  <CrawlWebSocketNode />
</CodeGroup>

<div id="crawl-webhook">
  ## Webhook de rastreo
</div>

Puedes configurar webhooks para recibir notificaciones en tiempo real a medida que avanza el rastreo. Esto te permite procesar las páginas conforme se van extrayendo, en lugar de esperar a que finalice todo el rastreo.

<CrawlWebhookCURL />

Para una documentación completa sobre webhooks, incluidos los tipos de eventos, la estructura del payload y ejemplos de implementación, consulta la [documentación de webhooks](/es/webhooks/overview).

<div id="quick-reference">
  ### Referencia rápida
</div>

**Tipos de eventos:**

* `crawl.started` - Cuando se inicia el rastreo
* `crawl.page` - Por cada página extraída correctamente
* `crawl.completed` - Cuando finaliza el rastreo
* `crawl.failed` - Si ocurre un error durante el rastreo

**Carga útil básica:**

```json
{
  "success": true,
  "type": "crawl.page",
  "id": "crawl-job-id",
  "data": [...], // Datos de página para eventos 'page'
  "metadata": {}, // Tus metadatos personalizados
  "error": null
}
```

<Note>
  Para obtener información detallada sobre la configuración de webhooks, las prácticas recomendadas de seguridad y la resolución de problemas, visita la [documentación sobre Webhooks](/es/webhooks/overview).
</Note>
