---
title: 'Raspado en lote'
description: 'Raspar múltiples URL en lote'
og:title: 'Raspado en lote | Firecrawl'
og:description: 'Raspar múltiples URL en lote'
---

import BatchScrapePython from '/snippets/es/v2/batch-scrape/base/python.mdx';
import BatchScrapeNode from '/snippets/es/v2/batch-scrape/base/js.mdx';
import BatchScrapeCURL from '/snippets/es/v2/batch-scrape/base/curl.mdx';
import BatchScrapeOutput from '/snippets/es/v2/batch-scrape/base/output.mdx';
import BatchScrapeAsyncOutput from '/snippets/es/v2/batch-scrape/base/async-output.mdx';
import BatchScrapeExtractPython from '/snippets/es/v2/batch-scrape/json/python.mdx';
import BatchScrapeExtractNode from '/snippets/es/v2/batch-scrape/json/js.mdx';
import BatchScrapeExtractCURL from '/snippets/es/v2/batch-scrape/json/curl.mdx';
import BatchScrapeExtractOutput from '/snippets/es/v2/batch-scrape/json/output.mdx';
import BatchScrapeExtractAsyncOutput from '/snippets/es/v2/batch-scrape/json/async-output.mdx';
import BatchScrapeWebhookCURL from '/snippets/es/v1/batch-scrape-webhook/base/curl.mdx';

<div id="batch-scraping-multiple-urls">
  ## Raspado por lotes de múltiples URL
</div>

Ahora puedes raspar por lotes varias URL al mismo tiempo. Recibe las URL iniciales y parámetros opcionales como argumentos. El argumento params te permite especificar opciones adicionales para el trabajo de raspado por lotes, como los formatos de salida.

<div id="how-it-works">
  ### Cómo funciona
</div>

Es muy similar al funcionamiento del punto de conexión `/crawl`. Puedes iniciar el lote y esperar a que termine, o iniciarlo y gestionar tú mismo su finalización.

* `batchScrape` (JS) / `batch_scrape` (Python): inicia un trabajo por lotes, espera a que finalice y devuelve los resultados.
* `startBatchScrape` (JS) / `start_batch_scrape` (Python): inicia un trabajo por lotes y devuelve el ID del trabajo para que puedas hacer polling o usar webhooks.

<div id="usage">
  ### Uso
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Respuesta
</div>

* Llamar a `batchScrape`/`batch_scrape` devuelve los resultados completos cuando el lote finaliza.

<BatchScrapeOutput />`- Llamar a`startBatchScrape`/`start&#95;batch&#95;scrape`devuelve
un ID de tarea que puedes monitorear mediante`getBatchScrapeStatus`/`get&#95;batch&#95;scrape&#95;status`, usando
el endpoint de la API `/batch/scrape/{id}`, o webhooks. Este endpoint está diseñado para
consultas mientras está en progreso o justo después de completarse, **ya que los trabajos por lotes expiran después de
24 horas**.

<BatchScrapeAsyncOutput />

<div id="batch-scrape-with-structured-extraction">
  ## Raspado por lotes con extracción estructurada
</div>

También puedes usar el punto de conexión de raspado por lotes para extraer datos estructurados de las páginas. Esto es útil si quieres obtener los mismos datos estructurados de una lista de URL.

<CodeGroup>
  <BatchScrapeExtractPython />

  <BatchScrapeExtractNode />

  <BatchScrapeExtractCURL />
</CodeGroup>

<div id="response">
  ### Respuesta
</div>

* `batchScrape`/`batch_scrape` devuelve los resultados completos:

<BatchScrapeExtractOutput />

* `startBatchScrape`/`start_batch_scrape` devuelve un ID de tarea:

<BatchScrapeExtractAsyncOutput />

<div id="batch-scrape-with-webhooks">
  ## Raspado por lotes con webhooks
</div>

Puedes configurar webhooks para recibir notificaciones en tiempo real a medida que se rastrea cada URL de tu lote. Esto te permite procesar los resultados de inmediato en lugar de esperar a que finalice todo el lote.

<BatchScrapeWebhookCURL />

Para documentación completa sobre webhooks, incluidos los tipos de eventos, la estructura del payload y ejemplos de implementación, consulta la [documentación de webhooks](/es/webhooks/overview).

<div id="quick-reference">
  ### Referencia rápida
</div>

**Tipos de eventos:**

* `batch_scrape.started` - Cuando comienza el scraping por lotes
* `batch_scrape.page` - Para cada URL extraída correctamente
* `batch_scrape.completed` - Cuando se procesan todas las URL
* `batch_scrape.failed` - Si el scraping por lotes presenta un error

**Carga útil básica:**

```json
{
  "success": true,
  "type": "batch_scrape.page",
  "id": "batch-job-id",
  "data": [...], // Datos de página para eventos 'page'
  "metadata": {}, // Tus metadatos personalizados
  "error": null
}
```

<Note>
  Para conocer la configuración detallada de los webhooks, las mejores prácticas de seguridad y
  la resolución de problemas, visita la [documentación de webhooks](/es/webhooks/overview).
</Note>
