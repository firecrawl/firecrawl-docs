---
title: "Extracción por lotes"
description: "Extrae varias URL por lotes"
og:title: "Extracción por lotes | Firecrawl"
og:description: "Extrae varias URL por lotes"
---

import BatchScrapePython from "/snippets/es/v2/batch-scrape/base/python.mdx";
import BatchScrapeNode from "/snippets/es/v2/batch-scrape/base/js.mdx";
import BatchScrapeCURL from "/snippets/es/v2/batch-scrape/base/curl.mdx";
import BatchScrapeOutput from "/snippets/es/v2/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/es/v2/batch-scrape/base/async-output.mdx";
import BatchScrapeExtractPython from "/snippets/es/v2/batch-scrape/json/python.mdx";
import BatchScrapeExtractNode from "/snippets/es/v2/batch-scrape/json/js.mdx";
import BatchScrapeExtractCURL from "/snippets/es/v2/batch-scrape/json/curl.mdx";
import BatchScrapeExtractOutput from "/snippets/es/v2/batch-scrape/json/output.mdx";
import BatchScrapeExtractAsyncOutput from "/snippets/es/v2/batch-scrape/json/async-output.mdx";
import BatchScrapeWebhookCURL from "/snippets/es/v1/batch-scrape-webhook/base/curl.mdx";

<div id="batch-scraping-multiple-urls">
  ## Raspado por lotes de múltiples URL
</div>

Ahora puedes raspar por lotes varias URL al mismo tiempo. Recibe las URL iniciales y parámetros opcionales como argumentos. El argumento params te permite especificar opciones adicionales para el trabajo de raspado por lotes, como los formatos de salida.

<div id="how-it-works">
  ### Cómo funciona
</div>

Es muy similar al funcionamiento del punto de conexión `/crawl`. Puedes iniciar el lote y esperar a que termine, o iniciarlo y gestionar tú mismo su finalización.

* `batchScrape` (JS) / `batch_scrape` (Python): inicia un trabajo por lotes, espera a que finalice y devuelve los resultados.
* `startBatchScrape` (JS) / `start_batch_scrape` (Python): inicia un trabajo por lotes y devuelve el ID del trabajo para que puedas hacer polling o usar webhooks.

<div id="usage">
  ### Uso
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Respuesta
</div>

* Llamar a `batchScrape`/`batch_scrape` devuelve los resultados completos cuando el lote finaliza.

<BatchScrapeOutput />

`

* Llamar a `startBatchScrape`/`start_batch_scrape` devuelve un ID de trabajo que puedes seguir mediante `getBatchScrapeStatus`/`get_batch_scrape_status`, usando el punto de conexión de la API `/batch/scrape/{id}` o webhooks. Este punto de conexión está pensado para comprobaciones en curso o justo después de finalizar, **ya que los trabajos por lotes expiran a las 24 horas**.

<BatchScrapeAsyncOutput />

<div id="batch-scrape-with-structured-extraction">
  ## Raspado por lotes con extracción estructurada
</div>

También puedes usar el punto de conexión de raspado por lotes para extraer datos estructurados de las páginas. Esto es útil si quieres obtener los mismos datos estructurados de una lista de URL.

<CodeGroup>
  <BatchScrapeExtractPython />

  <BatchScrapeExtractNode />

  <BatchScrapeExtractCURL />
</CodeGroup>

<div id="response">
  ### Respuesta
</div>

* `batchScrape`/`batch_scrape` devuelve los resultados completos:

<BatchScrapeExtractOutput />

* `startBatchScrape`/`start_batch_scrape` devuelve un ID de tarea:

<BatchScrapeExtractAsyncOutput />

<div id="batch-scrape-with-webhooks">
  ## Raspado por lotes con webhooks
</div>

Puedes configurar webhooks para recibir notificaciones en tiempo real a medida que se raspa cada URL de tu lote. Esto te permite procesar los resultados de inmediato en lugar de esperar a que se complete todo el lote.

<BatchScrapeWebhookCURL />

Para obtener documentación completa sobre webhooks, incluidos los tipos de eventos, la estructura del payload y ejemplos de implementación, consulta la [documentación de webhooks](/es/features/webhooks).

<div id="quick-reference">
  ### Referencia rápida
</div>

**Tipos de eventos:**

* `batch_scrape.started` - Cuando inicia el scraping por lotes
* `batch_scrape.page` - Para cada URL raspada correctamente
* `batch_scrape.completed` - Cuando se procesan todas las URL
* `batch_scrape.failed` - Si el scraping por lotes encuentra un error

**Carga útil básica:**

```json
{
  "success": true,
  "type": "batch_scrape.page",
  "id": "batch-job-id",
  "data": [...], // Datos de la página para eventos 'page'
  "metadata": {}, // Tus metadatos personalizados
  "error": null
}
```

<Note>
  Para ver la configuración detallada de webhooks, las prácticas recomendadas de seguridad y la resolución de problemas, visita la [documentación de webhooks](/es/features/webhooks).
</Note>
