---
title: "Extracción"
description: "Convierte cualquier URL en datos limpios"
og:title: "Extracción | Firecrawl"
og:description: "Convierte cualquier URL en datos limpios"
---

import InstallationPython from "/snippets/es/v2/installation/python.mdx";
import InstallationNode from "/snippets/es/v2/installation/js.mdx";
import InstallationCLI from "/snippets/es/v2/installation/cli.mdx";
import ScrapePython from "/snippets/es/v2/scrape/base/python.mdx";
import ScrapeNode from "/snippets/es/v2/scrape/base/js.mdx";
import ScrapeCURL from "/snippets/es/v2/scrape/base/curl.mdx";
import ScrapeCLI from "/snippets/es/v2/scrape/base/cli.mdx";
import ScrapeResponse from "/snippets/es/v2/scrape/base/output.mdx";
import ExtractCURL from "/snippets/es/v2/scrape/json/base/curl.mdx";
import ExtractPython from "/snippets/es/v2/scrape/json/base/python.mdx";
import ExtractNode from "/snippets/es/v2/scrape/json/base/js.mdx";
import ExtractOutput from "/snippets/es/v2/scrape/json/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/es/v2/scrape/json/no-schema/curl.mdx";
import ExtractNoSchemaPython from "/snippets/es/v2/scrape/json/no-schema/python.mdx";
import ExtractNoSchemaNode from "/snippets/es/v2/scrape/json/no-schema/js.mdx";
import ExtractNoSchemaOutput from "/snippets/es/v2/scrape/json/no-schema/output.mdx";
import ScrapeActionsPython from "/snippets/es/v2/scrape/actions/python.mdx";
import ScrapeActionsNode from "/snippets/es/v2/scrape/actions/js.mdx";
import ScrapeActionsCURL from "/snippets/es/v2/scrape/actions/curl.mdx";
import ScrapeActionsOutput from "/snippets/es/v2/scrape/actions/output.mdx";
import BatchScrapePython from "/snippets/es/v2/batch-scrape/short/python.mdx";
import BatchScrapeNode from "/snippets/es/v2/batch-scrape/short/js.mdx";
import BatchScrapeCURL from "/snippets/es/v2/batch-scrape/short/curl.mdx";
import BatchScrapeOutput from "/snippets/es/v2/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/es/v2/batch-scrape/base/async-output.mdx";
import ScrapeLocationPython from "/snippets/es/v2/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/es/v2/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/es/v2/scrape/location/curl.mdx";
import ScrapeBrandingPython from "/snippets/es/v2/scrape/branding/base/python.mdx";
import ScrapeBrandingNode from "/snippets/es/v2/scrape/branding/base/js.mdx";
import ScrapeBrandingCURL from "/snippets/es/v2/scrape/branding/base/curl.mdx";
import ScrapeBrandingOutput from "/snippets/es/v2/scrape/branding/base/output.mdx";
import ScrapeBrandingCombinedPython from "/snippets/es/v2/scrape/branding/combined/python.mdx";
import ScrapeBrandingCombinedNode from "/snippets/es/v2/scrape/branding/combined/js.mdx";
import ScrapeBrandingCombinedCURL from "/snippets/es/v2/scrape/branding/combined/curl.mdx";

Firecrawl convierte páginas web en markdown, ideal para aplicaciones con LLM.

* Gestiona las complejidades: proxies, caché, límites de velocidad, contenido bloqueado por JS
* Maneja contenido dinámico: sitios dinámicos, sitios renderizados con JS, PDF, imágenes
* Genera markdown limpio, datos estructurados, capturas de pantalla o HTML.

Para más detalles, consulta la [referencia de la API del endpoint Scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="scraping-a-url-with-firecrawl">
  ## Extraer datos de una URL con Firecrawl
</div>

<div id="scrape-endpoint">
  ### punto de conexión /scrape
</div>

Se utiliza para extraer el contenido de una URL.

<div id="installation">
  ### Instalación
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationCLI />
</CodeGroup>

<div id="usage">
  ### Uso
</div>

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeCURL />

  <ScrapeCLI />
</CodeGroup>

Para más información sobre los parámetros, consulta la [Referencia de la API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="response">
  ### Respuesta
</div>

Los SDK devolverán el objeto de datos directamente. cURL devolverá el payload exactamente como se muestra a continuación.

<ScrapeResponse />

<div id="scrape-formats">
  ## Formatos de extracción
</div>

Ahora puedes elegir en qué formatos quieres el resultado. Puedes especificar múltiples formatos de salida. Los formatos compatibles son:

* Markdown (`markdown`)
* Resumen (`summary`)
* HTML (`html`) - versión limpia del HTML de la página
* HTML sin procesar (`rawHtml`) - HTML sin modificar tal como se recibe de la página
* Captura de pantalla (`screenshot`, con opciones como `fullPage`, `quality`, `viewport`)
* Enlaces (`links`)
* JSON (`json`) - salida estructurada
* Imágenes (`images`) - extrae todas las URL de imágenes de la página
* Identidad de marca (`branding`) - extrae la identidad de marca y el sistema de diseño

Las claves de salida coincidirán con el formato que elijas.

<div id="extract-structured-data">
  ## Extrae datos estructurados
</div>

<div id="scrape-with-json-endpoint">
  ### punto de conexión /scrape (con json)
</div>

Se usa para extraer datos estructurados de páginas rastreadas.

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

Salida:

<ExtractOutput />

<div id="extracting-without-schema">
  ### Extracción sin esquema
</div>

Ahora puedes extraer sin un esquema con solo pasar un `prompt` al punto de conexión. El LLM elige la estructura de los datos.

<CodeGroup>
  <ExtractNoSchemaPython />

  <ExtractNoSchemaNode />

  <ExtractNoSchemaCURL />
</CodeGroup>

Salida:

<ExtractNoSchemaOutput />

<div id="json-format-options">
  ### Opciones del formato JSON
</div>

Al usar el formato `json`, pasa un objeto dentro de `formats` con los siguientes parámetros:

* `schema`: JSON Schema para la salida estructurada.
* `prompt`: Prompt opcional para ayudar a guiar la extracción cuando hay un esquema o cuando prefieras una guía ligera.

<div id="extract-brand-identity">
  ## Extraer la identidad de la marca
</div>

<div id="scrape-with-branding-endpoint">
  ### endpoint /scrape (con branding)
</div>

El formato de branding extrae información completa sobre la identidad de marca de una página web, incluidos colores, fuentes, tipografía, espaciado, componentes de la UI y más. Es útil para analizar sistemas de diseño, monitorear marcas o crear herramientas que necesiten comprender la identidad visual de un sitio web.

<CodeGroup>
  <ScrapeBrandingPython />

  <ScrapeBrandingNode />

  <ScrapeBrandingCURL />
</CodeGroup>

### Respuesta

El formato de marca devuelve un objeto `BrandingProfile` completo con la siguiente estructura:

<ScrapeBrandingOutput />

<div id="branding-profile-structure">
  ### Estructura del perfil de marca
</div>

El objeto `branding` contiene las siguientes propiedades:

* `colorScheme`: El esquema de color detectado (&quot;light&quot; o &quot;dark&quot;)
* `logo`: URL del logotipo principal
* `colors`: Objeto que contiene los colores de la marca:
  * `primary`, `secondary`, `accent`: Colores principales de la marca
  * `background`, `textPrimary`, `textSecondary`: Colores de la interfaz
  * `link`, `success`, `warning`, `error`: Colores semánticos
* `fonts`: Lista de familias tipográficas usadas en la página
* `typography`: Información tipográfica detallada:
  * `fontFamilies`: Familias tipográficas principal, de encabezados y de código
  * `fontSizes`: Definiciones de tamaños para encabezados y cuerpo de texto
  * `fontWeights`: Definiciones de grosor (light, regular, medium, bold)
  * `lineHeights`: Valores de interlineado para distintos tipos de texto
* `spacing`: Información de espaciado y maquetación:
  * `baseUnit`: Unidad base de espaciado en píxeles
  * `borderRadius`: Radio de borde predeterminado
  * `padding`, `margins`: Valores de espaciado
* `components`: Estilos de componentes de la interfaz:
  * `buttonPrimary`, `buttonSecondary`: Estilos de botones
  * `input`: Estilos de campos de entrada
* `icons`: Información sobre el estilo de los íconos
* `images`: Imágenes de marca (logo, favicon, og:image)
* `animations`: Configuración de animaciones y transiciones
* `layout`: Configuración de distribución (grid, alturas de encabezado/pie)
* `personality`: Rasgos de personalidad de la marca (tono, energía, público objetivo)

<div id="combining-with-other-formats">
  ### Combinar con otros formatos
</div>

Puedes combinar el formato de branding con otros formatos para obtener datos completos de la página:

<CodeGroup>
  <ScrapeBrandingCombinedPython />

  <ScrapeBrandingCombinedNode />

  <ScrapeBrandingCombinedCURL />
</CodeGroup>

<div id="interacting-with-the-page-with-actions">
  ## Interacción con la página mediante acciones
</div>

Firecrawl te permite realizar varias acciones en una página web antes de extraer su contenido. Esto es especialmente útil para interactuar con contenido dinámico, navegar entre páginas o acceder a contenido que requiera interacción del usuario.

Aquí tienes un ejemplo de cómo usar acciones para ir a google.com, buscar Firecrawl, hacer clic en el primer resultado y tomar una captura de pantalla.

Es importante usar casi siempre la acción `wait` antes y/o después de ejecutar otras acciones para dar tiempo suficiente a que la página cargue.

<div id="example">
  ### Ejemplo
</div>

<CodeGroup>
  <ScrapeActionsPython />

  <ScrapeActionsNode />

  <ScrapeActionsCURL />
</CodeGroup>

<div id="output">
  ### Salida
</div>

<CodeGroup>
  <ScrapeActionsOutput />
</CodeGroup>

Para más información sobre los parámetros de las acciones, consulta la [Referencia de la API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="location-and-language">
  ## Ubicación e idioma
</div>

Especifica el país y los idiomas preferidos para obtener contenido relevante según tu ubicación de destino y tus preferencias de idioma.

<div id="how-it-works">
  ### Cómo funciona
</div>

Cuando especificas la ubicación, Firecrawl usará un proxy adecuado si está disponible y emulará el idioma y la zona horaria correspondientes. De forma predeterminada, la ubicación se establece en &quot;US&quot; si no se indica lo contrario.

<div id="usage">
  ### Uso
</div>

Para usar la configuración de ubicación e idioma, incluye el objeto `location` en el cuerpo de la solicitud con las siguientes propiedades:

* `country`: código de país ISO 3166-1 alfa-2 (p. ej., &#39;US&#39;, &#39;AU&#39;, &#39;DE&#39;, &#39;JP&#39;). Por defecto: &#39;US&#39;.
* `languages`: una lista de idiomas y configuraciones regionales preferidas para la solicitud en orden de prioridad. Por defecto, usa el idioma de la ubicación especificada.

<CodeGroup>
  <ScrapeLocationPython />

  <ScrapeLocationNode />

  <ScrapeLocationCURL />
</CodeGroup>

Para más información sobre las ubicaciones compatibles, consulta la [documentación de proxies](/es/features/proxies).

<div id="caching-and-maxage">
  ## Caché y maxAge
</div>

Para acelerar las solicitudes, Firecrawl sirve resultados desde la caché por defecto cuando hay una copia reciente disponible.

* **Ventana de frescura predeterminada**: `maxAge = 172800000` ms (2 días). Si la copia en caché es más reciente que esto, se devuelve al instante; de lo contrario, la página se vuelve a extraer y luego se almacena en caché.
* **Rendimiento**: Esto puede acelerar las extracciones hasta 5× cuando los datos no necesitan estar ultra frescos.
* **Obtener siempre contenido fresco**: Establece `maxAge` en `0`.
* **Evitar almacenamiento**: Establece `storeInCache` en `false` si no quieres que Firecrawl almacene/guarde los resultados de esta solicitud en la caché.
* **Seguimiento de cambios**: Las solicitudes que incluyen `changeTracking` omiten la caché, por lo que se ignora `maxAge`.

Ejemplo (forzar contenido fresco):

<CodeGroup>
  ```python Python
  from firecrawl import Firecrawl
  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  doc = firecrawl.scrape(url='https://example.com', maxAge=0, formats=['markdown'])
  print(doc)
  ```

  ```js Node
  import Firecrawl from '@mendable/firecrawl-js';

  const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

  const doc = await firecrawl.scrape('https://example.com', { maxAge: 0, formats: ['markdown'] });
  console.log(doc);
  ```

  ```bash cURL
  curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
    -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "url": "https://example.com",
      "maxAge": 0,
      "formats": ["markdown"]
    }'
  ```
</CodeGroup>

Ejemplo (usar una ventana de caché de 10 minutos):

<CodeGroup>
  ```python Python
  from firecrawl import Firecrawl
  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  doc = firecrawl.scrape(url='https://example.com', maxAge=600000, formats=['markdown', 'html'])
  print(doc)
  ```

  ```js Node

  const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

  const doc = await firecrawl.scrape('https://example.com', { maxAge: 600000, formats: ['markdown', 'html'] });
  console.log(doc);
  ```

  ```bash cURL
  curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
    -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "url": "https://example.com",
      "maxAge": 600000,
      "formats": ["markdown", "html"]
    }'
  ```
</CodeGroup>

<div id="batch-scraping-multiple-urls">
  ## Raspado por lotes de múltiples URL
</div>

Ahora puedes raspar varias URL en lote al mismo tiempo. Recibe las URL iniciales y parámetros opcionales como argumentos. El argumento params te permite especificar opciones adicionales para el trabajo de raspado por lotes, como los formatos de salida.

<div id="how-it-works">
  ### Cómo funciona
</div>

Es muy similar al funcionamiento del punto de conexión `/crawl`. Envía un trabajo de scraping por lotes y devuelve un ID de trabajo para consultar el estado del scraping por lotes.

El SDK ofrece 2 métodos: sincrónico y asincrónico. El método sincrónico devuelve los resultados del trabajo de scraping por lotes, mientras que el asincrónico devuelve un ID de trabajo que puedes usar para consultar el estado del scraping por lotes.

<div id="usage">
  ### Uso
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Respuesta
</div>

Si usas los métodos síncronos de los SDK, devolverá los resultados del scraping por lotes. De lo contrario, devolverá un ID de tarea que puedes usar para consultar el estado del scraping por lotes.

<div id="synchronous">
  #### Sincronía
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### Asíncrono
</div>

Luego puedes usar el ID del trabajo para consultar el estado del scrape por lotes llamando al punto de conexión `/batch/scrape/{id}`. Este punto de conexión está pensado para usarse mientras el trabajo sigue en ejecución o justo después de que haya finalizado, **ya que los trabajos de scrape por lotes expiran a las 24 horas**.

<BatchScrapeAsyncOutput />

<div id="enhanced-mode">
  ## Modo mejorado
</div>

Para sitios web complejos, Firecrawl ofrece un modo mejorado que proporciona mayores tasas de éxito a la vez que mantiene la privacidad.

Obtén más información sobre el [Modo mejorado](/es/features/enhanced-mode).