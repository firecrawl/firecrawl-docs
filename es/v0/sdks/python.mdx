---
title: 'Python'
description: 'El SDK de Python de Firecrawl es un envoltorio de la API de Firecrawl que te ayuda a convertir sitios web en Markdown de forma sencilla.'
icon: 'python'
og:title: "Python SDK | Firecrawl"
og:description: "El SDK de Python de Firecrawl es un envoltorio de la API de Firecrawl que te ayuda a convertir sitios web en Markdown de forma sencilla."
---

> Nota: esto utiliza la [versión v0 de la API de Firecrawl](/es/v0/introduction), que está en proceso de desuso. Recomendamos pasarte a [v1](/es/sdks/python).

<div id="installation">
  ## Instalación
</div>

Para instalar el SDK de Python de Firecrawl, puedes usar pip:

```bash
pip install firecrawl-py==0.0.16
```

<div id="usage">
  ## Uso
</div>

1. Obtén una clave de API en [firecrawl.dev](https://firecrawl.dev)
2. Define la clave de API como una variable de entorno llamada `FIRECRAWL_API_KEY` o pásala como parámetro a la clase `FirecrawlApp`.

Aquí tienes un ejemplo de cómo usar el SDK:

```python
from firecrawl import FirecrawlApp

# Inicializa FirecrawlApp con tu clave de API
app = FirecrawlApp(api_key='your_api_key')

# Extrae una única URL
url = 'https://docs.firecrawl.dev'
scraped_data = app.scrape_url(url)

# Rastrea un sitio web
crawl_url = 'https://docs.firecrawl.dev'
params = {
    'pageOptions': {
        'onlyMainContent': True
    }
}
crawl_result = app.crawl_url(crawl_url, params=params)
```

<div id="scraping-a-url">
  ### Extracción de una URL
</div>

Para extraer una única URL, usa el método `scrape_url`. Recibe la URL como parámetro y devuelve los datos extraídos como un diccionario.

```python
url = 'https://example.com'
scraped_data = app.scrape_url(url)
```

<div id="extracting-structured-data-from-a-url">
  ### Extracción de datos estructurados desde una URL
</div>

Con la extracción mediante LLM, puedes extraer fácilmente datos estructurados de cualquier URL. También admitimos esquemas de Pydantic para hacerlo más sencillo. Aquí te mostramos cómo usarlo:

```python
class ArticleSchema(BaseModel):
    title: str
    points: int 
    by: str
    commentsURL: str

class TopArticlesSchema(BaseModel):
    top: List[ArticleSchema] = Field(..., max_items=5, description="Las 5 principales noticias")

data = app.scrape_url('https://news.ycombinator.com', {
    'extractorOptions': {
        'extractionSchema': TopArticlesSchema.model_json_schema(),
        'mode': 'llm-extraction'
    },
    'pageOptions':{
        'onlyMainContent': True
    }
})
print(data["llm_extraction"])
```

<div id="crawling-a-website">
  ### Rastreo de un sitio web
</div>

Para rastrear un sitio web, utiliza el método `crawl_url`. Recibe la URL inicial y parámetros opcionales como argumentos. El argumento `params` te permite especificar opciones adicionales para el trabajo de rastreo, como el número máximo de páginas a rastrear, los dominios permitidos y el formato de salida.

El parámetro `wait_until_done` determina si el método debe esperar a que el trabajo de rastreo se complete antes de devolver el resultado. Si se establece en `True`, el método comprobará periódicamente el estado del trabajo de rastreo hasta que se complete o se alcance el `timeout` especificado (en segundos). Si se establece en `False`, el método devolverá inmediatamente el ID del trabajo, y podrás comprobar manualmente el estado del trabajo de rastreo usando el método `check_crawl_status`.

```python
crawl_url = 'https://example.com'
params = {
    'crawlerOptions': {
        'excludes': ['blog/*'],
        'includes': [], # dejarlo vacío para incluir todas las páginas
        'limit': 1000,
    },
    'pageOptions': {
        'onlyMainContent': True
    }
}
crawl_result = app.crawl_url(crawl_url, params=params, wait_until_done=True, timeout=5)
```

Si `wait_until_done` se establece en `True`, el método `crawl_url` devolverá el resultado del rastreo una vez que el trabajo haya finalizado. Si el trabajo falla o se detiene, se lanzará una excepción.

<div id="checking-crawl-status">
  ### Comprobar el estado del rastreo
</div>

Para comprobar el estado de un trabajo de rastreo, usa el método `check_crawl_status`. Recibe el ID del trabajo como parámetro y devuelve el estado actual del rastreo.

```python
job_id = crawl_result['jobId']
status = app.check_crawl_status(job_id)
```

<div id="search-for-a-query">
  ### Buscar una consulta
</div>

Se usa para buscar en la web, obtener los resultados más relevantes, hacer scraping de cada página y devolver el contenido en markdown.

```python
query = '¿Qué es Mendable?'
search_result = app.search(query)
```

<div id="error-handling">
  ## Manejo de errores
</div>

El SDK gestiona los errores que devuelve la API de Firecrawl y arroja las excepciones correspondientes. Si se produce un error durante una solicitud, se generará una excepción con un mensaje descriptivo.