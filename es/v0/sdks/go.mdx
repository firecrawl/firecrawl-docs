---
title: 'Go'
description: 'El SDK de Go de Firecrawl es un wrapper de la API de Firecrawl que te ayuda a convertir sitios web en markdown fácilmente.'
icon: 'golang'
og:title: "Go SDK | Firecrawl"
og:description: "El SDK de Go de Firecrawl es un wrapper de la API de Firecrawl que te ayuda a convertir sitios web en markdown fácilmente."
---

> Nota: esto usa la [versión v0 de la API de Firecrawl](/es/v0/introduction), que está en proceso de desuso. Recomendamos pasar a [v1](/es/sdks/go).

<div id="installation">
  ## Instalación
</div>

Para instalar el SDK de Go de Firecrawl, puedes usar go get:

```bash
go get github.com/mendableai/firecrawl-go
```

<div id="usage">
  ## Uso
</div>

1. Obtén una clave de API en [firecrawl.dev](https://firecrawl.dev)
2. Define la clave de API como una variable de entorno llamada `FIRECRAWL_API_KEY` o pásala como parámetro a la estructura `FirecrawlApp`.

Aquí tienes un ejemplo de cómo usar el SDK con manejo de errores:

```go
import (
  "fmt"
  "log"

  "github.com/mendableai/firecrawl-go"
)

func main() {
  // Inicializa FirecrawlApp con tu clave de API
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
    log.Fatalf("Error al inicializar FirecrawlApp: %v", err)
  }

  // Extrae una única URL
  scrapedData, err := app.ScrapeURL("docs.firecrawl.dev", nil)
  if err != nil {
    log.Fatalf("Se produjo un error durante la extracción: %v", err)
  }
  fmt.Println(scrapedData)

  // Rastrea un sitio web
  params := map[string]any{
    "pageOptions": map[string]any{
      "onlyMainContent": true,
    },
  }

  crawlResult, err := app.CrawlURL("docs.firecrawl.dev", params)
  if err != nil {
    log.Fatalf("Se produjo un error durante el rastreo: %v", err)
  }
  fmt.Println(crawlResult)
}
```

<div id="scraping-a-url">
  ### Extracción de una URL
</div>

Para extraer una única URL con manejo de errores, usa el método `ScrapeURL`. Recibe la URL como parámetro y devuelve los datos extraídos como un diccionario.

```go
scrapedData, err := app.ScrapeURL("docs.firecrawl.dev", nil)
if err != nil {
  log.Fatalf("No se pudo extraer la URL: %v", err)
}
fmt.Println(scrapedData)
```

<div id="crawling-a-website">
  ### Rastrear un sitio web
</div>

Para rastrear un sitio web, usa el método `CrawlUrl`. Recibe la URL inicial y parámetros opcionales como argumentos. El argumento `params` te permite especificar opciones adicionales para la tarea de rastreo, como el número máximo de páginas a rastrear, los dominios permitidos y el formato de salida.

```go
crawlParams := map[string]any{
  "crawlerOptions": map[string]any{
    "excludes": []string{"blog/*"},
    "includes": []string{}, // dejar vacío para incluir todas las páginas
    "limit": 1000,
  },
  "pageOptions": map[string]any{
    "onlyMainContent": true,
  },
}
crawlResult, err := app.CrawlURL("docs.firecrawl.dev", crawlParams, true, 2, idempotencyKey)
if err != nil {
  log.Fatalf("No se pudo rastrear la URL: %v", err)
}
fmt.Println(crawlResult)
```

<div id="checking-crawl-status">
  ### Comprobar el estado del rastreo
</div>

Para comprobar el estado de un trabajo de rastreo, utiliza el método `CheckCrawlStatus`. Recibe el ID del trabajo como parámetro y devuelve el estado actual del rastreo.

```go
status, err := app.CheckCrawlStatus(jobId)
if err != nil {
  log.Fatalf("No se pudo verificar el estado del rastreo: %v", err)
}
fmt.Println(status)
```

<div id="canceling-a-crawl-job">
  ### Cancelación de un trabajo de rastreo
</div>

Para cancelar un trabajo de rastreo, utiliza el método `CancelCrawlJob`. Recibe el ID del trabajo como parámetro y devuelve el estado de la cancelación del trabajo de rastreo.

```go
canceled, err := app.CancelCrawlJob(jobId)
if err != nil {
  log.Fatalf("No se pudo cancelar el job de rastreo: %v", err)
}
fmt.Println(canceled)
```

<div id="extracting-structured-data-from-a-url">
  ### Extracción de datos estructurados desde una URL
</div>

Con la extracción con LLM, puedes obtener fácilmente datos estructurados de cualquier URL. Aquí te explicamos cómo usarla:

```go
jsonSchema := map[string]any{
  "type": "object",
  "properties": map[string]any{
    "top": map[string]any{
      "type": "array",
      "items": map[string]any{
        "type": "object",
        "properties": map[string]any{
          "title":       map[string]string{"type": "string"},
          "points":      map[string]string{"type": "number"},
          "by":          map[string]string{"type": "string"},
          "commentsURL": map[string]string{"type": "string"},
        },
        "required": []string{"title", "points", "by", "commentsURL"},
      },
      "minItems":    5,
      "maxItems":    5,
      "description": "Las 5 principales noticias de Hacker News",
    },
  },
  "required": []string{"top"},
}

llmExtractionParams := map[string]any{
  "extractorOptions": firecrawl.ExtractorOptions{
    ExtractionSchema: jsonSchema,
  },
}

scrapeResult, err := app.ScrapeURL("https://news.ycombinator.com", llmExtractionParams)
if err != nil {
  log.Fatalf("Error al realizar la extracción con LLM: %v", err)
}
fmt.Println(scrapeResult)
```

<div id="search-for-a-query">
  ### Buscar una consulta
</div>

Para buscar en la web, obtener los resultados más relevantes, extraer cada página y devolver el contenido en markdown, usa el método `Search`. Este método recibe la consulta como parámetro y devuelve los resultados de búsqueda.

```go
query := "¿Qué es Firecrawl?"
searchResult, err := app.Search(query)
if err != nil {
  log.Fatalf("Error al realizar la búsqueda: %v", err)
}
fmt.Println(searchResult)
```

<div id="error-handling">
  ## Manejo de errores
</div>

El SDK gestiona los errores que devuelve la API de Firecrawl y lanza las excepciones correspondientes. Si se produce un error durante una solicitud, se generará una excepción con un mensaje descriptivo.