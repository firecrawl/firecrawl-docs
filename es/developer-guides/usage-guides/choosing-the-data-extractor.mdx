---
title: "Elegir el extractor de datos"
description: "Comparar /agent, /extract y /scrape (modo JSON) para elegir la herramienta adecuada para la extracción de datos estructurados"
og:title: "Elegir el extractor de datos | Firecrawl"
og:description: "Comparar /agent, /extract y /scrape (modo JSON) para elegir la herramienta adecuada para la extracción de datos estructurados"
sidebarTitle: "Elegir el extractor de datos"
---

import AgentWithSchemaPython from "/snippets/es/v2/agent/with-schema/python.mdx";
import AgentWithSchemaJS from "/snippets/es/v2/agent/with-schema/js.mdx";
import AgentWithSchemaCURL from "/snippets/es/v2/agent/with-schema/curl.mdx";
import ExtractPython from "/snippets/es/v2/extract/base/python.mdx";
import ExtractNode from "/snippets/es/v2/extract/base/js.mdx";
import ExtractCURL from "/snippets/es/v2/extract/base/curl.mdx";
import ScrapeJsonPython from "/snippets/es/v2/scrape/json/base/python.mdx";
import ScrapeJsonNode from "/snippets/es/v2/scrape/json/base/js.mdx";
import ScrapeJsonCURL from "/snippets/es/v2/scrape/json/base/curl.mdx";

Firecrawl ofrece tres métodos para extraer datos estructurados de páginas web. Cada uno se adapta a distintos casos de uso, con diferentes niveles de automatización y control.

<div id="quick-comparison">
  ## Comparación rápida
</div>

| Funcionalidad | `/agent` | `/extract` | `/scrape` (modo JSON) |
|---------------|----------|------------|------------------------|
| **Estado** | Activo | Usa `/agent` en su lugar | Activo |
| **URL requerida** | No (opcional) | Sí (comodines admitidos) | Sí (una sola URL) |
| **Alcance** | Descubrimiento en toda la web | Varias páginas/dominios | Una sola página |
| **Descubrimiento de URL** | Búsqueda web autónoma | Rastrea a partir de las URL proporcionadas | Ninguno |
| **Procesamiento** | Asíncrono | Asíncrono | Síncrono |
| **Esquema requerido** | No (prompt o esquema) | No (prompt o esquema) | No (prompt o esquema) |
| **Precio** | Dinámico (5 ejecuciones gratis/día) | Basado en tokens (1 crédito = 15 tokens) | 1 crédito/página |
| **Ideal para** | Investigación, descubrimiento, recopilación compleja | Extracción de varias páginas (cuando conoces las URL) | Extracción de una sola página conocida |

<div id="1-agent-endpoint">
  ## 1. `/agent` Endpoint
</div>

El endpoint `/agent` es la funcionalidad más avanzada de Firecrawl, el sucesor de `/extract`. Utiliza agentes de IA para buscar, navegar y recopilar datos de forma autónoma en toda la web.

<div id="key-characteristics">
  ### Características clave
</div>

* **URLs opcionales**: Basta con describir lo que necesitas mediante `prompt`; las URLs son completamente opcionales
* **Navegación autónoma**: El agente busca y explora en profundidad sitios web para encontrar tus datos
* **Búsqueda profunda en la web**: Descubre información de forma autónoma en múltiples dominios y páginas
* **Procesamiento en paralelo**: Procesa múltiples fuentes simultáneamente para obtener resultados más rápidos
* **Modelos disponibles**: `spark-1-mini` (predeterminado, 60% más económico) y `spark-1-pro` (mayor precisión)

<div id="example">
  ### Ejemplo
</div>

<CodeGroup>
  <AgentWithSchemaPython />

  <AgentWithSchemaJS />

  <AgentWithSchemaCURL />
</CodeGroup>

<div id="best-use-case-autonomous-research-discovery">
  ### Caso de uso ideal: investigación y descubrimiento autónomos
</div>

**Escenario**: Necesitas encontrar información sobre startups de IA que hayan levantado una ronda de financiación Serie A, incluyendo sus fundadores y los montos de la financiación.

**Por qué `/agent`**: No sabes qué sitios web contienen esta información. El agente buscará de forma autónoma en la web, navegará a fuentes relevantes (Crunchbase, sitios de noticias, páginas de empresas) y compilará los datos estructurados para ti.

Para más detalles, consulta la [documentación del agente](/es/features/agent).

***

<div id="2-extract-endpoint">
  ## 2. Endpoint `/extract`
</div>

<Note>
  **Usa `/agent` en su lugar**: Recomendamos migrar a [`/agent`](/es/features/agent): es más rápido, más fiable, no requiere URL y cubre todos los casos de uso de `/extract`, y más.
</Note>

El endpoint `/extract` recopila datos estructurados de URL específicas o dominios completos mediante extracción basada en LLM.

<div id="key-characteristics">
  ### Características clave
</div>

* **URLs normalmente requeridas**: Se debe proporcionar al menos una URL (admite comodines como `example.com/*`)
* **Rastreo de dominio**: Puede rastrear y analizar todas las URLs descubiertas en un dominio
* **Mejora de la búsqueda web**: `enableWebSearch` opcional para seguir enlaces fuera de los dominios especificados
* **Esquema opcional**: Admite un esquema JSON estricto O prompts en lenguaje natural
* **Procesamiento asíncrono**: Devuelve un ID de tarea para consultar el estado

<div id="the-url-limitation">
  ### La limitación de las URL
</div>

El desafío fundamental con `/extract` es que normalmente necesitas conocer las URL de antemano:

1. **Brecha de descubrimiento**: Para tareas como &quot;find YC W24 companies&quot;, no sabes qué URL contienen los datos. Necesitarías un paso de búsqueda adicional antes de llamar a `/extract`.
2. **Búsqueda web incómoda**: Aunque existe `enableWebSearch`, está limitado a empezar desde las URL que proporcionas, un flujo de trabajo poco cómodo para tareas de descubrimiento.
3. **Por qué se creó `/agent`**: `/extract` es bueno para extraer desde ubicaciones conocidas, pero es menos eficaz para descubrir dónde se encuentran los datos.

<div id="example">
  ### Ejemplo
</div>

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

<div id="best-use-case-targeted-multi-page-extraction">
  ### Caso de uso ideal: extracción dirigida de múltiples páginas
</div>

**Escenario**: Tienes la URL de la documentación de tu competidor y quieres extraer todos sus endpoints de API de `docs.competitor.com/*`.

**Por qué `/extract` funcionó aquí**: Conocías el dominio exacto. Pero incluso en ese caso, `/agent` con las URL que proporciones suele ofrecer mejores resultados hoy.

Para más detalles, consulta la [documentación de Extract](/es/features/extract).

***

<div id="3-scrape-endpoint-with-json-mode">
  ## 3. Endpoint `/scrape` con modo JSON
</div>

El endpoint `/scrape` con modo JSON es la forma más controlada: extrae datos estructurados de una única URL conocida utilizando un LLM para analizar el contenido de la página y convertirlo al esquema que especifiques.

<div id="key-characteristics">
  ### Características clave
</div>

* **Solo una URL**: Diseñado para extraer datos de una única página específica por vez
* **URL exacta requerida**: Debes conocer la URL precisa que contiene los datos
* **Esquema opcional**: Puede usar un esquema JSON O solo un prompt (el LLM elige la estructura)
* **Sincrónico**: Devuelve los datos inmediatamente (no es necesario consultar el estado de ningún job)
* **Formatos adicionales**: Puede combinar extracción JSON con markdown, HTML y capturas de pantalla en una sola solicitud

<div id="example">
  ### Ejemplo
</div>

<CodeGroup>
  <ScrapeJsonPython />

  <ScrapeJsonNode />

  <ScrapeJsonCURL />
</CodeGroup>

<div id="best-use-case-single-page-precision-extraction">
  ### Mejor caso de uso: extracción precisa en una sola página
</div>

**Escenario**: Estás creando una herramienta de seguimiento de precios y necesitas extraer el precio, el estado de existencias y los detalles del producto de una página de producto específica para la que ya tienes la URL.

**Por qué usar `/scrape` con modo JSON**: Sabes exactamente qué página contiene los datos, necesitas una extracción precisa de una sola página y quieres resultados síncronos sin la sobrecarga de gestionar tareas.

Para más detalles, consulta la [documentación del modo JSON](/es/features/llm-extract).

***

<div id="decision-guide">
  ## Guía de decisión
</div>

**¿Conoces la(s) URL(s) exacta(s) que contienen tus datos?**

* **NO** → Usa `/agent` (descubrimiento web autónomo)
* **SÍ**
  * **¿Una sola página?** → Usa `/scrape` con modo JSON
  * **¿Múltiples páginas?** → Usa `/agent` con URLs (o `/scrape` por lotes)

<div id="recommendations-by-scenario">
  ### Recomendaciones por escenario
</div>

| Escenario | Endpoint recomendado |
|----------|---------------------|
| &quot;Encontrar todas las startups de IA y su financiación&quot; | `/agent` |
| &quot;Extraer datos de esta página de producto específica&quot; | `/scrape` (modo JSON) |
| &quot;Obtener todas las entradas del blog de competitor.com&quot; | `/agent` con URL |
| &quot;Supervisar precios en varias URL conocidas&quot; | `/scrape` con procesamiento por lotes |
| &quot;Investigar empresas en una industria específica&quot; | `/agent` |
| &quot;Extraer información de contacto de 50 páginas de empresas conocidas&quot; | `/scrape` con procesamiento por lotes |

***

<div id="pricing">
  ## Precios
</div>

| Endpoint | Costo | Notas |
|----------|-------|-------|
| `/scrape` (modo JSON) | 1 crédito/página | Fijo, predecible |
| `/extract` | Basado en tokens (1 crédito = 15 tokens) | Variable según el contenido |
| `/agent` | Dinámico | 5 ejecuciones gratuitas/día; varía según la complejidad |

<div id="example-find-the-founders-of-firecrawl">
  ### Ejemplo: &quot;Encuentra a las personas fundadoras de Firecrawl&quot;
</div>

| Endpoint | Cómo funciona | Créditos usados |
|----------|--------------|--------------|
| `/scrape` | Localizas la URL manualmente y luego haces scraping de 1 página | ~1 crédito |
| `/extract` | Proporcionas una o varias URL; extrae datos estructurados | Variable (según tokens) |
| `/agent` | Simplemente envía el prompt: el agente encuentra y extrae | ~15 créditos |

**Trade-off**: `/scrape` es la opción más barata, pero requiere que ya conozcas la URL. `/agent` cuesta más, pero se encarga del descubrimiento automáticamente.

Para obtener precios detallados, consulta [Precios de Firecrawl](https://firecrawl.dev/pricing).

***

<div id="migration-extract-agent">
  ## Migración: `/extract` → `/agent`
</div>

Si actualmente utilizas `/extract`, la migración es directa:

**Antes (extract):**

```python
result = app.extract(
    urls=["https://example.com/*"],
    prompt="Extraer información del producto",
    schema=schema
)
```

**Después (agente):**

```python
result = app.agent(
    urls=["https://example.com"],  # Opcional - se puede omitir por completo
    prompt="Extract product information from example.com",
    schema=schema,
    model="spark-1-mini"  # or "spark-1-pro" for higher accuracy
)
```

La principal ventaja es que, con `/agent`, puedes omitir por completo las URL y simplemente describir lo que necesitas.

***

<div id="key-takeaways">
  ## Puntos clave
</div>

1. **¿Conoces la URL exacta?** Usa `/scrape` con modo JSON: es la opción más económica (1 crédito/página), más rápida (sincrónica) y más predecible.

2. **¿Necesitas investigación autónoma?** Usa `/agent`: gestiona el descubrimiento automáticamente con 5 ejecuciones gratuitas al día y luego precios dinámicos según la complejidad.

3. **Migra de `/extract`** a `/agent` para proyectos nuevos: `/agent` es el sucesor con mejores capacidades.

4. **Equilibrio entre costo y conveniencia**: `/scrape` es lo más rentable cuando conoces tus URL; `/agent` cuesta más, pero elimina el descubrimiento manual de URL.

***

<div id="further-reading">
  ## Lecturas adicionales
</div>

* [Documentación de Agent](/es/features/agent)
* [Modelos de Agent](/es/features/models)
* [Documentación del modo JSON](/es/features/llm-extract)
* [Documentación de Extract](/es/features/extract)
* [Scraping por lotes](/es/features/batch-scrape)