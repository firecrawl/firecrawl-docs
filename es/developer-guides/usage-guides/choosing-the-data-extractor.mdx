---
title: "Cómo elegir el extractor de datos"
description: "Compara /agent, /extract y /scrape (modo JSON) para escoger la herramienta adecuada para extraer datos estructurados"
og:title: "Cómo elegir el extractor de datos | Firecrawl"
og:description: "Compara /agent, /extract y /scrape (modo JSON) para escoger la herramienta adecuada para extraer datos estructurados"
sidebarTitle: "Cómo elegir el extractor de datos"
---

import AgentWithSchemaPython from "/snippets/es/v2/agent/with-schema/python.mdx";
import AgentWithSchemaJS from "/snippets/es/v2/agent/with-schema/js.mdx";
import AgentWithSchemaCURL from "/snippets/es/v2/agent/with-schema/curl.mdx";
import ExtractPython from "/snippets/es/v2/extract/base/python.mdx";
import ExtractNode from "/snippets/es/v2/extract/base/js.mdx";
import ExtractCURL from "/snippets/es/v2/extract/base/curl.mdx";
import ScrapeJsonPython from "/snippets/es/v2/scrape/json/base/python.mdx";
import ScrapeJsonNode from "/snippets/es/v2/scrape/json/base/js.mdx";
import ScrapeJsonCURL from "/snippets/es/v2/scrape/json/base/curl.mdx";

Firecrawl ofrece tres métodos para extraer datos estructurados de páginas web. Cada uno se adapta a distintos casos de uso, con diferentes niveles de automatización y control.

<div id="quick-comparison">
  ## Comparación rápida
</div>

| Función | `/agent` | `/extract` | `/scrape` (modo JSON) |
|---------|----------|------------|----------------------|
| **Estado** | Activo | Usa `/agent` en su lugar | Activo |
| **URL requerida** | No (opcional) | Sí (se admiten comodines) | Sí (URL única) |
| **Alcance** | Descubrimiento en toda la web | Varias páginas/dominios | Una sola página |
| **Descubrimiento de URL** | Búsqueda web autónoma | Rastrea desde las URL dadas | Ninguno |
| **Procesamiento** | Asíncrono | Asíncrono | Síncrono |
| **Esquema requerido** | No (prompt o esquema) | No (prompt o esquema) | No (prompt o esquema) |
| **Precios** | Dinámico (5 ejecuciones gratuitas al día) | Basado en tokens (1 crédito = 15 tokens) | 1 crédito/página |
| **Ideal para** | Investigación, descubrimiento, recopilación compleja | Extracción multipágina (cuando ya conoces las URL) | Extracción de una sola página conocida |

<div id="1-agent-endpoint">
  ## 1. Endpoint `/agent`
</div>

El endpoint `/agent` es la funcionalidad más avanzada de Firecrawl, el sucesor de `/extract`. Utiliza agentes de IA para buscar, navegar y recopilar datos de forma autónoma en toda la web.

<div id="key-characteristics">
  ### Características clave
</div>

* **URLs opcionales**: Solo tienes que describir lo que necesitas mediante el `prompt`; las URLs son completamente opcionales
* **Navegación autónoma**: El agente busca y navega en profundidad por sitios web para encontrar tus datos
* **Búsqueda profunda en la web**: Descubre información de forma autónoma en múltiples dominios y páginas
* **Procesamiento en paralelo**: Procesa múltiples fuentes simultáneamente para obtener resultados más rápidos
* **Modelos disponibles**: `spark-1-mini` (predeterminado, 60% más económico) y `spark-1-pro` (mayor precisión)

<div id="example">
  ### Ejemplo
</div>

<CodeGroup>
  <AgentWithSchemaPython />

  <AgentWithSchemaJS />

  <AgentWithSchemaCURL />
</CodeGroup>

<div id="best-use-case-autonomous-research-discovery">
  ### Caso de uso ideal: investigación y descubrimiento autónomos
</div>

**Escenario**: Necesitas encontrar información sobre startups de IA que hayan obtenido una ronda de financiación Serie A, incluyendo sus fundadores y los montos financiados.

**Por qué `/agent`**: No sabes qué sitios web contienen esta información. El agente buscará de forma autónoma en la web, navegará a fuentes relevantes (Crunchbase, sitios de noticias, páginas de empresas) y recopilará los datos estructurados por ti.

Para más información, consulta la [documentación del agente](/es/features/agent).

***

<div id="2-extract-endpoint">
  ## 2. Endpoint `/extract`
</div>

<Note>
  **Usa `/agent` en su lugar**: Recomendamos migrar a [`/agent`](/es/features/agent): es más rápido, más fiable, no requiere URL y cubre todos los casos de uso de `/extract` y más.
</Note>

El endpoint `/extract` recopila datos estructurados a partir de URL específicas o dominios completos usando extracción basada en LLM.

<div id="key-characteristics">
  ### Características clave
</div>

* **URLs normalmente requeridas**: Proporciona al menos una URL (admite comodines como `example.com/*`)
* **Rastreo de dominio**: Puede rastrear y analizar todas las URLs descubiertas en un dominio
* **Mejora de la búsqueda web**: `enableWebSearch` opcional para seguir enlaces fuera de los dominios especificados
* **Esquema opcional**: Admite un esquema JSON estricto O prompts en lenguaje natural
* **Procesamiento asíncrono**: Devuelve un ID de tarea para comprobar el estado

<div id="the-url-limitation">
  ### La limitación de las URL
</div>

El desafío fundamental con `/extract` es que normalmente necesitas conocer las URL de antemano:

1. **Brecha de descubrimiento**: Para tareas como &quot;find YC W24 companies&quot;, no sabes qué URL contienen los datos. Necesitarías un paso de búsqueda por separado antes de llamar a `/extract`.
2. **Búsqueda web poco práctica**: Aunque existe `enableWebSearch`, está limitado a comenzar desde las URL que proporcionas, lo que resulta en un flujo de trabajo poco práctico para tareas de descubrimiento.
3. **Por qué se creó `/agent`**: `/extract` es bueno para extraer desde ubicaciones conocidas, pero es menos efectivo para descubrir dónde están los datos.

<div id="example">
  ### Ejemplo
</div>

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

<div id="best-use-case-targeted-multi-page-extraction">
  ### Mejor caso de uso: extracción específica de múltiples páginas
</div>

**Escenario**: Tienes la URL de la documentación de tu competidor y quieres extraer todos los endpoints de su API de `docs.competitor.com/*`.

**Por qué `/extract` funcionó aquí**: Conocías el dominio exacto. Pero incluso así, hoy en día `/agent` con URLs proporcionadas normalmente ofrece mejores resultados.

Para más detalles, consulta la [documentación de Extract](/es/features/extract).

***

<div id="3-scrape-endpoint-with-json-mode">
  ## 3. Endpoint `/scrape` con modo JSON
</div>

El endpoint `/scrape` con modo JSON es el enfoque con mayor control: extrae datos estructurados de una única URL conocida usando un LLM para convertir el contenido de la página en el esquema que especifiques.

<div id="key-characteristics">
  ### Características clave
</div>

* **Solo una URL**: Diseñado para extraer datos de una única página específica a la vez
* **URL exacta requerida**: Debes conocer la URL precisa que contiene los datos
* **Esquema opcional**: Puedes usar un esquema JSON o solo un prompt (el LLM elige la estructura)
* **Síncrono**: Devuelve los datos de inmediato (no hace falta hacer polling de jobs)
* **Formatos adicionales**: Puede combinar la extracción en JSON con markdown, HTML y capturas de pantalla en una sola solicitud

<div id="example">
  ### Ejemplo
</div>

<CodeGroup>
  <ScrapeJsonPython />

  <ScrapeJsonNode />

  <ScrapeJsonCURL />
</CodeGroup>

<div id="best-use-case-single-page-precision-extraction">
  ### Caso de uso ideal: extracción precisa de una sola página
</div>

**Escenario**: Estás creando una herramienta de monitoreo de precios y necesitas extraer el precio, la disponibilidad y los detalles del producto de una página de producto específica para la que ya tienes la URL.

**Por qué usar `/scrape` con modo JSON**: Sabes exactamente qué página contiene los datos, necesitas una extracción precisa de una sola página y quieres resultados síncronos sin la sobrecarga de gestionar tareas.

Para más detalles, consulta la [documentación del modo JSON](/es/features/llm-extract).

***

<div id="decision-guide">
  ## Guía de decisiones
</div>

**¿Conoces las URL exactas que contienen tus datos?**

* **NO** → Usa `/agent` (descubrimiento web autónomo)
* **SÍ**
  * **¿Una sola página?** → Usa `/scrape` con modo JSON
  * **¿Múltiples páginas?** → Usa `/agent` con URLs (o `/scrape` por lotes)

<div id="recommendations-by-scenario">
  ### Recomendaciones por escenario
</div>

| Escenario | Endpoint recomendado |
|----------|---------------------|
| &quot;Encontrar todas las startups de IA y su financiación&quot; | `/agent` |
| &quot;Extraer datos de esta página de producto específica&quot; | `/scrape` (modo JSON) |
| &quot;Obtener todas las publicaciones de blog de competitor.com&quot; | `/agent` con URL |
| &quot;Monitorizar precios en múltiples URLs conocidas&quot; | `/scrape` con procesamiento por lotes |
| &quot;Investigar empresas en un sector específico&quot; | `/agent` |
| &quot;Extraer información de contacto de 50 páginas de empresas conocidas&quot; | `/scrape` con procesamiento por lotes |

***

<div id="pricing">
  ## Precios
</div>

| Endpoint | Costo | Notas |
|----------|-------|-------|
| `/scrape` (modo JSON) | 1 crédito/página | Fijo, predecible |
| `/extract` | Basado en tokens (1 crédito = 15 tokens) | Variable según el contenido |
| `/agent` | Dinámico | 5 ejecuciones gratuitas al día; varía según la complejidad |

<div id="example-find-the-founders-of-firecrawl">
  ### Ejemplo: &quot;Encuentra a los fundadores de Firecrawl&quot;
</div>

| Endpoint | Cómo funciona | Créditos usados |
|----------|--------------|--------------|
| `/scrape` | Encuentras la URL manualmente y luego haces scraping de 1 página | ~1 crédito |
| `/extract` | Proporcionas una o varias URL y extrae datos estructurados | Variable (basado en tokens) |
| `/agent` | Solo tienes que enviar el prompt: el agente encuentra y extrae | ~15 créditos |

**Compensación**: `/scrape` es el más barato pero requiere que conozcas la URL. `/agent` cuesta más pero se encarga del descubrimiento automáticamente.

Para ver los precios detallados, consulta [Precios de Firecrawl](https://firecrawl.dev/pricing).

***

<div id="migration-extract-agent">
  ## Migración: `/extract` → `/agent`
</div>

Si actualmente estás utilizando `/extract`, la migración es muy sencilla:

**Antes (extract):**

```python
result = app.extract(
    urls=["https://example.com/*"],
    prompt="Extraer información del producto",
    schema=schema
)
```

**Después (agente):**

```python
result = app.agent(
    urls=["https://example.com"],  # Opcional - se puede omitir por completo
    prompt="Extract product information from example.com",
    schema=schema,
    model="spark-1-mini"  # or "spark-1-pro" for higher accuracy
)
```

La ventaja clave es que, con `/agent`, puedes prescindir por completo de las URL y simplemente describir lo que necesitas.

***

<div id="key-takeaways">
  ## Puntos clave
</div>

1. **¿Sabes la URL exacta?** Usa `/scrape` con modo JSON: es la opción más barata (1 crédito/página), la más rápida (sincrónica) y la más predecible.

2. **¿Necesitas investigación autónoma?** Usa `/agent`: gestiona el descubrimiento automáticamente con 5 ejecuciones gratuitas al día y luego precios dinámicos según la complejidad.

3. **Migra de `/extract`** a `/agent` para proyectos nuevos: `/agent` es el sucesor con mejores capacidades.

4. **Equilibrio entre costo y conveniencia**: `/scrape` es lo más rentable cuando conoces tus URLs; `/agent` cuesta más, pero elimina el descubrimiento manual de URLs.

***

<div id="further-reading">
  ## Lecturas adicionales
</div>

* [Documentación de Agent](/es/features/agent)
* [Modelos de Agent](/es/features/models)
* [Documentación del modo JSON](/es/features/llm-extract)
* [Documentación de Extract](/es/features/extract)
* [Rastreo por lotes](/es/features/batch-scrape)