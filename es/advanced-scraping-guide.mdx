---
title: "Guía avanzada de scraping"
description: "Aprende a mejorar tu scraping con Firecrawl usando opciones avanzadas."
og:title: "Guía avanzada de scraping | Firecrawl"
og:description: "Aprende a mejorar tu scraping con Firecrawl usando opciones avanzadas."
---

Esta guía te mostrará los distintos endpoints de Firecrawl y cómo aprovecharlos al máximo con todos sus parámetros.

<div id="basic-scraping-with-firecrawl">
  ## Scraping básico con Firecrawl
</div>

Para extraer una sola página y obtener contenido limpio en Markdown, puedes usar el endpoint `/scrape`.

<CodeGroup>

```python Python
# pip install firecrawl-py

from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key="fc-YOUR-API-KEY")

doc = firecrawl.scrape("https://firecrawl.dev")

print(doc.markdown)
```

```JavaScript JavaScript
// npm install @mendable/firecrawl-js

import { Firecrawl } from 'firecrawl-js';

const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR-API-KEY' });

const doc = await firecrawl.scrape('https://firecrawl.dev');

console.log(doc.markdown);
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-YOUR-API-KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

</CodeGroup>

<div id="scraping-pdfs">
  ## Extracción de PDF
</div>

Firecrawl admite PDF. Usa la opción `parsers` (por ejemplo, `parsers: ["pdf"]`) cuando quieras asegurar el análisis de PDF.

<div id="scrape-options">
  ## Opciones de extracción
</div>

Al usar el endpoint `/scrape`, puedes personalizar la extracción con las siguientes opciones.

<div id="formats-formats">
  ### Formatos (`formats`)
</div>

- **Tipo**: `array`
- **Cadenas**: `["markdown", "links", "html", "rawHtml", "summary", "images"]`
- **Formatos de objeto**:
  - JSON: `{ type: "json", prompt, schema }`
  - Captura de pantalla: `{ type: "screenshot", fullPage?, quality?, viewport? }`
  - Seguimiento de cambios: `{ type: "changeTracking", modes?, prompt?, schema?, tag? }` (requiere `markdown`)
- **Valor predeterminado**: `["markdown"]`

<div id="full-page-content-vs-main-content-onlymaincontent">
  ### Contenido completo de la página vs contenido principal (`onlyMainContent`)
</div>

- **Tipo**: `boolean`
- **Descripción**: De forma predeterminada, el scraper devuelve solo el contenido principal. Configúralo en `false` para devolver el contenido completo de la página.
- **Predeterminado**: `true`

<div id="include-tags-includetags">
  ### Incluir etiquetas (`includeTags`)
</div>

- **Tipo**: `array`
- **Descripción**: Etiquetas, clases o IDs de HTML que se incluirán en el scraping.

<div id="exclude-tags-excludetags">
  ### Excluir etiquetas (`excludeTags`)
</div>

- **Tipo**: `array`
- **Descripción**: Etiquetas/clases/ids HTML que se excluirán del scraping.

<div id="wait-for-page-readiness-waitfor">
  ### Esperar a que la página esté lista (`waitFor`)
</div>

- **Tipo**: `integer`
- **Descripción**: Milisegundos que esperar antes de hacer scraping (úsalo con moderación).
- **Predeterminado**: `0`

<div id="freshness-and-cache-maxage">
  ### Actualidad y caché (`maxAge`)
</div>

- **Tipo**: `integer` (milisegundos)
- **Descripción**: Si existe una versión en caché de la página más reciente que `maxAge`, Firecrawl la devuelve al instante; de lo contrario, vuelve a extraerla y actualiza la caché. Establece `0` para forzar siempre una obtención en vivo.
- **Predeterminado**: `172800000` (2 días)

<div id="request-timeout-timeout">
  ### Tiempo de espera de la solicitud (`timeout`)
</div>

- **Tipo**: `integer`
- **Descripción**: Duración máxima, en milisegundos, antes de cancelar.
- **Valor predeterminado**: `30000` (30 segundos)

<div id="pdf-parsing-parsers">
  ### Análisis de PDF (`parsers`)
</div>

- **Tipo**: `array`
- **Descripción**: Controla el comportamiento del análisis. Para analizar PDFs, establece `parsers: ["pdf"]`.

<div id="actions-actions">
  ### Acciones (`actions`)
</div>

Al usar el endpoint /scrape, Firecrawl te permite realizar varias acciones en una página web antes de extraer su contenido. Esto es especialmente útil para interactuar con contenido dinámico, navegar por páginas o acceder a contenido que requiere interacción del usuario.

- **Tipo**: `array`
- **Descripción**: Secuencia de pasos del navegador que se ejecutan antes de la extracción.
- **Acciones compatibles**:
    - `wait` `{ milliseconds }`
    - `click` `{ selector }`
    - `write` `{ selector, text }`
    - `press` `{ key }`
    - `scroll` `{ direction: "up" | "down" }`
    - `scrape` `{ selector }` (extrae un subelemento)
    - `executeJavascript` `{ script }`
    - `pdf` (inicia el renderizado de PDF en algunos flujos)

<CodeGroup>

```python Python
from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key='fc-YOUR-API-KEY')

doc = firecrawl.scrape('https://example.com', {
  actions: [
    { type: 'wait', milliseconds: 1000 },
    { type: 'click', selector: '#accept' },
    { type: 'scroll', direction: 'down' },
    { type: 'write', selector: '#q', text: 'firecrawl' },
    { type: 'press', key: 'Enter' }
  ],
  formats: ['markdown']
})

print(doc.markdown)
```

```js Node
import { Firecrawl } from 'firecrawl-js';

const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR-API-KEY' });

const doc = await firecrawl.scrape('https://example.com', {
  actions: [
    { type: 'wait', milliseconds: 1000 },
    { type: 'click', selector: '#accept' },
    { type: 'scroll', direction: 'down' },
    { type: 'write', selector: '#q', text: 'firecrawl' },
    { type: 'press', key: 'Enter' }
  ],
  formats: ['markdown']
});

console.log(doc.markdown);
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "url": "https://example.com",
    "actions": [
      { "type": "wait", "milliseconds": 1000 },
      { "type": "click", "selector": "#accept" },
      { "type": "scroll", "direction": "down" },
      { "type": "write", "selector": "#q", "text": "firecrawl" },
      { "type": "press", "key": "Enter" }
    ],
    "formats": ["markdown"]
  }'
```

</CodeGroup>

<div id="example-usage">
  ### Ejemplo de uso
</div>

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer fc-TU-API-KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "formats": [
        "markdown",
        "links",
        "html",
        "rawHtml",
        { "type": "screenshot", "fullPage": true, "quality": 80 }
      ],
      "includeTags": ["h1", "p", "a", ".main-content"],
      "excludeTags": ["#ad", "#footer"],
      "onlyMainContent": false,
      "waitFor": 1000,
      "timeout": 15000,
      "parsers": ["pdf"]
    }'
```

En este ejemplo, el scraper:

* Devuelve el contenido completo de la página en markdown.
* Incluye el markdown, el HTML sin procesar, el HTML, los enlaces y una captura de pantalla en la respuesta.
* Incluye solo las etiquetas HTML `<h1>`, `<p>`, `<a>` y los elementos con la clase `.main-content`, excluyendo cualquier elemento con los IDs `#ad` y `#footer`.
* Espera 1000 milisegundos (1 segundo) antes de realizar el scraping para permitir que la página cargue.
* Establece la duración máxima de la solicitud de scraping en 15000 milisegundos (15 segundos).
* Analiza PDF explícitamente mediante `parsers: ["pdf"]`.

Referencia de la API: [Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)


<div id="json-extraction-via-formats">
  ## Extracción de JSON mediante formatos
</div>

Usa el objeto de formato JSON en `formats` para extraer datos estructurados en una sola pasada:

```bash
curl -X POST https://api.firecrawl.dev/v2/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "url": "https://firecrawl.dev",
    "formats": [{
      "type": "json",
      "prompt": "Extrae las funciones del producto",
      "schema": {"type": "object", "properties": {"features": {"type": "object"}}, "required": ["features"]}
    }]
  }'
```


<div id="extract-endpoint">
  ## Endpoint /extract
</div>

Usa la API dedicada de trabajos de extracción cuando necesites extracción asíncrona con sondeo de estado.

<CodeGroup>

```js Node
import Firecrawl from '@mendable/firecrawl-js';

const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR-API-KEY' });

// Iniciar trabajo de extracción
const started = await firecrawl.startExtract({
  urls: ['https://docs.firecrawl.dev'],
  prompt: 'Extract title',
  schema: { type: 'object', properties: { title: { type: 'string' } }, required: ['title'] }
});

// Consultar estado
const status = await firecrawl.getExtractStatus(started.id);
console.log(status.status, status.data);
```

```python Python
from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key='fc-YOUR-API-KEY')

# Iniciar trabajo de extracción
started = firecrawl.start_extract(
    urls=["https://docs.firecrawl.dev"],
    prompt="Extract title",
    schema={"type": "object", "properties": {"title": {"type": "string"}}, "required": ["title"]}
)
# Consultar estado
status = firecrawl.get_extract_status(started.id)
print(status.get("status"), status.get("data"))
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/extract \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "urls": ["https://docs.firecrawl.dev"],
    "prompt": "Extract title",
    "schema": {"type": "object", "properties": {"title": {"type": "string"}}, "required": ["title"]}
  }'
```
</CodeGroup>

<div id="crawling-multiple-pages">
  ## Rastreo de múltiples páginas
</div>

Para rastrear varias páginas, usa el endpoint `/v2/crawl`.

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-TU-CLAVE-DE-API' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

Devuelve un ID

```json
{ "id": "1234-5678-9101" }
```


<div id="check-crawl-job">
  ### Consultar trabajo de rastreo
</div>

Se utiliza para verificar el estado de un trabajo de rastreo y obtener su resultado.

```bash cURL
curl -X GET https://api.firecrawl.dev/v2/crawl/1234-5678-9101 \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-TU-API-KEY'
```


<div id="paginationnext-url">
  #### Paginación/URL siguiente
</div>

Si el contenido supera los 10 MB o si el trabajo de rastreo aún se está ejecutando, la respuesta puede incluir un parámetro `next`, que es una URL a la siguiente página de resultados.

<div id="crawl-prompt-and-params-preview">
  ### Vista previa del prompt y parámetros de rastreo
</div>

Puedes proporcionar un `prompt` en lenguaje natural para que Firecrawl derive la configuración de rastreo. Primero, prévisualízala:

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/crawl/params-preview \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "url": "https://docs.firecrawl.dev",
    "prompt": "Extrae la documentación y el blog"
  }'
```


<div id="crawler-options">
  ### Opciones del rastreador
</div>

Al usar el endpoint `/v2/crawl`, puedes personalizar el comportamiento del rastreo con:

<div id="includepaths">
  #### includePaths
</div>

- **Tipo**: `array`
- **Descripción**: Patrones de expresiones regulares a incluir.
- **Ejemplo**: `["^/blog/.*$", "^/docs/.*$"]`

<div id="excludepaths">
  #### excludePaths
</div>

- **Tipo**: `array`
- **Descripción**: Patrones de expresiones regulares para excluir.
- **Ejemplo**: `["^/admin/.*$", "^/private/.*$"]`

<div id="maxdiscoverydepth">
  #### maxDiscoveryDepth
</div>

- **Tipo**: `integer`
- **Descripción**: Profundidad máxima de descubrimiento para encontrar nuevas URLs.

<div id="limit">
  #### limit
</div>

- **Tipo**: `integer`
- **Descripción**: Número máximo de páginas a rastrear.
- **Valor predeterminado**: `10000`

<div id="crawlentiredomain">
  #### crawlEntireDomain
</div>

- **Type**: `boolean`
- **Description**: Explorar a través de páginas hermanas y superiores para cubrir todo el dominio.
- **Default**: `false`

<div id="allowexternallinks">
  #### allowExternalLinks
</div>

- **Tipo**: `boolean`
- **Descripción**: Seguir enlaces a dominios externos.
- **Valor predeterminado**: `false`

<div id="allowsubdomains">
  #### allowSubdomains
</div>

- **Tipo**: `boolean`
- **Descripción**: Seguir los subdominios del dominio principal.
- **Valor predeterminado**: `false`

<div id="delay">
  #### delay
</div>

- **Tipo**: `number`
- **Descripción**: Retraso en segundos entre scrapes.
- **Predeterminado**: `undefined`

<div id="scrapeoptions">
  #### scrapeOptions
</div>

- **Tipo**: `object`
- **Descripción**: Opciones para el scraper (consulta los formatos arriba).
- **Ejemplo**: `{ "formats": ["markdown", "links", {"type": "screenshot", "fullPage": true}], "includeTags": ["h1", "p", "a", ".main-content"], "excludeTags": ["#ad", "#footer"], "onlyMainContent": false, "waitFor": 1000, "timeout": 15000}`
- **Valores predeterminados**: `formats: ["markdown"]`, caché habilitada de forma predeterminada (maxAge ~ 2 días)

<div id="example-usage">
  ### Ejemplo de uso
</div>

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-TU-API-KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "includePaths": ["^/blog/.*$", "^/docs/.*$"],
      "excludePaths": ["^/admin/.*$", "^/private/.*$"],
      "maxDiscoveryDepth": 2,
      "limit": 1000
    }'
```


<div id="mapping-website-links">
  ## Mapeo de enlaces de un sitio web
</div>

El endpoint `/v2/map` identifica las URL relacionadas con un sitio web determinado.

<div id="usage">
  ### Uso
</div>

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/map \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-TU-API-KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```


<div id="map-options">
  ### Opciones de mapeo
</div>

<div id="search">
  #### search
</div>

- **Tipo**: `string`
- **Descripción**: Filtra enlaces que contengan el texto.

<div id="limit">
  #### limit
</div>

- **Tipo**: `integer`
- **Descripción**: Número máximo de enlaces a retornar.
- **Valor por defecto**: `100`

<div id="sitemap">
  #### sitemap
</div>

- **Tipo**: `"only" | "include" | "skip"`
- **Descripción**: Controla el uso del sitemap durante el mapeo.
- **Predeterminado**: `"include"`

<div id="includesubdomains">
  #### includeSubdomains
</div>

- **Tipo**: `boolean`
- **Descripción**: Incluye los subdominios del sitio web.
- **Valor predeterminado**: `true`

Consulta la referencia de la API: [Documentación del endpoint /map](https://docs.firecrawl.dev/api-reference/endpoint/map)

¡Gracias por leer!