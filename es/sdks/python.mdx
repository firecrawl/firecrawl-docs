---
title: "Python"
description: "El SDK de Python de Firecrawl es un envoltorio de la API de Firecrawl que te ayuda a convertir sitios web en Markdown fácilmente."
icon: "python"
og:title: "SDK de Python | Firecrawl"
og:description: "El SDK de Python de Firecrawl es un envoltorio de la API de Firecrawl que te ayuda a convertir sitios web en Markdown fácilmente."
---

import InstallationPython from '/snippets/es/v2/installation/python.mdx'
import ScrapePythonShort from '/snippets/v2/scrape/short/python.mdx'
import CrawlPythonShort from '/snippets/es/v2/crawl/short/python.mdx'
import CheckCrawlStatusPythonShort from '/snippets/es/v2/crawl-status/short/python.mdx'
import StartCrawlPythonShort from '/snippets/v2/start-crawl/short/python.mdx'
import CancelCrawlPythonShort from '/snippets/es/v2/crawl-delete/short/python.mdx'
import MapPythonShort from '/snippets/v2/map/short/python.mdx'
import ExtractPythonShort from '/snippets/v2/extract/short/python.mdx'
import ScrapeAndCrawlExamplePython from '/snippets/es/v2/scrape-and-crawl/python.mdx'
import CrawlWebSocketPythonBase from '/snippets/es/v2/crawl-websocket/base/python.mdx'
import AIOPython from '/snippets/es/v2/async/base/python.mdx'

<div id="installation">
  ## Instalación
</div>

Para instalar el SDK de Python de Firecrawl, puedes usar pip:

<InstallationPython />

<div id="usage">
  ## Uso
</div>

1. Obtén una clave de API en [firecrawl.dev](https://firecrawl.dev)
2. Define la clave de API como una variable de entorno llamada `FIRECRAWL_API_KEY` o pásala como parámetro a la clase `Firecrawl`.

Aquí tienes un ejemplo de cómo usar el SDK:

<ScrapeAndCrawlExamplePython />

<div id="scraping-a-url">
  ### Extracción de una URL
</div>

Para extraer una sola URL, usa el método `scrape`. Recibe la URL como parámetro y devuelve el documento extraído.

<ScrapePythonShort />

<div id="crawl-a-website">
  ### Rastrear un sitio web
</div>

Para rastrear un sitio web, usa el método `crawl`. Recibe la URL inicial y opciones opcionales como argumentos. Estas opciones te permiten definir ajustes adicionales para el trabajo de rastreo, como el número máximo de páginas a rastrear, los dominios permitidos y el formato de salida. Consulta [Paginación](#pagination) para la paginación automática/manual y los límites.

<CrawlPythonShort />

<div id="start-a-crawl">
  ### Iniciar un rastreo
</div>

<Tip>¿Prefieres no bloquear? Consulta la sección [Clase asíncrona](#async-class) a continuación.</Tip>

Inicia un trabajo sin esperar con `start_crawl`. Devuelve un `ID` de trabajo que puedes usar para consultar el estado. Usa `crawl` cuando quieras un “waiter” que bloquee hasta completarse. Consulta [Paginación](#pagination) para el comportamiento y los límites de paginado.

<StartCrawlPythonShort />

<div id="checking-crawl-status">
  ### Comprobar el estado del rastreo
</div>

Para comprobar el estado de un job de rastreo, usa el método `get_crawl_status`. Recibe el ID del job como parámetro y devuelve el estado actual del rastreo.

<CheckCrawlStatusPythonShort />

<div id="cancelling-a-crawl">
  ### Cancelar un rastreo
</div>

Para cancelar un trabajo de rastreo, usa el método `cancel_crawl`. Recibe el ID del trabajo iniciado con `start_crawl` como parámetro y devuelve el estado de la cancelación.

<CancelCrawlPythonShort />

<div id="map-a-website">
  ### Mapear un sitio web
</div>

Usa `map` para generar una lista de URL de un sitio web. Las opciones te permiten personalizar el proceso de mapeo, como excluir subdominios o aprovechar el sitemap.

<MapPythonShort />

{/* ### Extracción de datos estructurados de sitios web

  Para extraer datos estructurados de sitios web, usa el método `extract`. Recibe las URL de las que extraer datos, un prompt y un esquema como argumentos. El esquema es un modelo de Pydantic que define la estructura de los datos extraídos.

  <ExtractPythonShort /> */}

<div id="crawling-a-website-with-websockets">
  ### Rastreo de un sitio web con WebSockets
</div>

Para rastrear un sitio web con WebSockets, inicia el trabajo con `start_crawl` y suscríbete usando el helper `watcher`. Crea un watcher con el ID del trabajo y adjunta handlers (p. ej., para page, completed, failed) antes de llamar a `start()`.

<CrawlWebSocketPythonBase />

<div id="pagination">
  ### Paginación
</div>

Los puntos de conexión de Firecrawl para crawl y batch devuelven una URL `next` cuando hay más datos disponibles. El SDK de Python paginá automáticamente por defecto y agrega todos los documentos; en ese caso `next` será `None`. Puedes desactivar la paginación automática o establecer límites.

<div id="crawl">
  #### Rastreo
</div>

Usa el método auxiliar `crawl` para la forma más sencilla, o inicia un job y pagina manualmente.

<div id="simple-crawl-auto-pagination-default">
  ##### Rastreo simple (paginación automática, por defecto)
</div>

* Consulta el flujo por defecto en [Rastrear un sitio web](#crawl-a-website).

<div id="manual-crawl-with-pagination-control-single-page">
  ##### Rastreo manual con control de paginación (una sola página)
</div>

* Inicia un job y luego obtén una página a la vez con `auto_paginate=False`.

```python Python
crawl_job = client.start_crawl("https://example.com", limit=100)

status = client.get_crawl_status(crawl_job.id, pagination_config=PaginationConfig(auto_paginate=False))
print("rastrear una sola página:", status.status, "docs:", len(status.data), "siguiente:", status.next)
```

<div id="manual-crawl-with-limits-auto-pagination-early-stop">
  ##### Rastreo manual con límites (paginación automática + detención anticipada)
</div>

* Mantén la paginación automática activada, pero detén antes con `max_pages`, `max_results` o `max_wait_time`.

```python Python
status = client.get_crawl_status(
    crawl_job.id,
    pagination_config=PaginationConfig(max_pages=2, max_results=50, max_wait_time=15),
)
print("rastreo limitado:", status.status, "docs:", len(status.data), "siguiente:", status.next)
```

<div id="batch-scrape">
  #### Extracción por lotes
</div>

Usa el método de espera `batch_scrape` o inicia un job y pagina manualmente.

<div id="simple-batch-scrape-auto-pagination-default">
  ##### Raspado por lotes simple (paginación automática, por defecto)
</div>

* Consulta el flujo por defecto en [Batch Scrape](/es/features/batch-scrape).

<div id="manual-batch-scrape-with-pagination-control-single-page">
  ##### Raspado manual por lotes con control de paginación (una sola página)
</div>

* Inicia un trabajo y luego obtén una página a la vez con `auto_paginate=False`.

```python Python
batch_job = client.start_batch_scrape(urls)
status = client.get_batch_scrape_status(batch_job.id, pagination_config=PaginationConfig(auto_paginate=False))
print("lote: una sola página:", status.status, "docs:", len(status.data), "siguiente:", status.next)
```

<div id="manual-batch-scrape-with-limits-auto-pagination-early-stop">
  ##### Raspado manual por lotes con límites (paginación automática + detención anticipada)
</div>

* Mantén la paginación automática activada, pero detén antes con `max_pages`, `max_results` o `max_wait_time`.

```python Python
status = client.get_batch_scrape_status(
    batch_job.id,
    pagination_config=PaginationConfig(max_pages=2, max_results=100, max_wait_time=20),
)
print("lote limitado:", status.status, "docs:", len(status.data), "siguiente:", status.next)
```

<div id="error-handling">
  ## Manejo de errores
</div>

El SDK gestiona los errores que devuelve la API de Firecrawl y genera las excepciones correspondientes. Si se produce un error durante una solicitud, se lanzará una excepción con un mensaje descriptivo.

<div id="async-class">
  ## Clase asíncrona
</div>

Para operaciones asíncronas, utiliza la clase `AsyncFirecrawl`. Sus métodos son equivalentes a los de `Firecrawl`, pero no bloquean el hilo principal.

<AIOPython />