---
title: 'CLI'
description: 'Firecrawl CLI es una interfaz de línea de comandos para realizar scraping, crawling, mapeo y búsqueda en la web directamente desde tu terminal.'
icon: 'terminal'
og:title: "CLI | Firecrawl"
og:description: "Firecrawl CLI es una interfaz de línea de comandos para realizar scraping, crawling, mapeo y búsqueda en la web directamente desde tu terminal."
---

import InstallationCLI from '/snippets/es/v2/cli/installation/bash.mdx'
import AuthLogin from '/snippets/es/v2/cli/auth/login.mdx'
import AuthLogout from '/snippets/es/v2/cli/auth/logout.mdx'
import AuthConfig from '/snippets/es/v2/cli/auth/config.mdx'
import ScrapeBasic from '/snippets/es/v2/cli/scrape/basic.mdx'
import ScrapeFormats from '/snippets/es/v2/cli/scrape/formats.mdx'
import ScrapeOptions from '/snippets/es/v2/cli/scrape/options.mdx'
import CrawlBasic from '/snippets/es/v2/cli/crawl/basic.mdx'
import CrawlStatus from '/snippets/es/v2/cli/crawl/status.mdx'
import CrawlOptions from '/snippets/es/v2/cli/crawl/options.mdx'
import MapBasic from '/snippets/es/v2/cli/map/basic.mdx'
import MapOptions from '/snippets/es/v2/cli/map/options.mdx'
import SearchBasic from '/snippets/es/v2/cli/search/basic.mdx'
import SearchOptions from '/snippets/es/v2/cli/search/options.mdx'


<div id="installation">
  ## Instalación
</div>

Instala la CLI de Firecrawl de forma global con npm:

<InstallationCLI />

Si lo estás usando en un agente de IA como Claude Code, puedes instalar la habilidad con:

```bash
npx skills add firecrawl/cli
```


<div id="authentication">
  ## Autenticación
</div>

Antes de usar la CLI, primero debes autenticarte con tu clave de API de Firecrawl.

<div id="login">
  ### Inicio de sesión
</div>

<AuthLogin />

<div id="view-configuration">
  ### Ver la configuración
</div>

<AuthConfig />

<div id="logout">
  ### Cerrar sesión
</div>

<AuthLogout />

<div id="commands">
  ## Comandos
</div>

<div id="scrape">
  ### Scrape
</div>

Extrae el contenido de una única URL en distintos formatos.

<ScrapeBasic />

<div id="output-formats">
  #### Formatos de salida
</div>

<ScrapeFormats />

<div id="scrape-options">
  #### Opciones de Scrape
</div>

<ScrapeOptions />

**Opciones disponibles:**

| Opción | Alias | Descripción |
|--------|-------|-------------|
| `--url <url>` | `-u` | URL para hacer scrape (alternativa al argumento posicional) |
| `--format <formats>` | `-f` | Formatos de salida (separados por comas): `markdown`, `html`, `rawHtml`, `links`, `images`, `screenshot`, `json` |
| `--html` | `-H` | Atajo para `--format html` |
| `--only-main-content` | | Extraer solo el contenido principal |
| `--wait-for <ms>` | | Tiempo de espera en milisegundos para el renderizado de JS |
| `--screenshot` | | Tomar una captura de pantalla |
| `--include-tags <tags>` | | Etiquetas HTML a incluir (separadas por comas) |
| `--exclude-tags <tags>` | | Etiquetas HTML a excluir (separadas por comas) |
| `--output <path>` | `-o` | Guardar la salida en un archivo |
| `--pretty` | | Imprimir el JSON de salida con formato |

---

<div id="crawl">
  ### Rastrear
</div>

Rastrea todo un sitio web a partir de una URL.

<CrawlBasic />

<div id="check-crawl-status">
  #### Consultar el estado del rastreo
</div>

<CrawlStatus />

<div id="crawl-options">
  #### Opciones de rastreo
</div>

<CrawlOptions />

**Opciones disponibles:**

| Opción | Descripción |
|--------|-------------|
| `--url <url>` | URL a rastrear (alternativa al argumento posicional) |
| `--wait` | Esperar a que el rastreo termine |
| `--progress` | Mostrar indicador de progreso mientras se espera |
| `--poll-interval <seconds>` | Intervalo de sondeo (por defecto: 5) |
| `--timeout <seconds>` | Tiempo máximo de espera |
| `--status` | Consultar el estado de un trabajo de rastreo existente |
| `--limit <number>` | Número máximo de páginas a rastrear |
| `--max-depth <number>` | Profundidad máxima de rastreo |
| `--include-paths <paths>` | Rutas a incluir (separadas por comas) |
| `--exclude-paths <paths>` | Rutas a excluir (separadas por comas) |
| `--allow-subdomains` | Incluir subdominios |
| `--allow-external-links` | Seguir enlaces externos |
| `--output <path>` | Guardar el resultado en un archivo |
| `--pretty` | Imprimir la salida JSON con formato legible |

---

<div id="map">
  ### Map
</div>

Obtén rápidamente todas las URL de un sitio web.

<MapBasic />

<div id="map-options">
  #### Opciones de mapeo
</div>

<MapOptions />

**Opciones disponibles:**

| Opción | Descripción |
|--------|-------------|
| `--url <url>` | URL que se va a mapear (alternativa al argumento posicional) |
| `--limit <number>` | Número máximo de URLs a descubrir |
| `--search <query>` | Filtrar URLs por consulta de búsqueda |
| `--sitemap <mode>` | Manejo del sitemap: `include`, `skip`, `only` |
| `--include-subdomains` | Incluir subdominios |
| `--ignore-query-parameters` | Tratar URLs con distintos parámetros de consulta como la misma |
| `--json` | Salida en formato JSON |
| `--output <path>` | Guardar la salida en un archivo |
| `--pretty` | Imprimir la salida JSON con formato legible |

---

<div id="search">
  ### Buscar
</div>

Busca en la web y, opcionalmente, hace scraping de los resultados.

<SearchBasic />

<div id="search-options">
  #### Opciones de búsqueda
</div>

<SearchOptions />

**Opciones disponibles:**

| Opción | Descripción |
|--------|-------------|
| `--limit <number>` | Número máximo de resultados (predeterminado: 5, máximo: 100) |
| `--sources <sources>` | Fuentes de búsqueda: `web`, `images`, `news` (separadas por comas) |
| `--categories <categories>` | Filtrar por categoría: `github`, `research`, `pdf` (separadas por comas) |
| `--tbs <value>` | Filtro de tiempo: `qdr:h` (hora), `qdr:d` (día), `qdr:w` (semana), `qdr:m` (mes), `qdr:y` (año) |
| `--location <location>` | Segmentación geográfica (p. ej., "Berlin,Germany") |
| `--country <code>` | Código de país ISO (predeterminado: US) |
| `--scrape` | Extraer resultados de búsqueda |
| `--scrape-formats <formats>` | formatos para contenido extraído (predeterminado: markdown) |
| `--only-main-content` | Incluir solo el contenido principal al extraer |
| `--json` | Salida como JSON |
| `--output <path>` | Guardar la salida en un archivo |
| `--pretty` | Imprimir salida JSON con formato legible |

---

<div id="credit-usage">
  ### Uso de créditos
</div>

Consulta el saldo y el uso de créditos de tu equipo.

```bash CLI
# View credit usage
firecrawl credit-usage

# Salida en formato JSON
firecrawl credit-usage --json --pretty
```

***


<div id="version">
  ### Versión
</div>

Mostrar la versión de la CLI.

```bash CLI
firecrawl version
# o
firecrawl --version
```


<div id="global-options">
  ## Opciones globales
</div>

Estas opciones están disponibles para todos los comandos:

| Opción | Abrev. | Descripción |
|--------|--------|-------------|
| `--api-key <key>` | `-k` | Sobrescribe la clave de API almacenada para este comando |
| `--help` | `-h` | Muestra la ayuda para un comando |
| `--version` | `-V` | Muestra la versión de la CLI |

<div id="output-handling">
  ## Manejo de la salida
</div>

La CLI envía la salida a stdout de forma predeterminada, lo que facilita usarla en pipes o redirigirla:

```bash CLI
# Canalizar markdown a otro comando
firecrawl https://example.com | head -50

# Redirigir a un archivo
firecrawl https://example.com > output.md

# Guardar JSON con formato legible
firecrawl https://example.com --format markdown,links --pretty -o data.json
```


<div id="examples">
  ## Ejemplos
</div>

<div id="quick-scrape">
  ### Scrape rápido
</div>

```bash CLI
# Obtener contenido markdown de una URL
firecrawl https://docs.firecrawl.dev

# Get HTML content
firecrawl https://example.com --html -o page.html
```


<div id="full-site-crawl">
  ### Rastreo completo del sitio web
</div>

```bash CLI
# Rastrear un sitio de documentación con límites
firecrawl crawl https://docs.example.com --limit 50 --max-depth 2 --wait --progress -o docs.json
```


<div id="site-discovery">
  ### Descubrimiento de sitios web
</div>

```bash CLI
# Buscar todas las publicaciones del blog
firecrawl map https://example.com --search "blog" -o blog-urls.txt
```


<div id="research-workflow">
  ### Flujo de trabajo de investigación
</div>

```bash CLI
# Buscar y extraer resultados para investigación
firecrawl search "machine learning best practices 2024" --scrape --scrape-formats markdown --pretty
```
