---
title: 'Skill + CLI'
description: 'Firecrawl Skill es una forma sencilla de hacer que los agentes de IA como Claude Code, Antigravity y OpenCode usen Firecrawl a trav√©s de la CLI.'
og:title: "CLI | Firecrawl"
og:description: "Firecrawl Skills es una forma sencilla de hacer que los agentes de IA usen Firecrawl a trav√©s de la CLI. Los agentes de IA pueden obtener datos web mediante una interfaz mejor y m√°s eficiente en el uso del contexto."
---

import InstallationCLI from '/snippets/es/v2/cli/installation/bash.mdx'
import AuthLogin from '/snippets/es/v2/cli/auth/login.mdx'
import AuthLogout from '/snippets/es/v2/cli/auth/logout.mdx'
import AuthConfig from '/snippets/es/v2/cli/auth/config.mdx'
import AuthSelfHosted from '/snippets/es/v2/cli/auth/self-hosted.mdx'
import ScrapeBasic from '/snippets/es/v2/cli/scrape/basic.mdx'
import ScrapeFormats from '/snippets/es/v2/cli/scrape/formats.mdx'
import ScrapeOptions from '/snippets/es/v2/cli/scrape/options.mdx'
import CrawlBasic from '/snippets/es/v2/cli/crawl/basic.mdx'
import CrawlStatus from '/snippets/es/v2/cli/crawl/status.mdx'
import CrawlOptions from '/snippets/es/v2/cli/crawl/options.mdx'
import MapBasic from '/snippets/es/v2/cli/map/basic.mdx'
import MapOptions from '/snippets/es/v2/cli/map/options.mdx'
import SearchBasic from '/snippets/es/v2/cli/search/basic.mdx'
import SearchOptions from '/snippets/es/v2/cli/search/options.mdx'
import AgentBasic from '/snippets/es/v2/cli/agent/basic.mdx'
import AgentOptions from '/snippets/es/v2/cli/agent/options.mdx'
import BrowserBasic from '/snippets/es/v2/browser/cli/basic.mdx'
import BrowserOptions from '/snippets/es/v2/browser/cli/options.mdx'


<div id="installation">
  ## Instalaci√≥n
</div>

Instala la CLI de Firecrawl de forma global con npm:

<InstallationCLI />

Si lo est√°s usando en un agente de IA como Claude Code, puedes instalar la habilidad de Firecrawl a continuaci√≥n y el agente podr√° configurarla por ti.

```bash
npx -y firecrawl-cli init --all --browser
```

<Note>
  Despu√©s de instalar la habilidad, reinicia Claude Code para que detecte la nueva habilidad.
</Note>


<div id="authentication">
  ## Autenticaci√≥n
</div>

Antes de usar la CLI, primero debes autenticarte con tu clave de API de Firecrawl.

<div id="login">
  ### Inicio de sesi√≥n
</div>

<AuthLogin />

<div id="view-configuration">
  ### Ver la configuraci√≥n
</div>

<AuthConfig />

<div id="logout">
  ### Cerrar sesi√≥n
</div>

<AuthLogout />

<div id="self-hosted-local-development">
  ### Autohospedado / Desarrollo local
</div>

Para instancias de Firecrawl autohospedadas o desarrollo local, usa la opci√≥n `--api-url`:

<AuthSelfHosted />

Cuando uses una URL de la API personalizada (cualquier valor distinto de `https://api.firecrawl.dev`), la autenticaci√≥n con clave de API se omite autom√°ticamente, lo que te permite usar instancias locales sin una clave de API.

<div id="check-status">
  ### Comprobar estado
</div>

Comprueba la instalaci√≥n, la autenticaci√≥n y consulta los l√≠mites de velocidad (rate limits):

```bash CLI
firecrawl --status
```

Genera la salida cuando est√© listo:

```
  üî• firecrawl cli v1.1.1

  ‚óè Authenticated via FIRECRAWL_API_KEY
  Concurrency: 0/100 jobs (parallel scrape limit)
  Credits: 500,000 remaining
```

* **Concurrencia**: M√°ximo de tareas en paralelo. Ejecuta operaciones en paralelo lo m√°s cerca posible de este l√≠mite, pero sin superarlo.
* **Cr√©ditos**: Cr√©ditos de API restantes. Cada operaci√≥n de scrape/crawl consume cr√©ditos.


<div id="commands">
  ## Comandos
</div>

<div id="scrape">
  ### Scrape
</div>

Extrae el contenido de una √∫nica URL en distintos formatos.

<Tip>
Usa `--only-main-content` para obtener una salida limpia sin navegaci√≥n, pies de p√°gina ni anuncios. Se recomienda para la mayor√≠a de los casos de uso en los que solo quieres el art√≠culo o el contenido principal de la p√°gina.
</Tip>

<ScrapeBasic />

<div id="output-formats">
  #### Formatos de salida
</div>

<ScrapeFormats />

<div id="scrape-options">
  #### Opciones de Scrape
</div>

<ScrapeOptions />

**Opciones disponibles:**

| Opci√≥n | Alias | Descripci√≥n |
|--------|-------|-------------|
| `--url <url>` | `-u` | URL para hacer scrape (alternativa al argumento posicional) |
| `--format <formats>` | `-f` | Formatos de salida (separados por comas): `markdown`, `html`, `rawHtml`, `links`, `screenshot`, `json`, `images`, `summary`, `seguimientoDeCambios`, `attributes`, `branding` |
| `--html` | `-H` | Atajo para `--format html` |
| `--only-main-content` | | Extraer solo el contenido principal |
| `--wait-for <ms>` | | Tiempo de espera en milisegundos para el renderizado de JS |
| `--screenshot` | | Tomar una captura de pantalla |
| `--include-tags <tags>` | | Etiquetas HTML a incluir (separadas por comas) |
| `--exclude-tags <tags>` | | Etiquetas HTML a excluir (separadas por comas) |
| `--output <path>` | `-o` | Guardar la salida en un archivo |
| `--json` | | Forzar salida JSON incluso con un solo formato |
| `--pretty` | | Imprimir el JSON de salida con formato |
| `--timing` | | Mostrar el tiempo de la solicitud y otra informaci√≥n √∫til |

---

<div id="search">
  ### Buscar
</div>

Busca en la web y, opcionalmente, hace scraping de los resultados.

<SearchBasic />

<div id="search-options">
  #### Opciones de b√∫squeda
</div>

<SearchOptions />

**Opciones disponibles:**

| Opci√≥n | Descripci√≥n |
|--------|-------------|
| `--limit <number>` | N√∫mero m√°ximo de resultados (predeterminado: 5, m√°ximo: 100) |
| `--sources <sources>` | Fuentes de b√∫squeda: `web`, `images`, `news` (separadas por comas) |
| `--categories <categories>` | Filtrar por categor√≠a: `github`, `research`, `pdf` (separadas por comas) |
| `--tbs <value>` | Filtro de tiempo: `qdr:h` (hora), `qdr:d` (d√≠a), `qdr:w` (semana), `qdr:m` (mes), `qdr:y` (a√±o) |
| `--location <location>` | Segmentaci√≥n geogr√°fica (p. ej., "Berlin,Germany") |
| `--country <code>` | C√≥digo de pa√≠s ISO (predeterminado: US) |
| `--timeout <ms>` | Tiempo de espera en milisegundos (predeterminado: 60000) |
| `--ignore-invalid-urls` | Excluir URLs no v√°lidas para otros endpoints de Firecrawl |
| `--scrape` | Extraer resultados de b√∫squeda |
| `--scrape-formats <formats>` | formatos para contenido extra√≠do (predeterminado: markdown) |
| `--only-main-content` | Incluir solo el contenido principal al extraer (predeterminado: true) |
| `--json` | Salida como JSON |
| `--output <path>` | Guardar la salida en un archivo |
| `--pretty` | Imprimir salida JSON con formato legible |

---

<div id="map">
  ### Map
</div>

Obt√©n r√°pidamente todas las URL de un sitio web.

<MapBasic />

<div id="map-options">
  #### Opciones de mapeo
</div>

<MapOptions />

**Opciones disponibles:**

| Opci√≥n | Descripci√≥n |
|--------|-------------|
| `--url <url>` | URL que se va a mapear (alternativa al argumento posicional) |
| `--limit <number>` | N√∫mero m√°ximo de URLs a descubrir |
| `--search <query>` | Filtrar URLs por consulta de b√∫squeda |
| `--sitemap <mode>` | Manejo del sitemap: `include`, `skip`, `only` |
| `--include-subdomains` | Incluir subdominios |
| `--ignore-query-parameters` | Tratar URLs con distintos par√°metros de consulta como la misma |
| `--wait` | Esperar a que finalice el mapeo |
| `--timeout <seconds>` | Tiempo de espera en segundos |
| `--json` | Salida en formato JSON |
| `--output <path>` | Guardar la salida en un archivo |
| `--pretty` | Imprimir la salida JSON con formato legible |

---

<div id="crawl">
  ### Rastrear
</div>

Rastrea todo un sitio web a partir de una URL.

<CrawlBasic />

<div id="check-crawl-status">
  #### Consultar el estado del rastreo
</div>

<CrawlStatus />

<div id="crawl-options">
  #### Opciones de rastreo
</div>

<CrawlOptions />

**Opciones disponibles:**

| Opci√≥n | Descripci√≥n |
|--------|-------------|
| `--url <url>` | URL a rastrear (alternativa al argumento posicional) |
| `--wait` | Esperar a que el rastreo termine |
| `--progress` | Mostrar indicador de progreso mientras se espera |
| `--poll-interval <seconds>` | Intervalo de sondeo (por defecto: 5) |
| `--timeout <seconds>` | Tiempo m√°ximo de espera |
| `--status` | Consultar el estado de un trabajo de rastreo existente |
| `--limit <number>` | N√∫mero m√°ximo de p√°ginas a rastrear |
| `--max-depth <number>` | Profundidad m√°xima de rastreo |
| `--include-paths <paths>` | Rutas a incluir (separadas por comas) |
| `--exclude-paths <paths>` | Rutas a excluir (separadas por comas) |
| `--sitemap <mode>` | Manejo del sitemap: `include`, `skip`, `only` |
| `--allow-subdomains` | Incluir subdominios |
| `--allow-external-links` | Seguir enlaces externos |
| `--crawl-entire-domain` | Rastrear todo el dominio |
| `--ignore-query-parameters` | Considerar URLs con distintos par√°metros como iguales |
| `--delay <ms>` | Retraso entre solicitudes |
| `--max-concurrency <n>` | N√∫mero m√°ximo de solicitudes concurrentes |
| `--output <path>` | Guardar el resultado en un archivo |
| `--pretty` | Imprimir la salida JSON con formato legible |

---

<div id="agent">
  ### Agente
</div>

Busca y recopila datos de la web usando indicaciones en lenguaje natural.

<AgentBasic />

<div id="agent-options">
  #### Opciones del agente
</div>

<AgentOptions />

**Opciones disponibles:**

| Opci√≥n | Descripci√≥n |
|--------|-------------|
| `--urls <urls>` | Lista opcional de URL en las que enfocar el agente (separadas por comas) |
| `--model <model>` | Modelo a utilizar: `spark-1-mini` (predeterminado, 60% m√°s barato) o `spark-1-pro` (mayor precisi√≥n) |
| `--schema <json>` | Esquema JSON para salida estructurada (cadena JSON en l√≠nea) |
| `--schema-file <path>` | Ruta al archivo de esquema JSON para salida estructurada |
| `--max-credits <number>` | Cr√©ditos m√°ximos que se pueden gastar (el trabajo falla si se alcanza el l√≠mite) |
| `--status` | Verificar el estado de un trabajo de agente existente |
| `--wait` | Esperar a que el agente termine antes de devolver los resultados |
| `--poll-interval <seconds>` | Intervalo de sondeo mientras se espera (predeterminado: 5) |
| `--timeout <seconds>` | Tiempo m√°ximo de espera (predeterminado: sin l√≠mite) |
| `--output <path>` | Guardar la salida en un archivo |
| `--json` | Salida en formato JSON |

---

<div id="browser">
  ### Browser
</div>

Inicia sesiones de navegador en la nube y ejecuta c√≥digo en Python, JavaScript o bash de forma remota. Cada sesi√≥n ejecuta una instancia completa de Chromium ‚Äî no necesitas tener un navegador instalado localmente. El c√≥digo se ejecuta del lado del servidor con un objeto `page` de [Playwright](https://playwright.dev/) preconfigurado y listo para usar.

<BrowserBasic />

<div id="browser-options">
  #### Opciones del navegador
</div>

<BrowserOptions />

**Subcomandos:**

| Subcomando | Descripci√≥n |
|------------|-------------|
| `launch-session` | Inicia una nueva sesi√≥n de navegador en la nube (devuelve el ID de sesi√≥n, la URL de CDP y la URL de vista en vivo) |
| `execute <code>` | Ejecuta c√≥digo Playwright en Python/JS o comandos bash en una sesi√≥n |
| `list [status]` | Lista las sesiones de navegador (filtra por `active` o `destroyed`) |
| `close` | Cierra una sesi√≥n de navegador |

**Opciones de ejecuci√≥n:**

| Opci√≥n | Descripci√≥n |
|--------|-------------|
| `--bash` | Ejecuta comandos bash de forma remota en el entorno aislado (sandbox) (predeterminado). [agent-browser](https://github.com/vercel-labs/agent-browser) (m√°s de 40 comandos) viene preinstalado y se a√±ade autom√°ticamente como prefijo. `CDP_URL` se inyecta autom√°ticamente para que agent-browser se conecte a tu sesi√≥n de forma autom√°tica. La mejor opci√≥n para agentes de IA. |
| `--python` | Ejecuta como c√≥digo Playwright en Python. Hay disponible un objeto `page` de Playwright ‚Äî usa `await page.goto()`, `await page.title()`, etc. |
| `--node` | Ejecuta como c√≥digo Playwright en JavaScript. El mismo objeto `page` est√° disponible. |
| `--session <id>` | Apunta a una sesi√≥n espec√≠fica (predeterminado: sesi√≥n activa) |

**Opciones de lanzamiento:**

| Opci√≥n | Descripci√≥n |
|--------|-------------|
| `--ttl <seconds>` | TTL total de la sesi√≥n (predeterminado: 300, rango: 30‚Äì3600) |
| `--ttl-inactivity <seconds>` | Cierre autom√°tico tras inactividad (rango: 10‚Äì3600) |
| `--stream` | Habilita la transmisi√≥n de la vista en vivo |

**Opciones comunes:**

| Opci√≥n | Descripci√≥n |
|--------|-------------|
| `--output <path>` | Guarda la salida en un archivo |
| `--json` | Genera la salida en formato JSON |

---

<div id="credit-usage">
  ### Uso de cr√©ditos
</div>

Consulta el saldo y el uso de cr√©ditos de tu equipo.

```bash CLI
# View credit usage
firecrawl credit-usage

# Salida en formato JSON
firecrawl credit-usage --json --pretty
```

***


<div id="version">
  ### Versi√≥n
</div>

Mostrar la versi√≥n de la CLI.

```bash CLI
firecrawl version
# o
firecrawl --version
```


<div id="global-options">
  ## Opciones globales
</div>

Estas opciones est√°n disponibles para todos los comandos:

| Opci√≥n | Abrev. | Descripci√≥n |
|--------|--------|-------------|
| `--status` | | Muestra la versi√≥n, el estado de autenticaci√≥n, la concurrencia y los cr√©ditos |
| `--api-key <key>` | `-k` | Sobrescribe la clave de API almacenada para este comando |
| `--api-url <url>` | | Usa una URL de API personalizada (para entornos autoalojados/desarrollo local) |
| `--help` | `-h` | Muestra la ayuda para un comando |
| `--version` | `-V` | Muestra la versi√≥n de la CLI |

<div id="output-handling">
  ## Manejo de la salida
</div>

La CLI env√≠a la salida a stdout de forma predeterminada, lo que facilita usarla en pipes o redirigirla:

```bash CLI
# Canalizar markdown a otro comando
firecrawl https://example.com | head -50

# Redirigir a un archivo
firecrawl https://example.com > output.md

# Guardar JSON con formato legible
firecrawl https://example.com --format markdown,links --pretty -o data.json
```


<div id="format-behavior">
  ### Comportamiento de los formatos
</div>

* **Un solo formato**: Devuelve contenido sin procesar (texto markdown, HTML, etc.)
* **Varios formatos**: Devuelve JSON con todos los datos solicitados

```bash CLI
# Salida de markdown sin procesar
firecrawl https://example.com --format markdown

# Salida JSON con m√∫ltiples formatos
firecrawl https://example.com --format markdown,links
```


<div id="examples">
  ## Ejemplos
</div>

<div id="quick-scrape">
  ### Scrape r√°pido
</div>

```bash CLI
# Obtener contenido markdown de una URL (usar --only-main-content para una salida limpia)
firecrawl https://docs.firecrawl.dev --only-main-content

# Get HTML content
firecrawl https://example.com --html -o page.html
```


<div id="full-site-crawl">
  ### Rastreo completo del sitio web
</div>

```bash CLI
# Rastrear un sitio de documentaci√≥n con l√≠mites
firecrawl crawl https://docs.example.com --limit 50 --max-depth 2 --wait --progress -o docs.json
```


<div id="site-discovery">
  ### Descubrimiento de sitios web
</div>

```bash CLI
# Buscar todas las publicaciones del blog
firecrawl map https://example.com --search "blog" -o blog-urls.txt
```


<div id="research-workflow">
  ### Flujo de trabajo de investigaci√≥n
</div>

```bash CLI
# Buscar y extraer resultados para investigaci√≥n
firecrawl search "machine learning best practices 2024" --scrape --scrape-formats markdown --pretty
```


<div id="agent">
  ### Agente
</div>

```bash CLI
# URLs are optional
firecrawl agent "Encuentra las 5 principales startups de IA y sus montos de financiaci√≥n" --wait

# Focus on specific URLs
firecrawl agent "Compare pricing plans" --urls https://slack.com/pricing,https://teams.microsoft.com/pricing --wait
```


<div id="browser-automation">
  ### Automatizaci√≥n del navegador
</div>

```bash CLI
# Launch a session, scrape a page, and close
firecrawl browser launch-session
firecrawl browser execute "open https://news.ycombinator.com"
firecrawl browser execute "snapshot"
firecrawl browser execute "scrape"
firecrawl browser close

# Usar agent-browser mediante modo bash (predeterminado ‚Äî recomendado para agentes de IA)
firecrawl browser launch-session
firecrawl browser execute "open https://example.com"
firecrawl browser execute "snapshot"
# snapshot returns @ref IDs ‚Äî use them to interact
firecrawl browser execute "click @e5"
firecrawl browser execute "fill @e3 'search query'"
firecrawl browser execute "scrape"
# Run --help to see all 40+ commands
firecrawl browser execute --bash "agent-browser --help"
firecrawl browser close
```


<div id="combine-with-other-tools">
  ### Comb√≠nalo con otras herramientas
</div>

```bash CLI
# Extract URLs from search results
jq -r '.data.web[].url' search-results.json

# Obtener t√≠tulos de resultados de b√∫squeda
jq -r '.data.web[] | "\(.title): \(.url)"' search-results.json

# Extract links and process with jq
firecrawl https://example.com --format links | jq '.links[].url'

# Count URLs from map
firecrawl map https://example.com | wc -l
```


<div id="telemetry">
  ## Telemetr√≠a
</div>

La CLI recopila datos de uso an√≥nimos durante la autenticaci√≥n para ayudar a mejorar el producto:

* Versi√≥n de la CLI, sistema operativo y versi√≥n de Node.js
* Detecci√≥n de herramientas de desarrollo (por ejemplo, Cursor, VS Code, Claude Code)

**No se recopilan datos de comandos, URL ni contenido de archivos a trav√©s de la CLI.**

Para desactivar la telemetr√≠a, configura la siguiente variable de entorno:

```bash CLI
export FIRECRAWL_NO_TELEMETRY=1
```


<div id="open-source">
  ## C√≥digo abierto
</div>

La CLI y la Skill de Firecrawl son de c√≥digo abierto y est√°n disponibles en GitHub: [firecrawl/cli](https://github.com/firecrawl/cli)