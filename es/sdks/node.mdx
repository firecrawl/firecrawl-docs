---
title: 'Node'
description: 'El SDK de Firecrawl para Node es un envoltorio de la API de Firecrawl que te ayuda a convertir sitios web en Markdown de forma sencilla.'
icon: 'node'
og:title: "Node SDK | Firecrawl"
og:description: "El SDK de Firecrawl para Node es un envoltorio de la API de Firecrawl que te ayuda a convertir sitios web en Markdown de forma sencilla."
---

import InstallationNode from '/snippets/es/v2/installation/js.mdx'
import ScrapeAndCrawlExampleNode from '/snippets/es/v2/scrape-and-crawl/js.mdx'
import ScrapeNodeShort from '/snippets/es/v2/scrape/short/js.mdx'
import CrawlNodeShort from '/snippets/es/v2/crawl/short/js.mdx'
import StartCrawlNodeShort from '/snippets/es/v2/start-crawl/short/js.mdx'
import CheckCrawlStatusNodeShort from '/snippets/es/v2/crawl-status/short/js.mdx'
import CancelCrawlNodeShort from '/snippets/es/v2/crawl-delete/short/js.mdx'
import MapNodeShort from '/snippets/es/v2/map/short/js.mdx'
import ExtractNodeShort from '/snippets/v2/extract/short/js.mdx'
import CrawlWebSocketNodeBase from '/snippets/es/v2/crawl-websocket/base/js.mdx'

<div id="installation">
  ## Instalación
</div>

Para instalar el SDK de Firecrawl para Node, puedes usar npm:

<InstallationNode />

<div id="usage">
  ## Uso
</div>

1. Obtén una clave de API en [firecrawl.dev](https://firecrawl.dev)
2. Define la clave de API como una variable de entorno llamada `FIRECRAWL_API_KEY` o pásala como parámetro a la clase `FirecrawlApp`.

Aquí tienes un ejemplo de cómo usar el SDK con manejo de errores:

<ScrapeAndCrawlExampleNode />

<div id="scraping-a-url">
  ### Extracción de una URL
</div>

Para extraer una única URL con manejo de errores, usa el método `scrapeUrl`. Recibe la URL como parámetro y devuelve los datos extraídos como un diccionario.

<ScrapeNodeShort />

<div id="crawling-a-website">
  ### Rastreo de un sitio web
</div>

Para rastrear un sitio web con manejo de errores, usa el método `crawlUrl`. Recibe la URL inicial y parámetros opcionales como argumentos. El argumento `params` te permite especificar opciones adicionales para la tarea de rastreo, como el número máximo de páginas a rastrear, los dominios permitidos y el formato de salida. Consulta [Paginación](#pagination) para la paginación automática o manual y la configuración de límites.

<CrawlNodeShort />

<div id="start-a-crawl">
  ### Iniciar un rastreo
</div>

Inicia un trabajo sin esperar usando `startCrawl`. Devuelve un `ID` de trabajo que puedes usar para comprobar el estado. Usa `crawl` cuando necesites un proceso bloqueante que espere hasta la finalización. Consulta [Paginación](#pagination) para el comportamiento y los límites de paginación.

<StartCrawlNodeShort />

<div id="checking-crawl-status">
  ### Verificar el estado del rastreo
</div>

Para verificar el estado de un trabajo de rastreo con manejo de errores, usa el método `checkCrawlStatus`. Recibe el `ID` como parámetro y devuelve el estado actual del trabajo de rastreo.

<CheckCrawlStatusNodeShort />

<div id="cancelling-a-crawl">
  ### Cancelar un rastreo
</div>

Para cancelar un trabajo de rastreo, usa el método `cancelCrawl`. Recibe como parámetro el ID del trabajo iniciado con `startCrawl` y devuelve el estado de la cancelación.

<CancelCrawlNodeShort />

<div id="mapping-a-website">
  ### Mapear un sitio web
</div>

Para mapear un sitio web con manejo de errores, utiliza el método `mapUrl`. Recibe la URL inicial como parámetro y devuelve los datos del mapeo como un diccionario.

<MapNodeShort />

{/* ### Extraer datos estructurados de sitios web

  Para extraer datos estructurados de sitios web con manejo de errores, utiliza el método `extractUrl`. Recibe la URL inicial como parámetro y devuelve los datos extraídos como un diccionario.

  <ExtractNodeShort /> */}

<div id="crawling-a-website-with-websockets">
  ### Rastreo de un sitio web con WebSockets
</div>

Para rastrear un sitio web con WebSockets, usa el método `crawlUrlAndWatch`. Recibe la URL inicial y parámetros opcionales como argumentos. El argumento `params` te permite especificar opciones adicionales para la tarea de rastreo, como el número máximo de páginas a rastrear, los dominios permitidos y el formato de salida.

<CrawlWebSocketNodeBase />

<div id="pagination">
  ### Paginación
</div>

Los puntos de conexión de Firecrawl para crawl y batch devuelven una URL `next` cuando hay más datos disponibles. El SDK de Node realiza la paginación automáticamente por defecto y agrega todos los documentos; en ese caso, `next` será `null`. Puedes desactivar la paginación automática o establecer límites.

<div id="crawl">
  #### Rastreo
</div>

Usa el método auxiliar `crawl` para la forma más sencilla, o inicia un job y pagina manualmente.

<div id="simple-crawl-auto-pagination-default">
  ##### Rastreo simple (paginación automática, por defecto)
</div>

* Consulta el flujo por defecto en [Rastrear un sitio web](#crawling-a-website).

<div id="manual-crawl-with-pagination-control-single-page">
  ##### Rastreo manual con control de paginación (una sola página)
</div>

* Inicia un trabajo y luego recupera una página a la vez con `autoPaginate: false`.

```js Nodo
const crawlStart = await firecrawl.startCrawl('https://docs.firecrawl.dev', { limit: 5 });
const crawlJobId = crawlStart.id;

const crawlSingle = await firecrawl.getCrawlStatus(crawlJobId, { autoPaginate: false });
console.log('rastreo de una sola página:', crawlSingle.status, 'docs:', crawlSingle.data.length, 'siguiente:', crawlSingle.next);
```

<div id="manual-crawl-with-limits-auto-pagination-early-stop">
  ##### Rastreo manual con límites (paginación automática + parada anticipada)
</div>

* Mantén la paginación automática activada, pero deténla antes con `maxPages`, `maxResults` o `maxWaitTime`.

```js Node
const crawlLimited = await firecrawl.getCrawlStatus(crawlJobId, {
  autoPaginate: true,
  maxPages: 2,
  maxResults: 50,
  maxWaitTime: 15,
});
console.log('rastreo limitado:', crawlLimited.status, 'docs:', crawlLimited.data.length, 'siguiente:', crawlLimited.next);
```

<div id="batch-scrape">
  #### Scrape por lotes
</div>

Usa el método waiter `batchScrape`, o inicia un job y pagina manualmente.

<div id="simple-batch-scrape-auto-pagination-default">
  ##### Raspado por lotes simple (paginación automática, predeterminado)
</div>

* Consulta el flujo predeterminado en [Raspado por lotes](/es/features/batch-scrape).

<div id="manual-batch-scrape-with-pagination-control-single-page">
  ##### Raspado manual por lotes con control de paginación (una sola página)
</div>

* Inicia un job y luego recupera una página a la vez con `autoPaginate: false`.

```js Node
const batchStart = await firecrawl.startBatchScrape([
  'https://docs.firecrawl.dev',
  'https://firecrawl.dev',
], { options: { formats: ['markdown'] } });
const batchJobId = batchStart.id;

const batchSingle = await firecrawl.getBatchScrapeStatus(batchJobId, { autoPaginate: false });
console.log('lote, una sola página:', batchSingle.status, 'docs:', batchSingle.data.length, 'siguiente:', batchSingle.next);
```

<div id="manual-batch-scrape-with-limits-auto-pagination-early-stop">
  ##### Extracción manual por lotes con límites (paginación automática + detención anticipada)
</div>

* Mantén la paginación automática activada, pero deténla antes con `maxPages`, `maxResults` o `maxWaitTime`.

```js Node
const batchLimited = await firecrawl.getBatchScrapeStatus(batchJobId, {
  autoPaginate: true,
  maxPages: 2,
  maxResults: 100,
  maxWaitTime: 20,
});
console.log('lote limitado:', batchLimited.status, 'docs:', batchLimited.data.length, 'siguiente:', batchLimited.next);
```

<div id="error-handling">
  ## Manejo de errores
</div>

El SDK gestiona los errores devueltos por la API de Firecrawl y arroja las excepciones correspondientes. Si se produce un error durante una solicitud, se generará una excepción con un mensaje descriptivo. Los ejemplos anteriores muestran cómo manejar estos errores con bloques `try/catch`.