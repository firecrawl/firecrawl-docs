---
title: 'Rastreo'
description: 'Firecrawl puede buscar de forma recursiva en los subdominios de una URL y recopilar su contenido'
og:title: "Rastreo | Firecrawl"
og:description: "Firecrawl puede buscar de forma recursiva en los subdominios de una URL y recopilar su contenido"
---

import InstallationPython from "/snippets/es/v1/installation/python.mdx";
import InstallationNode from "/snippets/es/v1/installation/js.mdx";
import InstallationGo from "/snippets/es/v1/installation/go.mdx";
import InstallationRust from "/snippets/es/v1/installation/rust.mdx";
import CrawlPython from "/snippets/es/v1/crawl/base/python.mdx";
import CrawlNode from "/snippets/es/v1/crawl/base/js.mdx";
import CrawlGo from "/snippets/es/v1/crawl/base/go.mdx";
import CrawlRust from "/snippets/es/v1/crawl/base/rust.mdx";
import CrawlCURL from "/snippets/es/v1/crawl/base/curl.mdx";
import AsyncCrawlOutput from "/snippets/es/v1/crawl-async/base/output.mdx";
import CheckCrawlJobPython from "/snippets/es/v1/crawl-status/short/python.mdx";
import CheckCrawlJobNode from "/snippets/es/v1/crawl-status/short/js.mdx";
import CheckCrawlJobGo from "/snippets/es/v1/crawl-status/short/go.mdx";
import CheckCrawlJobRust from "/snippets/es/v1/crawl-status/short/rust.mdx";
import CheckCrawlJobCURL from "/snippets/es/v1/crawl-status/short/curl.mdx";
import CheckCrawlJobOutputScraping from "/snippets/es/v1/crawl-status/base/output-scraping.mdx";
import CheckCrawlJobOutputCompleted from "/snippets/es/v1/crawl-status/base/output-completed.mdx";
import CrawlWebSocketPython from "/snippets/es/v1/crawl-websocket/base/python.mdx";
import CrawlWebSocketNode from "/snippets/es/v1/crawl-websocket/base/js.mdx";
import CrawlWebhookCURL from "/snippets/es/v1/crawl-webhook/base/curl.mdx";
import PythonCrawlExample from "/snippets/es/v1/crawl/sdk-example/python.mdx";
import NodeCrawlExample from "/snippets/es/v1/crawl/sdk-example/js.mdx";
import PythonCrawlExampleResponse from "/snippets/es/v1/crawl/sdk-example/python-response.mdx";
import NodeCrawlExampleResponse from "/snippets/es/v1/crawl/sdk-example/js-response.mdx";
import FastCrawlPython from "/snippets/es/v1/crawl/fast/python.mdx";
import FastCrawlNode from "/snippets/es/v1/crawl/fast/js.mdx";
import FastCrawlGo from "/snippets/es/v1/crawl/fast/go.mdx";
import FastCrawlRust from "/snippets/es/v1/crawl/fast/rust.mdx";
import FastCrawlCURL from "/snippets/es/v1/crawl/fast/curl.mdx";

Firecrawl rastrea sitios web de forma eficiente para extraer datos completos mientras sortea bloqueos. El proceso:

1. **Análisis de URL:** Examina el sitemap y rastrea el sitio para identificar enlaces
2. **Recorrido:** Sigue los enlaces de forma recursiva para localizar todas las subpáginas
3. **Scraping:** Extrae el contenido de cada página, gestionando JS y límites de tasa
4. **Salida:** Convierte los datos a markdown limpio o a un formato estructurado

Esto garantiza una recopilación exhaustiva de datos desde cualquier URL de inicio.

<div id="crawling">
  ## Rastreando
</div>

<div id="crawl-endpoint">
  ### punto de conexión /crawl
</div>

Se usa para rastrear una URL y todas sus subpáginas accesibles. Esto envía un trabajo de rastreo y devuelve un ID de trabajo para comprobar el estado del rastreo.

<Warning>De forma predeterminada, /crawl ignorará los enlaces secundarios de una página si no son descendientes de la URL que proporcionas. Por ejemplo, website.com/other-parent/blog-1 no se devolverá si rastreas website.com/blogs/. Si necesitas incluir website.com/other-parent/blog-1, usa el parámetro `crawlEntireDomain`. Para rastrear subdominios como blog.website.com al rastrear website.com, usa el parámetro `allowSubdomains`.</Warning>

<div id="installation">
  ### Instalación
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationGo />

  <InstallationRust />
</CodeGroup>

<div id="usage">
  ### Uso
</div>

<CodeGroup>
  <CrawlPython />

  <CrawlNode />

  <CrawlGo />

  <CrawlRust />

  <CrawlCURL />
</CodeGroup>

<div id="api-response">
  ### Respuesta de la API
</div>

Si estás usando cURL o las funciones `async crawl` en los SDK, esto devolverá un `ID` que puedes usar para verificar el estado del rastreo.

<Note>Si estás usando el SDK, consulta la sección de respuesta del SDK [más abajo](#sdk-response).</Note>

<AsyncCrawlOutput />

<div id="check-crawl-job">
  ### Consultar trabajo de rastreo
</div>

Se utiliza para verificar el estado de un trabajo de rastreo y obtener su resultado.

<Note>Este punto de conexión solo funciona para rastreos que estén en curso o que se hayan completado recientemente. </Note>

<CodeGroup>
  <CheckCrawlJobPython />

  <CheckCrawlJobNode />

  <CheckCrawlJobGo />

  <CheckCrawlJobRust />

  <CheckCrawlJobCURL />
</CodeGroup>

<div id="response-handling">
  #### Gestión de la respuesta
</div>

La respuesta varía según el estado del rastreo.

Para respuestas no finalizadas o de gran tamaño que superen los 10 MB, se proporciona el parámetro de URL `next`. Debes solicitar esa URL para obtener los siguientes 10 MB de datos. Si el parámetro `next` no está presente, significa que no hay más datos del rastreo.

El parámetro skip define el número máximo de resultados que se devuelven en cada bloque.

<Info>
  Los parámetros skip y next solo son relevantes al acceder directamente a la API. Si usas el SDK, nosotros nos encargamos y te devolveremos todos los resultados de una sola vez.
</Info>

<CodeGroup>
  <CheckCrawlJobOutputScraping />

  <CheckCrawlJobOutputCompleted />
</CodeGroup>

<div id="sdk-response">
  ### Respuesta del SDK
</div>

El SDK ofrece dos formas de rastrear URL:

1. **Rastreo sincrónico** (`crawl_url`/`crawlUrl`):
   * Espera a que finalice el rastreo y devuelve la respuesta completa
   * Gestiona la paginación automáticamente
   * Recomendado para la mayoría de los casos de uso

<CodeGroup>
  <PythonCrawlExample />

  <NodeCrawlExample />
</CodeGroup>

La respuesta incluye el estado del rastreo y todos los datos extraídos:

<CodeGroup>
  <PythonCrawlExampleResponse />

  <NodeCrawlExampleResponse />
</CodeGroup>

2. **Rastreo asincrónico** (`async_crawl_url`/`asyncCrawlUrl`):
   * Devuelve inmediatamente un ID de rastreo
   * Permite comprobar el estado de forma manual
   * Útil para rastreos de larga duración o lógica de sondeo personalizada

<CodeGroup>
  <AsyncCrawlPython />

  <AsyncCrawlNode />
</CodeGroup>

<div id="faster-crawling">
  ## Rastreo más rápido
</div>

Acelera tus rastreos hasta un 500% cuando no necesitas los datos más recientes. Agrega `maxAge` a tus `scrapeOptions` para usar datos en caché de la página cuando haya disponibilidad.

<CodeGroup>
  <FastCrawlPython />

  <FastCrawlNode />

  <FastCrawlGo />

  <FastCrawlRust />

  <FastCrawlCURL />
</CodeGroup>

**Cómo funciona:**

* Cada página de tu rastreo comprueba si hay datos en caché más nuevos que `maxAge`
* Si los hay, se devuelve al instante desde la caché (hasta 500% más rápido)
* Si no, se raspa la página en fresco y se almacena el resultado en caché
* Ideal para rastrear sitios de documentación, catálogos de productos u otro contenido relativamente estático

Para más detalles sobre el uso de `maxAge (caché)`, consulta la documentación de [Raspado más rápido](/es/features/fast-scraping).

<div id="crawl-websocket">
  ## WebSocket de rastreo
</div>

El método de Firecrawl basado en WebSocket, `Crawl URL and Watch`, permite extraer y monitorear datos en tiempo real. Inicia un rastreo con una URL y personalízalo con opciones como límites de páginas, dominios permitidos y formatos de salida; ideal para necesidades de procesamiento de datos inmediatas.

<CodeGroup>
  <CrawlWebSocketPython />

  <CrawlWebSocketNode />
</CodeGroup>

<div id="crawl-webhook">
  ## Webhook de rastreo
</div>

Puedes configurar webhooks para recibir notificaciones en tiempo real a medida que avanza tu rastreo. Esto te permite procesar las páginas conforme se van extrayendo, en lugar de esperar a que finalice todo el rastreo.

<CrawlWebhookCURL />

Para obtener documentación completa sobre webhooks—including tipos de eventos, estructura del payload y ejemplos de implementación—consulta la [documentación de webhooks](/es/features/webhooks).

<div id="quick-reference">
  ### Referencia rápida
</div>

**Tipos de eventos:**

* `crawl.started` - Cuando se inicia el rastreo
* `crawl.page` - Para cada página extraída correctamente
* `crawl.completed` - Cuando finaliza el rastreo
* `crawl.failed` - Si el rastreo produce un error

**Carga útil básica:**

```json
{
  "success": true,
  "type": "crawl.page",
  "id": "crawl-job-id",
  "data": [...], // Datos de la página para eventos de tipo 'page'
  "metadata": {}, // Metadatos personalizados
  "error": null
}
```

<Note>
  Para ver la configuración de webhooks en detalle, las mejores prácticas de seguridad y la resolución de problemas, visita la [documentación sobre webhooks](/es/features/webhooks).
</Note>
