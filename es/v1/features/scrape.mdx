---
title: "Extracción"
description: "Convierte cualquier URL en datos limpios"
og:title: "Scrape | Firecrawl"
og:description: "Convierte cualquier URL en datos limpios"
---

import InstallationPython from "/snippets/es/v1/installation/python.mdx";
import InstallationNode from "/snippets/es/v1/installation/js.mdx";
import InstallationGo from "/snippets/es/v1/installation/go.mdx";
import InstallationRust from "/snippets/es/v1/installation/rust.mdx";
import ScrapePython from "/snippets/es/v1/scrape/base/python.mdx";
import ScrapeNode from "/snippets/es/v1/scrape/base/js.mdx";
import ScrapeGo from "/snippets/es/v1/scrape/base/go.mdx";
import ScrapeRust from "/snippets/es/v1/scrape/base/rust.mdx";
import ScrapeCURL from "/snippets/es/v1/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/es/v1/scrape/base/output.mdx";
import ExtractCURL from "/snippets/es/v1/llm-extract/base/curl.mdx";
import ExtractPython from "/snippets/es/v1/llm-extract/base/python.mdx";
import ExtractNode from "/snippets/es/v1/llm-extract/base/js.mdx";
import ExtractOutput from "/snippets/es/v1/llm-extract/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/es/v1/llm-extract/no-schema/curl.mdx";
import ExtractNoSchemaOutput from "/snippets/es/v1/llm-extract/no-schema/output.mdx";
import ScrapeActionsPython from "/snippets/es/v1/scrape/actions/python.mdx";
import ScrapeActionsNode from "/snippets/es/v1/scrape/actions/js.mdx";
import ScrapeActionsCURL from "/snippets/es/v1/scrape/actions/curl.mdx";
import ScrapeActionsOutput from "/snippets/es/v1/scrape/actions/output.mdx";
import BatchScrapePython from "/snippets/es/v1/batch-scrape/base/python.mdx";
import BatchScrapeNode from "/snippets/es/v1/batch-scrape/base/js.mdx";
import BatchScrapeCURL from "/snippets/es/v1/batch-scrape/base/curl.mdx";
import BatchScrapeOutput from "/snippets/es/v1/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/es/v1/batch-scrape/base/async-output.mdx";
import ScrapeLocationPython from "/snippets/es/v1/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/es/v1/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/es/v1/scrape/location/curl.mdx";

Firecrawl convierte páginas web en markdown, ideal para aplicaciones con LLM.

* Gestiona las complejidades: proxies, caché, límites de tasa, contenido bloqueado por JS
* Maneja contenido dinámico: sitios dinámicos, sitios renderizados con JS, PDF, imágenes
* Genera markdown limpio, datos estructurados, capturas de pantalla o HTML.

Para más detalles, consulta la [referencia de la API del punto de conexión /scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="scraping-a-url-with-firecrawl">
  ## Raspando una URL con Firecrawl
</div>

<div id="scrape-endpoint">
  ### punto de conexión /scrape
</div>

Se utiliza para hacer scraping de una URL y obtener su contenido.

<div id="installation">
  ### Instalación
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationGo />

  <InstallationRust />
</CodeGroup>

<div id="usage">
  ### Uso
</div>

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeGo />

  <ScrapeRust />

  <ScrapeCURL />
</CodeGroup>

Para más información sobre los parámetros, consulta la [referencia de la API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="response">
  ### Respuesta
</div>

Los SDK devolverán el objeto de datos directamente. cURL devolverá el payload exactamente como se muestra a continuación.

<ScrapeResponse />

<div id="scrape-formats">
  ## Formatos de scraping
</div>

Ahora puedes elegir en qué formatos quieres el resultado. Puedes especificar varios formatos de salida. Los formatos compatibles son:

* Markdown (markdown)
* HTML (html)
* HTML sin procesar (rawHtml) (sin modificaciones)
* Captura de pantalla (screenshot o screenshot@fullPage)
* Enlaces (links)
* JSON (json) - salida estructurada

Las claves del resultado coincidirán con el formato que elijas.

<div id="extract-structured-data">
  ## Extrae datos estructurados
</div>

<div id="scrape-with-json-endpoint">
  ### Punto de conexión /scrape (con json)
</div>

Se utiliza para extraer datos estructurados de páginas extraídas.

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

Salida:

<ExtractOutput />

<div id="extracting-without-schema-new">
  ### Extracción sin esquema (Nuevo)
</div>

Ahora puedes extraer sin un esquema con solo pasar un `prompt` al punto de conexión. El LLM decide la estructura de los datos.

<CodeGroup>
  <ExtractNoSchemaCURL />
</CodeGroup>

Salida:

<ExtractNoSchemaOutput />

<div id="json-options-object">
  ### Objeto de opciones JSON
</div>

El objeto `jsonOptions` admite los siguientes parámetros:

* `schema`: El esquema que se utilizará para la extracción.
* `systemPrompt`: El prompt del sistema que se utilizará para la extracción.
* `prompt`: El prompt que se utilizará para la extracción sin un esquema.

<div id="interacting-with-the-page-with-actions">
  ## Interacting with the page with Actions
</div>

Firecrawl te permite ejecutar varias acciones en una página web antes de extraer su contenido. Esto es especialmente útil para interactuar con contenido dinámico, navegar por páginas o acceder a contenido que requiere interacción del usuario.

Aquí tienes un ejemplo de cómo usar acciones para ir a google.com, buscar “Firecrawl”, hacer clic en el primer resultado y tomar una captura de pantalla.

Es importante usar casi siempre la acción `wait` antes y/o después de ejecutar otras acciones para dar tiempo suficiente a que la página cargue.

<div id="example">
  ### Ejemplo
</div>

<CodeGroup>
  <ScrapeActionsPython />

  <ScrapeActionsNode />

  <ScrapeActionsCURL />
</CodeGroup>

<div id="output">
  ### Salida
</div>

<CodeGroup>
  <ScrapeActionsOutput />
</CodeGroup>

Para más detalles sobre los parámetros de acciones, consulta la [Referencia de la API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="location-and-language">
  ## Ubicación e idioma
</div>

Especifica el país y los idiomas preferidos para obtener contenido relevante según tu ubicación y preferencias de idioma.

<div id="how-it-works">
  ### Cómo funciona
</div>

Cuando especificas la configuración de ubicación, Firecrawl utilizará un proxy adecuado si está disponible y emulará los ajustes de idioma y zona horaria correspondientes. De forma predeterminada, la ubicación se establece en “US” si no se especifica.

<div id="usage">
  ### Uso
</div>

Para usar la configuración de ubicación e idioma, incluye el objeto `location` en el cuerpo de tu solicitud con las siguientes propiedades:

* `country`: código de país ISO 3166-1 alfa-2 (p. ej., &#39;US&#39;, &#39;AU&#39;, &#39;DE&#39;, &#39;JP&#39;). Valor predeterminado: &#39;US&#39;.
* `languages`: una lista de idiomas y configuraciones regionales preferidos para la solicitud en orden de prioridad. Valor predeterminado: el idioma de la ubicación especificada.

<CodeGroup>
  <ScrapeLocationPython />

  <ScrapeLocationNode />

  <ScrapeLocationCURL />
</CodeGroup>

<div id="batch-scraping-multiple-urls">
  ## Raspado por lotes de varias URL
</div>

Ahora puedes raspar varias URL en lote al mismo tiempo. Recibe las URL iniciales y parámetros opcionales como argumentos. El argumento params te permite especificar opciones adicionales para el trabajo por lotes, como los formatos de salida.

<div id="how-it-works">
  ### Cómo funciona
</div>

Funciona de forma muy similar al punto de conexión `/crawl`. Envía un trabajo de scraping en lote y devuelve un ID de trabajo para consultar el estado del lote.

El SDK ofrece 2 métodos: sincrónico y asincrónico. El método sincrónico devuelve los resultados del trabajo en lote, mientras que el asincrónico devuelve un ID de trabajo que puedes usar para consultar el estado del lote.

<div id="usage">
  ### Uso
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Respuesta
</div>

Si usas los métodos síncronos de los SDK, se devolverán los resultados del trabajo de scraping por lotes. De lo contrario, se devolverá un ID de trabajo que puedes usar para consultar el estado del scraping por lotes.

<div id="synchronous">
  #### Sincrónico
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### Asíncrono
</div>

Luego puedes usar el ID del trabajo para consultar el estado del raspado por lotes llamando al punto de conexión `/batch/scrape/{id}`. Este punto de conexión está pensado para usarse mientras el trabajo aún se está ejecutando o justo después de que haya finalizado, **ya que los trabajos de raspado por lotes caducan a las 24 horas**.

<BatchScrapeAsyncOutput />

<div id="stealth-mode">
  ## Modo sigiloso
</div>

Para sitios web con protección anti‑bot avanzada, Firecrawl ofrece un modo de proxy sigiloso que mejora la tasa de éxito al extraer datos de sitios complejos.

Más información sobre el [Modo sigiloso](/es/features/stealth-mode).

<div id="using-fire-1-with-scrape">
  ## Usar FIRE-1 con Scrape
</div>

Puedes usar el FIRE-1 (Agente) con el punto de conexión `/scrape` para aplicar navegación inteligente antes de extraer el contenido final.

Activar FIRE-1 es sencillo. Solo incluye un objeto `agent` en tu solicitud a la API de scrape o extract:

```json
"agent": {
  "model": "FIRE-1",
  "prompt": "Aquí van tus instrucciones de navegación detalladas."
}
```

*Nota:* El campo `prompt` es obligatorio en las solicitudes de scraping y le indica a FIRE-1 con precisión cómo interactuar con la página web.

<div id="example-usage-with-scrape-endpoint">
  ### Ejemplo de uso con el punto de conexión /scrape
</div>

Aquí tienes un ejemplo rápido de cómo usar FIRE-1 con el punto de conexión /scrape para obtener las empresas del segmento de consumo de Y Combinator:

```bash
curl -X POST https://api.firecrawl.dev/v1/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer TU_API_KEY' \
  -d '{
    "url": "https://ycombinator.com/companies",
    "formats": ["markdown"],
    "agent": {
      "model": "FIRE-1",
      "prompt": "Obtén las empresas de W22 en el sector de consumo haciendo clic en los botones correspondientes"
    }
  }'
```
