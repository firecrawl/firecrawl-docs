---
title: 'Raspado en lote'
description: 'Raspar varias URL en lote'
og:title: 'Raspado en lote | Firecrawl'
og:description: 'Raspar varias URL en lote'
---

import BatchScrapePython from '/snippets/es/v1/batch-scrape/base/python.mdx';
import BatchScrapeNode from '/snippets/es/v1/batch-scrape/base/js.mdx';
import BatchScrapeCURL from '/snippets/es/v1/batch-scrape/base/curl.mdx';
import BatchScrapeOutput from '/snippets/es/v1/batch-scrape/base/output.mdx';
import BatchScrapeAsyncOutput from '/snippets/es/v1/batch-scrape/base/async-output.mdx';
import BatchScrapeExtractPython from '/snippets/es/v1/batch-scrape/extract/python.mdx';
import BatchScrapeExtractNode from '/snippets/es/v1/batch-scrape/extract/js.mdx';
import BatchScrapeExtractCURL from '/snippets/es/v1/batch-scrape/extract/curl.mdx';
import BatchScrapeExtractOutput from '/snippets/es/v1/batch-scrape/extract/output.mdx';
import BatchScrapeExtractAsyncOutput from '/snippets/es/v1/batch-scrape/extract/async-output.mdx';
import BatchScrapeWebhookCURL from '/snippets/es/v1/batch-scrape-webhook/base/curl.mdx';

<div id="batch-scraping-multiple-urls">
  ## Extracción por lotes de múltiples URL
</div>

Ahora puedes extraer varias URL en lote al mismo tiempo. Recibe las URL iniciales y parámetros opcionales como argumentos. El argumento params te permite especificar opciones adicionales para el trabajo de extracción por lotes, como los formatos de salida.

<div id="how-it-works">
  ### Cómo funciona
</div>

Es muy similar al funcionamiento del endpoint `/crawl`. Envía un trabajo de scraping por lotes y devuelve un ID de trabajo para consultar el estado del proceso por lotes.

El SDK ofrece 2 métodos: sincrónico y asincrónico. El método sincrónico devuelve los resultados del trabajo por lotes, mientras que el asincrónico devuelve un ID de trabajo que puedes usar para consultar su estado.

<div id="usage">
  ### Uso
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Respuesta
</div>

Si usas los métodos sincrónicos de los SDK, devolverá los resultados del trabajo de scraping por lotes. De lo contrario, devolverá un ID de trabajo que puedes usar para comprobar el estado del scraping por lotes.

<div id="synchronous">
  #### Sincrónico
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### Asíncrono
</div>

Luego puedes usar el ID del trabajo para consultar el estado del scraping por lotes llamando al punto de conexión `/batch/scrape/{id}`. Este punto de conexión está pensado para usarse mientras el trabajo aún se está ejecutando o justo después de que haya finalizado, **ya que los trabajos de scraping por lotes expiran a las 24 horas**.

<BatchScrapeAsyncOutput />

<div id="batch-scrape-with-extraction">
  ## Rastreo por lotes con extracción
</div>

También puedes usar el punto de conexión de rastreo por lotes para extraer datos estructurados de las páginas. Esto es útil si quieres obtener los mismos datos estructurados de una lista de URL.

<CodeGroup>
  <BatchScrapeExtractPython />

  <BatchScrapeExtractNode />

  <BatchScrapeExtractCURL />
</CodeGroup>

<div id="response">
  ### Respuesta
</div>

<div id="synchronous">
  #### Sincrónico
</div>

<BatchScrapeExtractOutput />

<div id="asynchronous">
  #### Asíncrono
</div>

<BatchScrapeExtractAsyncOutput />

<div id="batch-scrape-with-webhooks">
  ## Rastreo por lotes con webhooks
</div>

Puedes configurar webhooks para recibir notificaciones en tiempo real a medida que se rastrea cada URL del lote. Esto te permite procesar los resultados de inmediato en lugar de esperar a que finalice todo el lote.

<BatchScrapeWebhookCURL />

Para consultar la documentación completa de webhooks, incluidos los tipos de eventos, la estructura del payload y ejemplos de implementación, visita la [documentación de webhooks](/es/webhooks/overview).

<div id="quick-reference">
  ### Referencia rápida
</div>

**Tipos de eventos:**

* `batch_scrape.started` - Cuando comienza el scraping por lotes
* `batch_scrape.page` - Por cada URL extraída correctamente
* `batch_scrape.completed` - Cuando se procesan todas las URL
* `batch_scrape.failed` - Si el scraping por lotes encuentra un error

**Carga útil básica:**

```json
{
  "success": true,
  "type": "batch_scrape.page",
  "id": "batch-job-id",
  "data": [...], // Datos de página para eventos 'page'
  "metadata": {}, // Tus metadatos personalizados
  "error": null
}
```

<Note>
  Para obtener detalles sobre la configuración de webhooks, las prácticas recomendadas de seguridad y
  la resolución de problemas, visita la [documentación de webhooks](/es/webhooks/overview).
</Note>
