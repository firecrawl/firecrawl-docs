---
title: 'Skill + CLI'
description: 'Firecrawl Skill √© uma maneira f√°cil de fazer com que agentes de IA como Claude Code, Antigravity e OpenCode usem o Firecrawl pela CLI.'
icon: 'terminal'
og:title: "CLI | Firecrawl"
og:description: "Firecrawl Skills √© uma maneira f√°cil de fazer com que agentes de IA usem o Firecrawl pela CLI. Agentes de IA podem obter dados da web por meio de uma interface melhor e mais eficiente em termos de contexto"
---

import InstallationCLI from '/snippets/pt-BR/v2/cli/installation/bash.mdx'
import AuthLogin from '/snippets/pt-BR/v2/cli/auth/login.mdx'
import AuthLogout from '/snippets/pt-BR/v2/cli/auth/logout.mdx'
import AuthConfig from '/snippets/pt-BR/v2/cli/auth/config.mdx'
import AuthSelfHosted from '/snippets/pt-BR/v2/cli/auth/self-hosted.mdx'
import ScrapeBasic from '/snippets/pt-BR/v2/cli/scrape/basic.mdx'
import ScrapeFormats from '/snippets/pt-BR/v2/cli/scrape/formats.mdx'
import ScrapeOptions from '/snippets/pt-BR/v2/cli/scrape/options.mdx'
import CrawlBasic from '/snippets/pt-BR/v2/cli/crawl/basic.mdx'
import CrawlStatus from '/snippets/pt-BR/v2/cli/crawl/status.mdx'
import CrawlOptions from '/snippets/pt-BR/v2/cli/crawl/options.mdx'
import MapBasic from '/snippets/pt-BR/v2/cli/map/basic.mdx'
import MapOptions from '/snippets/pt-BR/v2/cli/map/options.mdx'
import SearchBasic from '/snippets/pt-BR/v2/cli/search/basic.mdx'
import SearchOptions from '/snippets/pt-BR/v2/cli/search/options.mdx'
import AgentBasic from '/snippets/pt-BR/v2/cli/agent/basic.mdx'
import AgentOptions from '/snippets/pt-BR/v2/cli/agent/options.mdx'


<div id="installation">
  ## Instala√ß√£o
</div>

Instale globalmente a CLI do Firecrawl com npm:

<InstallationCLI />

Se voc√™ estiver usando em algum agente de IA, como o Claude Code, poder√° instalar a skill do Firecrawl abaixo, e o agente conseguir√° configur√°-la para voc√™.

```bash
npx skills add firecrawl/cli
```

<Note>
  Depois de instalar a skill, reinicie o Claude Code para que ele reconhe√ßa a nova skill.
</Note>


<div id="authentication">
  ## Autentica√ß√£o
</div>

Antes de usar a CLI, voc√™ precisa autenticar-se com sua chave de API do Firecrawl.

<div id="login">
  ### Login
</div>

<AuthLogin />

<div id="view-configuration">
  ### Visualizar configura√ß√£o
</div>

<AuthConfig />

<div id="logout">
  ### Logout
</div>

<AuthLogout />

<div id="self-hosted-local-development">
  ### Auto-hospedado / Desenvolvimento local
</div>

Para inst√¢ncias auto-hospedadas do Firecrawl ou para desenvolvimento local, use a op√ß√£o `--api-url`:

<AuthSelfHosted />

Ao usar uma URL de API personalizada (qualquer endere√ßo diferente de `https://api.firecrawl.dev`), a autentica√ß√£o por chave de API √© automaticamente ignorada, permitindo que voc√™ use inst√¢ncias locais sem uma chave de API.

<div id="check-status">
  ### Verificar status
</div>

Verifique se a instala√ß√£o e a autentica√ß√£o est√£o corretas e consulte os limites de taxa:

```bash CLI
firecrawl --status
```

Resultado quando estiver pronto:

```
  üî• firecrawl cli v1.1.1

  ‚óè Authenticated via FIRECRAWL_API_KEY
  Concurrency: 0/100 jobs (parallel scrape limit)
  Credits: 500,000 remaining
```

* **Concorr√™ncia**: M√°ximo de tarefas em paralelo. Execute opera√ß√µes paralelas pr√≥ximas a esse limite, mas sem ultrapass√°-lo.
* **Cr√©ditos**: Cr√©ditos de API dispon√≠veis. Cada opera√ß√£o de scrape/crawl consome cr√©ditos.


<div id="commands">
  ## Comandos
</div>

<div id="scrape">
  ### Scrape
</div>

Fa√ßa scraping de uma √∫nica URL e extraia seu conte√∫do em v√°rios formatos.

<Tip>
Use `--only-main-content` para obter um resultado limpo, sem navega√ß√£o, rodap√©s ou an√∫ncios. Isso √© recomendado para a maioria dos casos de uso em que voc√™ quer apenas o artigo ou o conte√∫do principal da p√°gina.
</Tip>

<ScrapeBasic />

<div id="output-formats">
  #### Formatos de sa√≠da
</div>

<ScrapeFormats />

<div id="scrape-options">
  #### Op√ß√µes de Scrape
</div>

<ScrapeOptions />

**Op√ß√µes dispon√≠veis:**

| Op√ß√£o | Atalho | Descri√ß√£o |
|--------|-------|-------------|
| `--url <url>` | `-u` | URL para extrair conte√∫do (alternativa ao argumento posicional) |
| `--format <formats>` | `-f` | formatos de sa√≠da (separados por v√≠rgula): `markdown`, `html`, `rawHtml`, `links`, `screenshot`, `json`, `images`, `summary`, `rastreioDeMudan√ßas`, `attributes`, `branding` |
| `--html` | `-H` | Atalho para `--format html` |
| `--only-main-content` | | Extrair apenas o conte√∫do principal |
| `--wait-for <ms>` | | Tempo de espera, em milissegundos, para renderiza√ß√£o de JS |
| `--screenshot` | | Fazer uma captura de tela |
| `--include-tags <tags>` | | Tags HTML a incluir (separadas por v√≠rgula) |
| `--exclude-tags <tags>` | | Tags HTML a excluir (separadas por v√≠rgula) |
| `--output <path>` | `-o` | Salvar o resultado em um arquivo |
| `--json` | | For√ßar sa√≠da em JSON mesmo com um √∫nico formato |
| `--pretty` | | Imprimir a sa√≠da JSON formatada |
| `--timing` | | Mostrar tempo da requisi√ß√£o e outras informa√ß√µes √∫teis |

---

<div id="search">
  ### Pesquisar
</div>

Pesquise na web e, opcionalmente, fa√ßa o scraping dos resultados.

<SearchBasic />

<div id="search-options">
  #### Op√ß√µes de busca
</div>

<SearchOptions />

**Op√ß√µes dispon√≠veis:**

| Op√ß√£o | Descri√ß√£o |
|--------|-------------|
| `--limit <number>` | N√∫mero m√°ximo de resultados (padr√£o: 5, m√°x.: 100) |
| `--sources <sources>` | Fontes de pesquisa: `web`, `images`, `news` (separadas por v√≠rgula) |
| `--categories <categories>` | Filtrar por categoria: `github`, `research`, `pdf` (separadas por v√≠rgula) |
| `--tbs <value>` | Filtro de tempo: `qdr:h` (hora), `qdr:d` (dia), `qdr:w` (semana), `qdr:m` (m√™s), `qdr:y` (ano) |
| `--location <location>` | Segmenta√ß√£o geogr√°fica (ex.: "Berlin,Germany") |
| `--country <code>` | C√≥digo de pa√≠s ISO (padr√£o: US) |
| `--timeout <ms>` | Tempo limite em milissegundos (padr√£o: 60000) |
| `--ignore-invalid-urls` | Excluir URLs inv√°lidas para outros endpoints do Firecrawl |
| `--scrape` | Fazer scraping dos resultados da pesquisa |
| `--scrape-formats <formats>` | Formatos para o conte√∫do extra√≠do (padr√£o: markdown) |
| `--only-main-content` | Incluir apenas o conte√∫do principal ao fazer scraping (padr√£o: true) |
| `--json` | Sa√≠da em JSON |
| `--output <path>` | Salvar sa√≠da em arquivo |
| `--pretty` | Imprimir sa√≠da JSON formatada |

---

<div id="map">
  ### Mapear
</div>

Descubra rapidamente todas as URLs de um site.

<MapBasic />

<div id="map-options">
  #### Op√ß√µes do Map
</div>

<MapOptions />

**Op√ß√µes dispon√≠veis:**

| Op√ß√£o | Descri√ß√£o |
|--------|-------------|
| `--url <url>` | URL a ser mapeada (alternativa ao argumento posicional) |
| `--limit <number>` | N√∫mero m√°ximo de URLs a serem descobertas |
| `--search <query>` | Filtra URLs pela consulta de busca |
| `--sitemap <mode>` | Tratamento de sitemap: `include`, `skip`, `only` |
| `--include-subdomains` | Inclui subdom√≠nios |
| `--ignore-query-parameters` | Trata URLs com par√¢metros diferentes como iguais |
| `--wait` | Aguarda o t√©rmino do mapeamento |
| `--timeout <seconds>` | Tempo m√°ximo de espera, em segundos |
| `--json` | Sa√≠da em JSON |
| `--output <path>` | Salva a sa√≠da em um arquivo |
| `--pretty` | Imprime a sa√≠da JSON formatada |

---

<div id="crawl">
  ### Crawl
</div>

Rastreia um site inteiro a partir de uma URL.

<CrawlBasic />

<div id="check-crawl-status">
  #### Verificar status do crawl
</div>

<CrawlStatus />

<div id="crawl-options">
  #### Op√ß√µes de Crawl
</div>

<CrawlOptions />

**Op√ß√µes dispon√≠veis:**

| Op√ß√£o | Descri√ß√£o |
|--------|-------------|
| `--url <url>` | URL para rastrear (alternativa ao argumento posicional) |
| `--wait` | Aguardar a conclus√£o do crawl |
| `--progress` | Mostrar indicador de progresso enquanto aguarda |
| `--poll-interval <seconds>` | Intervalo de consulta (polling) (padr√£o: 5) |
| `--timeout <seconds>` | Tempo limite ao aguardar |
| `--status` | Verificar o status de uma tarefa de crawl existente |
| `--limit <number>` | N√∫mero m√°ximo de p√°ginas a rastrear |
| `--max-depth <number>` | Profundidade m√°xima do crawl |
| `--include-paths <paths>` | Caminhos a incluir (separados por v√≠rgula) |
| `--exclude-paths <paths>` | Caminhos a excluir (separados por v√≠rgula) |
| `--sitemap <mode>` | Tratamento de sitemap: `include`, `skip`, `only` |
| `--allow-subdomains` | Incluir subdom√≠nios |
| `--allow-external-links` | Seguir links externos |
| `--crawl-entire-domain` | Rastrear o dom√≠nio inteiro |
| `--ignore-query-parameters` | Tratar URLs com par√¢metros diferentes como iguais |
| `--delay <ms>` | Atraso entre requisi√ß√µes |
| `--max-concurrency <n>` | M√°ximo de requisi√ß√µes concorrentes |
| `--output <path>` | Salvar resultado em arquivo |
| `--pretty` | Imprimir sa√≠da JSON formatada |

---

### Agent

Busque e colete dados na web usando prompts em linguagem natural.

<AgentBasic />

<div id="agent-options">
  #### Op√ß√µes do Agente
</div>

<AgentOptions />

**Op√ß√µes dispon√≠veis:**

| Option | Description |
|--------|-------------|
| `--urls <urls>` | Lista opcional de URLs nas quais o agente deve focar (separadas por v√≠rgula) |
| `--model <model>` | Modelo a ser usado: `spark-1-mini` (padr√£o, 60% mais barato) ou `spark-1-pro` (maior precis√£o) |
| `--schema <json>` | Esquema JSON para sa√≠da estruturada (string JSON inline) |
| `--schema-file <path>` | Caminho para o arquivo de esquema JSON para sa√≠da estruturada |
| `--max-credits <number>` | M√°ximo de cr√©ditos a consumir (a tarefa falha se o limite for atingido) |
| `--status` | Verificar o status de uma tarefa de agente existente |
| `--wait` | Aguardar o agente concluir antes de retornar os resultados |
| `--poll-interval <seconds>` | Intervalo de polling enquanto aguarda (padr√£o: 5) |
| `--timeout <seconds>` | Tempo limite (timeout) enquanto aguarda (padr√£o: sem limite) |
| `--output <path>` | Salvar a sa√≠da em arquivo |
| `--json` | Sa√≠da em formato JSON |
| `--pretty` | Exibir sa√≠da JSON formatada (pretty print) |

---

<div id="credit-usage">
  ### Uso de cr√©ditos
</div>

Verifique o saldo de cr√©ditos e o uso pela sua equipe.

```bash CLI
# Ver uso de cr√©ditos
firecrawl credit-usage

# Sa√≠da em JSON
firecrawl credit-usage --json --pretty
```

***


<div id="version">
  ### Vers√£o
</div>

Exibe a vers√£o da CLI.

```bash CLI
firecrawl version
# ou
firecrawl --version
```


<div id="global-options">
  ## Op√ß√µes globais
</div>

Essas op√ß√µes est√£o dispon√≠veis para todos os comandos:

| Op√ß√£o | Atalho | Descri√ß√£o |
|--------|-------|-------------|
| `--status` | | Exibe a vers√£o, autentica√ß√£o, concorr√™ncia e cr√©ditos |
| `--api-key <key>` | `-k` | Substitui a chave de API armazenada para este comando |
| `--api-url <url>` | | Usa uma URL de API personalizada (para self-hosting/desenvolvimento local) |
| `--help` | `-h` | Exibe a ajuda de um comando |
| `--version` | `-V` | Exibe a vers√£o da CLI |

<div id="output-handling">
  ## Manipula√ß√£o da sa√≠da
</div>

A CLI envia a sa√≠da para stdout por padr√£o, facilitando o uso de pipes ou redirecionamentos:

```bash CLI
# Pipe markdown para outro comando
firecrawl https://example.com | head -50

# Redirecionar para um arquivo
firecrawl https://example.com > output.md

# Salvar JSON com formata√ß√£o leg√≠vel
firecrawl https://example.com --format markdown,links --pretty -o data.json
```


<div id="format-behavior">
  ### Comportamento dos formatos
</div>

* **Formato √∫nico**: Retorna o conte√∫do bruto (texto markdown, HTML, etc.)
* **M√∫ltiplos formatos**: Retorna um JSON com todos os dados solicitados

```bash CLI
# Raw markdown output
firecrawl https://example.com --format markdown

# Sa√≠da JSON com m√∫ltiplos formatos
firecrawl https://example.com --format markdown,links
```


<div id="examples">
  ## Exemplos
</div>

<div id="quick-scrape">
  ### Raspagem r√°pida
</div>

```bash CLI
# Obter conte√∫do markdown de uma URL (use --only-main-content para sa√≠da limpa)
firecrawl https://docs.firecrawl.dev --only-main-content

# Get HTML content
firecrawl https://example.com --html -o page.html
```


<div id="full-site-crawl">
  ### Rastreamento completo do site
</div>

```bash CLI
# Rastreia um site de docs com limites
firecrawl crawl https://docs.example.com --limit 50 --max-depth 2 --wait --progress -o docs.json
```


<div id="site-discovery">
  ### Descoberta de sites
</div>

```bash CLI
# Encontre todas as postagens do blog
firecrawl map https://example.com --search "blog" -o blog-urls.txt
```


<div id="research-workflow">
  ### Fluxo de Pesquisa
</div>

```bash CLI
# Buscar e raspar resultados para pesquisa
firecrawl search "machine learning best practices 2024" --scrape --scrape-formats markdown --pretty
```


<div id="agent">
  ### Agente
</div>

```bash CLI
# URLs are optional
firecrawl agent "Encontre as 5 principais startups de IA e seus valores de financiamento" --wait

# Focus on specific URLs
firecrawl agent "Compare pricing plans" --urls https://slack.com/pricing,https://teams.microsoft.com/pricing --wait
```


<div id="combine-with-other-tools">
  ### Combine com outras ferramentas
</div>

```bash CLI
# Extrair URLs dos resultados de pesquisa
jq -r '.data.web[].url' search-results.json

# Get titles from search results
jq -r '.data.web[] | "\(.title): \(.url)"' search-results.json

# Extract links and process with jq
firecrawl https://example.com --format links | jq '.links[].url'

# Count URLs from map
firecrawl map https://example.com | wc -l
```


<div id="telemetry">
  ## Telemetria
</div>

A CLI coleta dados de uso an√¥nimos durante a autentica√ß√£o para ajudar a melhorar o produto:

* Vers√£o da CLI, sistema operacional e vers√£o do Node.js
* Detec√ß√£o de ferramenta de desenvolvimento (como Cursor, VS Code, Claude Code)

**Nenhum dado relacionado a comandos, URLs ou conte√∫do de arquivos √© coletado via CLI.**

Para desativar a telemetria, defina a seguinte vari√°vel de ambiente:

```bash CLI
export FIRECRAWL_NO_TELEMETRY=1
```


<div id="open-source">
  ## Open Source
</div>

A CLI e a Skill do Firecrawl s√£o de c√≥digo aberto e est√£o dispon√≠veis no GitHub: [firecrawl/cli](https://github.com/firecrawl/cli)