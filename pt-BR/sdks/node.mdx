---
title: 'Node'
description: 'O Firecrawl Node SDK é um SDK que encapsula a Firecrawl API para ajudar você a transformar sites em Markdown com facilidade.'
icon: 'node'
og:title: "Node SDK | Firecrawl"
og:description: "O Firecrawl Node SDK é um SDK que encapsula a Firecrawl API para ajudar você a transformar sites em Markdown com facilidade."
---

import InstallationNode from '/snippets/pt-BR/v2/installation/js.mdx'
import ScrapeAndCrawlExampleNode from '/snippets/pt-BR/v2/scrape-and-crawl/js.mdx'
import ScrapeNodeShort from '/snippets/pt-BR/v2/scrape/short/js.mdx'
import CrawlNodeShort from '/snippets/pt-BR/v2/crawl/short/js.mdx'
import CrawlSitemapOnlyNode from '/snippets/pt-BR/v2/crawl/sitemap-only/js.mdx'
import StartCrawlNodeShort from '/snippets/pt-BR/v2/start-crawl/short/js.mdx'
import CheckCrawlStatusNodeShort from '/snippets/pt-BR/v2/crawl-status/short/js.mdx'
import CancelCrawlNodeShort from '/snippets/pt-BR/v2/crawl-delete/short/js.mdx'
import MapNodeShort from '/snippets/pt-BR/v2/map/short/js.mdx'
import ExtractNodeShort from '/snippets/v2/extract/short/js.mdx'
import CrawlWebSocketNodeBase from '/snippets/pt-BR/v2/crawl-websocket/base/js.mdx'


<div id="installation">
  ## Instalação
</div>

Para instalar o SDK do Firecrawl para Node, você pode usar o npm:

<InstallationNode />

<div id="usage">
  ## Uso
</div>

1. Obtenha uma chave de API em [firecrawl.dev](https://firecrawl.dev)
2. Defina a chave de API como uma variável de ambiente chamada `FIRECRAWL_API_KEY` ou passe-a como parâmetro para a classe `FirecrawlApp`.

Veja um exemplo de como usar o SDK com tratamento de erros:

<ScrapeAndCrawlExampleNode />

<div id="scraping-a-url">
  ### Extraindo dados de uma URL
</div>

Para extrair dados de uma única URL com tratamento de erros, use o método `scrapeUrl`. Ele recebe a URL como parâmetro e retorna os dados coletados como um dicionário.

<ScrapeNodeShort />

<div id="crawling-a-website">
  ### Rastreamento de um site
</div>

Para rastrear um site com tratamento de erros, use o método `crawlUrl`. Ele recebe a URL inicial e parâmetros opcionais como argumentos. O parâmetro `params` permite especificar opções adicionais para a tarefa de rastreamento, como o número máximo de páginas a rastrear, domínios permitidos e o formato de saída. Veja [Pagination](#pagination) para paginação automática/manual e limites.

<CrawlNodeShort />

<div id="sitemap-only-crawl">
  ### Rastreamento Somente do Sitemap
</div>

Use `sitemap: "only"` para rastrear apenas URLs do sitemap (a URL inicial sempre é incluída e a descoberta de links em HTML é ignorada).

<CrawlSitemapOnlyNode />

<div id="start-a-crawl">
  ### Iniciar um Crawl
</div>

Inicie um job sem esperar usando `startCrawl`. Ele retorna um `ID` de job que você pode usar para verificar o status. Use `crawl` quando quiser um “waiter” que bloqueia até a conclusão. Veja [Paginação](#pagination) para comportamento e limites de paginação.

<StartCrawlNodeShort />

<div id="checking-crawl-status">
  ### Verificando o status do rastreamento
</div>

Para verificar o status de um trabalho de rastreamento com tratamento de erros, use o método `checkCrawlStatus`. Ele recebe o `ID` como parâmetro e retorna o status atual do trabalho de rastreamento.

<CheckCrawlStatusNodeShort />

<div id="cancelling-a-crawl">
  ### Cancelando um Crawl
</div>

Para cancelar um job de crawl, use o método `cancelCrawl`. Ele recebe o ID do job retornado por `startCrawl` como parâmetro e retorna o status do cancelamento.

<CancelCrawlNodeShort />

<div id="mapping-a-website">
  ### Mapeando um site
</div>

Para mapear um site com tratamento de erros, use o método `mapUrl`. Ele recebe a URL inicial como parâmetro e retorna os dados mapeados como um dicionário.

<MapNodeShort />

{/* ### Extraindo dados estruturados de sites

  Para extrair dados estruturados de sites com tratamento de erros, use o método `extractUrl`. Ele recebe a URL inicial como parâmetro e retorna os dados extraídos como um dicionário.

  <ExtractNodeShort /> */}

<div id="crawling-a-website-with-websockets">
  ### Rastreando um site com WebSockets
</div>

Para rastrear um site com WebSockets, use o método `crawlUrlAndWatch`. Ele recebe a URL inicial e parâmetros opcionais como argumentos. O parâmetro `params` permite especificar opções adicionais para a tarefa de rastreamento, como o número máximo de páginas a rastrear, os domínios permitidos e o formato de saída.

<CrawlWebSocketNodeBase />

<div id="pagination">
  ### Paginação
</div>

Os endpoints do Firecrawl para crawl e batch retornam uma URL `next` quando há mais dados disponíveis. O SDK de Node faz paginação automática por padrão e agrega todos os documentos; nesse caso, `next` será `null`. Você pode desativar a paginação automática ou definir limites.

<div id="crawl">
  #### Rastreamento
</div>

Use o método waiter `crawl` para a experiência mais simples, ou inicie um job e faça a paginação manualmente.

<div id="simple-crawl-auto-pagination-default">
  ##### Rastreamento simples (paginação automática, padrão)
</div>

* Veja o fluxo padrão em [Rastrear um site](#crawling-a-website).

<div id="manual-crawl-with-pagination-control-single-page">
  ##### Rastreamento manual com controle de paginação (página única)
</div>

* Inicie um job e, em seguida, recupere uma página por vez com `autoPaginate: false`.

```js Node
const crawlStart = await firecrawl.startCrawl('https://docs.firecrawl.dev', { limit: 5 });
const crawlJobId = crawlStart.id;

const crawlSingle = await firecrawl.getCrawlStatus(crawlJobId, { autoPaginate: false });
console.log('rastreamento de página única:', crawlSingle.status, 'docs:', crawlSingle.data.length, 'próximo:', crawlSingle.next);
```

<div id="manual-crawl-with-limits-auto-pagination-early-stop">
  ##### Rastreamento manual com limites (paginação automática + parada antecipada)
</div>

* Mantenha a paginação automática ativada, mas interrompa antecipadamente com `maxPages`, `maxResults` ou `maxWaitTime`.

```js Node
const crawlLimited = await firecrawl.getCrawlStatus(crawlJobId, {
  autoPaginate: true,
  maxPages: 2,
  maxResults: 50,
  maxWaitTime: 15,
});
console.log('crawl limitado:', crawlLimited.status, 'docs:', crawlLimited.data.length, 'próximo:', crawlLimited.next);
```

<div id="batch-scrape">
  #### Coleta em lote
</div>

Use o método waiter `batchScrape` ou inicie uma tarefa e pagine manualmente.

<div id="simple-batch-scrape-auto-pagination-default">
  ##### Raspagem em lote simples (paginação automática, padrão)
</div>

* Veja o fluxo padrão em [Raspagem em lote](/pt-BR/features/batch-scrape).

<div id="manual-batch-scrape-with-pagination-control-single-page">
  ##### Coleta manual em lote com controle de paginação (página única)
</div>

* Inicie um job e, em seguida, recupere uma página por vez com `autoPaginate: false`.

```js Node
const batchStart = await firecrawl.startBatchScrape([
  'https://docs.firecrawl.dev',
  'https://firecrawl.dev',
], { options: { formats: ['markdown'] } });
const batchJobId = batchStart.id;

const batchSingle = await firecrawl.getBatchScrapeStatus(batchJobId, { autoPaginate: false });
console.log('página única do lote:', batchSingle.status, 'docs:', batchSingle.data.length, 'próximo:', batchSingle.next);
```

<div id="manual-batch-scrape-with-limits-auto-pagination-early-stop">
  ##### Coleta manual em lote com limites (paginação automática + parada antecipada)
</div>

* Mantenha a paginação automática ligada, mas interrompa antes com `maxPages`, `maxResults` ou `maxWaitTime`.

```js Node
const batchLimited = await firecrawl.getBatchScrapeStatus(batchJobId, {
  autoPaginate: true,
  maxPages: 2,
  maxResults: 100,
  maxWaitTime: 20,
});
console.log('lote limitado:', batchLimited.status, 'docs:', batchLimited.data.length, 'próximo:', batchLimited.next);
```

<div id="error-handling">
  ## Tratamento de erros
</div>

O SDK trata os erros retornados pela API do Firecrawl e lança as exceções apropriadas. Se ocorrer um erro durante uma solicitação, uma exceção será lançada com uma mensagem descritiva. Os exemplos acima mostram como lidar com esses erros usando blocos `try/catch`.