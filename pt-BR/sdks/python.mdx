---
title: 'Python'
description: 'O Firecrawl Python SDK é um encapsulador da Firecrawl API que ajuda você a transformar sites em Markdown com facilidade.'
icon: 'python'
og:title: "SDK Python | Firecrawl"
og:description: "O Firecrawl Python SDK é um encapsulador da Firecrawl API que ajuda você a transformar sites em Markdown com facilidade."
---

import InstallationPython from '/snippets/pt-BR/v2/installation/python.mdx'
import ScrapePythonShort from '/snippets/pt-BR/v2/scrape/short/python.mdx'
import CrawlPythonShort from '/snippets/pt-BR/v2/crawl/short/python.mdx'
import CheckCrawlStatusPythonShort from '/snippets/pt-BR/v2/crawl-status/short/python.mdx'
import StartCrawlPythonShort from '/snippets/pt-BR/v2/start-crawl/short/python.mdx'
import CancelCrawlPythonShort from '/snippets/pt-BR/v2/crawl-delete/short/python.mdx'
import MapPythonShort from '/snippets/pt-BR/v2/map/short/python.mdx'
import ExtractPythonShort from '/snippets/v2/extract/short/python.mdx'
import ScrapeAndCrawlExamplePython from '/snippets/pt-BR/v2/scrape-and-crawl/python.mdx'
import CrawlWebSocketPythonBase from '/snippets/pt-BR/v2/crawl-websocket/base/python.mdx'
import AIOPython from '/snippets/pt-BR/v2/async/base/python.mdx'

<div id="installation">
  ## Instalação
</div>

Para instalar o SDK do Firecrawl para Python, você pode usar o pip:

<InstallationPython />

<div id="usage">
  ## Uso
</div>

1. Obtenha uma chave de API em [firecrawl.dev](https://firecrawl.dev)
2. Configure a chave de API como uma variável de ambiente chamada `FIRECRAWL_API_KEY` ou passe-a como parâmetro para a classe `Firecrawl`.

Veja um exemplo de uso do SDK:

<ScrapeAndCrawlExamplePython />

<div id="scraping-a-url">
  ### Extraindo dados de uma URL
</div>

Para extrair dados de uma única URL, use o método `scrape`. Ele recebe a URL como parâmetro e retorna o documento raspado.

<ScrapePythonShort />

<div id="crawl-a-website">
  ### Rastrear um site
</div>

Para rastrear um site, use o método `crawl`. Ele recebe a URL inicial e, opcionalmente, um objeto de opções. Essas opções permitem definir configurações adicionais para a tarefa de rastreamento, como o número máximo de páginas, os domínios permitidos e o formato de saída. Consulte [Paginação](#pagination) para detalhes sobre paginação automática/manual e limites.

<CrawlPythonShort />

<div id="start-a-crawl">
  ### Iniciar um crawl
</div>

<Tip>Prefere não bloquear? Veja a seção [Classe assíncrona](#async-class) abaixo.</Tip>

Inicie uma tarefa sem esperar usando `start_crawl`. Ela retorna um `ID` de tarefa que você pode usar para verificar o status. Use `crawl` quando quiser um aguardador que bloqueia até a conclusão. Consulte [Paginação](#pagination) para o comportamento e os limites de paginação.

<StartCrawlPythonShort />

<div id="checking-crawl-status">
  ### Verificando o status do crawl
</div>

Para verificar o status de uma tarefa de crawl, use o método `get_crawl_status`. Ele recebe o ID da tarefa como parâmetro e retorna o status atual do crawl.

<CheckCrawlStatusPythonShort />

<div id="cancelling-a-crawl">
  ### Cancelando um Crawl
</div>

Para cancelar um job de crawl, use o método `cancel_crawl`. Ele recebe o ID do job do `start_crawl` como parâmetro e retorna o status do cancelamento.

<CancelCrawlPythonShort />

<div id="map-a-website">
  ### Mapear um site
</div>

Use `map` para gerar uma lista de URLs de um site. As opções permitem personalizar o processo de mapeamento, incluindo excluir subdomínios ou usar o sitemap.

<MapPythonShort />

{/* ### Extração de dados estruturados de sites

  Para extrair dados estruturados de sites, use o método `extract`. Ele recebe as URLs das quais extrair dados, um prompt e um schema como argumentos. O schema é um modelo Pydantic que define a estrutura dos dados extraídos.

  <ExtractPythonShort /> */}

<div id="crawling-a-website-with-websockets">
  ### Rastreamento de um site com WebSockets
</div>

Para rastrear um site com WebSockets, inicie a tarefa com `start_crawl` e faça a inscrição usando o helper `watcher`. Crie um watcher com o ID da tarefa e vincule handlers (por exemplo, para page, completed, failed) antes de chamar `start()`.

<CrawlWebSocketPythonBase />

<div id="pagination">
  ### Paginação
</div>

Os endpoints do Firecrawl para crawl e batch retornam uma URL `next` quando há mais dados disponíveis. O SDK Python pagina automaticamente por padrão e agrega todos os documentos; nesse caso, `next` será `None`. Você pode desativar a paginação automática ou definir limites.

<div id="crawl">
  #### Crawl
</div>

Use o método de espera `crawl` para a experiência mais simples ou inicie um job e faça a paginação manualmente.

<div id="simple-crawl-auto-pagination-default">
  ##### Rastreamento simples (paginação automática, padrão)
</div>

* Veja o fluxo padrão em [Rastrear um site](#crawl-a-website).

<div id="manual-crawl-with-pagination-control-single-page">
  ##### Rastreamento manual com controle de paginação (página única)
</div>

* Inicie um job e, em seguida, recupere uma página por vez com `auto_paginate=False`.

```python Python
crawl_job = client.start_crawl("https://example.com", limit=100)

status = client.get_crawl_status(crawl_job.id, pagination_config=PaginationConfig(auto_paginate=False))
print("rastrear página única:", status.status, "docs:", len(status.data), "próximo:", status.next)
```

<div id="manual-crawl-with-limits-auto-pagination-early-stop">
  ##### Rastreamento manual com limites (paginação automática + interrupção antecipada)
</div>

* Mantenha a paginação automática ativada, mas interrompa antecipadamente com `max_pages`, `max_results` ou `max_wait_time`.

```python Python
status = client.get_crawl_status(
    crawl_job.id,
    pagination_config=PaginationConfig(max_pages=2, max_results=50, max_wait_time=15),
)
print("rastreamento limitado:", status.status, "docs:", len(status.data), "próximo:", status.next)
```

<div id="batch-scrape">
  #### Coleta em lote
</div>

Use o método waiter `batch_scrape` ou inicie um job e faça a paginação manualmente.

<div id="simple-batch-scrape-auto-pagination-default">
  ##### Coleta em lote simples (paginação automática, padrão)
</div>

* Veja o fluxo padrão em [Coleta em Lote](/pt-BR/features/batch-scrape).

<div id="manual-batch-scrape-with-pagination-control-single-page">
  ##### Raspagem manual em lote com controle de paginação (página única)
</div>

* Inicie um job e recupere uma página por vez com `auto_paginate=False`.

```python Python
batch_job = client.start_batch_scrape(urls)
status = client.get_batch_scrape_status(batch_job.id, pagination_config=PaginationConfig(auto_paginate=False))
print("lote página única:", status.status, "docs:", len(status.data), "próximo:", status.next)
```

<div id="manual-batch-scrape-with-limits-auto-pagination-early-stop">
  ##### Coleta manual em lote com limites (paginação automática + parada antecipada)
</div>

* Deixe a paginação automática ativada, mas interrompa antes usando `max_pages`, `max_results` ou `max_wait_time`.

```python Python
status = client.get_batch_scrape_status(
    batch_job.id,
    pagination_config=PaginationConfig(max_pages=2, max_results=100, max_wait_time=20),
)
print("lote limitado:", status.status, "docs:", len(status.data), "próximo:", status.next)
```

<div id="error-handling">
  ## Tratamento de erros
</div>

O SDK trata os erros retornados pela API do Firecrawl e lança exceções apropriadas. Se ocorrer um erro durante uma requisição, uma exceção será lançada com uma mensagem descritiva.

<div id="async-class">
  ## Classe assíncrona
</div>

Para operações assíncronas, use a classe `AsyncFirecrawl`. Seus métodos espelham os de `Firecrawl`, mas não bloqueiam a thread principal.

<AIOPython />