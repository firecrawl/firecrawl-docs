---
title: 'Crawlear'
description: 'O Firecrawl pode percorrer recursivamente os subdomínios de uma URL e coletar o conteúdo'
og:title: 'Crawlear | Firecrawl'
og:description: 'O Firecrawl pode percorrer recursivamente os subdomínios de uma URL e coletar o conteúdo'
icon: 'spider'
---

import InstallationPython from '/snippets/pt-BR/v2/installation/python.mdx';
import InstallationNode from '/snippets/pt-BR/v2/installation/js.mdx';
import InstallationCLI from '/snippets/pt-BR/v2/installation/cli.mdx';
import CrawlPython from '/snippets/pt-BR/v2/crawl/base/python.mdx';
import CrawlNode from '/snippets/pt-BR/v2/crawl/base/js.mdx';
import CrawlCURL from '/snippets/pt-BR/v2/crawl/base/curl.mdx';
import CrawlCLI from '/snippets/pt-BR/v2/crawl/base/cli.mdx';
import CheckCrawlJobPython from '/snippets/pt-BR/v2/crawl-status/short/python.mdx';
import CheckCrawlJobNode from '/snippets/pt-BR/v2/crawl-status/short/js.mdx';
import CheckCrawlJobCURL from '/snippets/pt-BR/v2/crawl-status/short/curl.mdx';
import CheckCrawlJobCLI from '/snippets/pt-BR/v2/crawl-status/short/cli.mdx';
import CheckCrawlJobOutputScraping from '/snippets/pt-BR/v2/crawl-status/base/output-scraping.mdx';
import CheckCrawlJobOutputCompleted from '/snippets/pt-BR/v2/crawl-status/base/output-completed.mdx';
import CrawlWebSocketPython from '/snippets/pt-BR/v2/crawl-websocket/base/python.mdx';
import CrawlWebSocketNode from '/snippets/pt-BR/v2/crawl-websocket/base/js.mdx';
import CrawlWebhookCURL from '/snippets/pt-BR/v2/crawl-webhook/base/curl.mdx';
import PythonCrawlExample from '/snippets/pt-BR/v2/crawl/sdk-example/python.mdx';
import NodeCrawlExample from '/snippets/pt-BR/v2/crawl/sdk-example/js.mdx';
import PythonCrawlExampleResponse from '/snippets/pt-BR/v2/crawl/sdk-example/python-response.mdx';
import NodeCrawlExampleResponse from '/snippets/pt-BR/v2/crawl/sdk-example/js-response.mdx';
import StartCrawlPython from '/snippets/pt-BR/v2/start-crawl/base/python.mdx';
import StartCrawlNode from '/snippets/pt-BR/v2/start-crawl/base/js.mdx';
import StartCrawlCURL from '/snippets/pt-BR/v2/start-crawl/base/curl.mdx';
import StartCrawlCLI from '/snippets/pt-BR/v2/start-crawl/base/cli.mdx';
import StartCrawlOutput from '/snippets/pt-BR/v2/start-crawl/base/output.mdx';

Firecrawl rastreia sites com eficiência para extrair dados completos, lidando com infraestrutura web complexa. O processo:

1. **Análise de URL:** Examina o sitemap e percorre o site para identificar links
2. **Navegação:** Segue links recursivamente para encontrar todas as subpáginas
3. **Extração:** Extrai o conteúdo de cada página, lidando com JS e limites de taxa
4. **Resultado:** Converte os dados em markdown limpo ou em formato estruturado

Isso garante uma coleta abrangente de dados a partir de qualquer URL inicial.

<div id="crawling">
  ## Rastreamento
</div>

<div id="crawl-endpoint">
  ### endpoint /crawl
</div>

Usado para rastrear uma URL e todas as subpáginas acessíveis. Isso cria um job de rastreamento e retorna um ID de job para acompanhar o status do rastreamento.

<Warning>
  Por padrão, o Crawl ignora sublinks de uma página se eles não forem filhos da
  URL fornecida. Assim, website.com/other-parent/blog-1 não será
  retornado se você rastrear website.com/blogs/. Se você quiser
  website.com/other-parent/blog-1, use o parâmetro `crawlEntireDomain`. Para
  rastrear subdomínios como blog.website.com ao rastrear website.com, use o
  parâmetro `allowSubdomains`.
</Warning>

<Info>
  Cada página rastreada consome 1 crédito (mais créditos adicionais para opções como modo JSON, proxy aprimorado ou análise de PDF). O `limit` padrão é 10.000 páginas. Defina um `limit` menor para controlar o uso de créditos — por exemplo, `limit: 100` para limitar o rastreamento a 100 páginas.
</Info>

<div id="installation">
  ### Instalação
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationCLI />
</CodeGroup>

<div id="usage">
  ### Como usar
</div>

<CodeGroup>
  <CrawlPython />

  <CrawlNode />

  <CrawlCURL />

  <CrawlCLI />
</CodeGroup>

<div id="scrape-options-in-crawl">
  ### Opções de scrape no crawl
</div>

Todas as opções do endpoint Scrape estão disponíveis no Crawl via `scrapeOptions` (JS) / `scrape_options` (Python). Elas se aplicam a cada página que o crawler coleta: formatos, proxy, cache, ações, localização, tags etc. Veja a lista completa na [Referência da API de Scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<CodeGroup>
  ```js Node
  import Firecrawl from '@mendable/firecrawl-js';

  const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR_API_KEY' });

  // Crawl with scrape options
  const crawlResponse = await firecrawl.crawl('https://example.com', {
    limit: 100,
    scrapeOptions: {
      formats: [
        'markdown',
        {
          type: 'json',
          schema: { type: 'object', properties: { title: { type: 'string' } } },
        },
      ],
      proxy: 'auto',
      maxAge: 600000,
      onlyMainContent: true,
    },
  });
  ```

  ```python Python
  from firecrawl import Firecrawl

  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  # Crawl with scrape options
  response = firecrawl.crawl('https://example.com',
      limit=100,
      scrape_options={
          'formats': [
              'markdown',
              { 'type': 'json', 'schema': { 'type': 'object', 'properties': { 'title': { 'type': 'string' } } } }
          ],
          'proxy': 'auto',
          'maxAge': 600000,
          'onlyMainContent': True
      }
  )
  ```
</CodeGroup>

<div id="api-response">
  ### Resposta da API
</div>

Se você estiver usando cURL ou o método starter, será retornado um `ID` para verificar o status do crawl.

<Note>
  Se você estiver usando o SDK, consulte os métodos abaixo para entender o comportamento de waiter vs starter.
</Note>

<StartCrawlOutput />

<div id="check-crawl-job">
  ### Verificar Job de Rastreamento
</div>

Usado para verificar o status de um job de rastreamento e obter seu resultado.

<Note>
  Os resultados do job ficam disponíveis via API por 24 horas após a conclusão. Após esse período, você ainda pode ver o histórico e os resultados dos seus rastreamentos nos [activity logs](https://www.firecrawl.dev/app/logs).
</Note>

<CodeGroup>
  <CheckCrawlJobPython />

  <CheckCrawlJobNode />

  <CheckCrawlJobCURL />

  <CheckCrawlJobCLI />
</CodeGroup>

<div id="response-handling">
  #### Tratamento de respostas
</div>

A resposta varia conforme o status da varredura.

Para respostas não concluídas ou grandes (acima de 10 MB), é fornecido um parâmetro de URL `next`. Você deve requisitar essa URL para obter os próximos 10 MB de dados. Se o parâmetro `next` estiver ausente, isso indica o fim dos dados da varredura.

O parâmetro skip define o número máximo de resultados retornados em cada bloco.

<Info>
  Os parâmetros skip e next são relevantes apenas ao acessar a API diretamente.
  Se você estiver usando o SDK, nós cuidamos disso para você e retornaremos
  todos os resultados de uma vez.
</Info>

<CodeGroup>
  <CheckCrawlJobOutputScraping />

  <CheckCrawlJobOutputCompleted />
</CodeGroup>

<div id="sdk-methods">
  ### Métodos do SDK
</div>

Existem duas maneiras de usar o SDK:

1. **Crawl e aguarde** (`crawl`):
   * Aguarda a conclusão do crawl e retorna a resposta completa
   * Faz a paginação automaticamente
   * Recomendado para a maioria dos casos de uso

<CodeGroup>
  <PythonCrawlExample />

  <NodeCrawlExample />
</CodeGroup>

A resposta inclui o status do crawl e todos os dados extraídos:

<CodeGroup>
  <PythonCrawlExampleResponse />

  <NodeCrawlExampleResponse />
</CodeGroup>

2. **Inicie e depois verifique o status** (`startCrawl`/`start_crawl`):
   * Retorna imediatamente com um ID de crawl
   * Permite verificar o status manualmente
   * Útil para crawls de longa duração ou lógica de polling personalizada

<CodeGroup>
  <StartCrawlPython />

  <StartCrawlNode />

  <StartCrawlCURL />

  <StartCrawlCLI />
</CodeGroup>

<div id="crawl-websocket">
  ## WebSocket de Crawl
</div>

O método do Firecrawl baseado em WebSocket, `Crawl URL and Watch`, permite extrair e monitorar dados em tempo real. Inicie um crawl a partir de uma URL e personalize-o com opções como limite de páginas, domínios permitidos e formatos de saída — ideal para necessidades de processamento de dados imediatas.

<CodeGroup>
  <CrawlWebSocketPython />

  <CrawlWebSocketNode />
</CodeGroup>

<div id="crawl-webhook">
  ## Webhook de Rastreamento
</div>

Você pode configurar webhooks para receber notificações em tempo real conforme o rastreamento avança. Isso permite processar páginas à medida que são coletadas, em vez de esperar a conclusão de todo o rastreamento.

<CrawlWebhookCURL />

<div id="quick-reference">
  ### Referência rápida
</div>

**Tipos de eventos:**

* `crawl.started` - Quando o rastreamento começa
* `crawl.page` - Para cada página rastreada com sucesso
* `crawl.completed` - Quando o rastreamento termina
* `crawl.failed` - Se o rastreamento encontrar um erro

**Payload básico:**

```json
{
  "success": true,
  "type": "crawl.page",
  "id": "crawl-job-id",
  "data": [...], // Dados da página para eventos 'page'
  "metadata": {}, // Your custom metadata
  "error": null
}
```

<div id="security-verifying-webhook-signatures">
  ### Segurança: Verificando Assinaturas de Webhook
</div>

Toda requisição de webhook do Firecrawl inclui um cabeçalho `X-Firecrawl-Signature` contendo uma assinatura HMAC-SHA256. **Sempre verifique essa assinatura** para garantir que o webhook é autêntico e não foi adulterado.

**Como funciona:**

1. Obtenha seu segredo de webhook na [aba Advanced](https://www.firecrawl.dev/app/settings?tab=advanced) das configurações da sua conta
2. Extraia a assinatura do cabeçalho `X-Firecrawl-Signature`
3. Calcule o HMAC-SHA256 do corpo bruto da requisição usando o seu segredo
4. Compare com o cabeçalho de assinatura usando uma função com proteção contra ataques de timing (tempo constante)

<Warning>
  Nunca processe um webhook sem verificar sua assinatura primeiro. O cabeçalho `X-Firecrawl-Signature` contém a assinatura no formato: `sha256=abc123def456...`
</Warning>

Para exemplos completos de implementação em JavaScript e Python, consulte a [documentação de segurança de webhooks](/pt-BR/webhooks/security).

<div id="full-documentation">
  ### Documentação completa
</div>

Para uma documentação completa sobre webhooks, incluindo payloads de eventos detalhados, estrutura do payload, configuração avançada e solução de problemas, consulte a [documentação de Webhooks](/pt-BR/webhooks/overview).