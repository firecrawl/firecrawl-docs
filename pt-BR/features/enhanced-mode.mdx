---
title: "Modo Avançado"
description: "Use proxies aprimorados para scraping confiável em sites complexos, mantendo a privacidade"
og:title: "Modo Avançado | Firecrawl"
og:description: "Use proxies aprimorados para scraping confiável em sites complexos, mantendo a privacidade"
---

import ProxyPython from "/snippets/pt-BR/v2/scrape/proxy/python.mdx";
import ProxyNode from "/snippets/pt-BR/v2/scrape/proxy/js.mdx";
import ProxyCURL from "/snippets/pt-BR/v2/scrape/proxy/curl.mdx";
import ProxyRetryPython from "/snippets/pt-BR/v2/scrape/proxy-retry/python.mdx";
import ProxyRetryNode from "/snippets/pt-BR/v2/scrape/proxy-retry/js.mdx";
import ProxyRetryCURL from "/snippets/pt-BR/v2/scrape/proxy-retry/curl.mdx";

Firecrawl oferece diferentes tipos de proxy para ajudar você a fazer scraping de sites com diferentes níveis de complexidade. O tipo de proxy pode ser especificado usando o parâmetro `proxy`.


<div id="proxy-types">
  ### Tipos de Proxy
</div>

Firecrawl oferece suporte a três tipos de proxy:

- **basic**: Proxies básicos para fazer scraping na maioria dos sites. Rápidos e geralmente funcionam bem.
- **enhanced**: Proxies avançados para fazer scraping em sites complexos mantendo a privacidade. Mais lentos, mas mais confiáveis em determinados sites.
- **auto**: Firecrawl tentará automaticamente repetir o scraping com proxies enhanced se o proxy basic falhar. Se a nova tentativa com enhanced for bem-sucedida, 5 créditos serão cobrados pelo scraping. Se a primeira tentativa com basic for bem-sucedida, apenas o custo normal será cobrado.

Se você não especificar um proxy, o Firecrawl usará auto por padrão.

<div id="using-enhanced-mode">
  ### Usando o Modo Avançado
</div>

Ao fazer scraping de sites complexos, você pode usar o modo avançado (`enhanced mode`) para aumentar sua taxa de sucesso, mantendo a privacidade.

<CodeGroup>

<ProxyPython />

<ProxyNode />

<ProxyCURL />

</CodeGroup>

**Observação:** As requisições com proxy avançado custam 5 créditos por requisição.

<div id="using-enhanced-as-a-retry-mechanism">
  ## Usando Enhanced como mecanismo de nova tentativa
</div>

Um padrão comum é primeiro tentar fazer o scraping com as configurações de proxy padrão e, em seguida, tentar novamente no modo enhanced se você encontrar códigos de status de erro específicos (401, 403 ou 500) no campo `metadata.statusCode` da resposta. Esses códigos de status podem indicar que o site está bloqueando sua requisição.

<CodeGroup>

<ProxyRetryPython />

<ProxyRetryNode />

<ProxyRetryCURL />

</CodeGroup>

Essa abordagem permite otimizar o uso de créditos, usando o modo enhanced apenas quando necessário.