---
title: "Modo Enhanced"
description: "Use proxies Enhanced para extração confiável em sites complexos, mantendo a privacidade"
og:title: "Modo Enhanced | Firecrawl"
og:description: "Use proxies Enhanced para extração confiável em sites complexos, mantendo a privacidade"
---

import ProxyPython from "/snippets/pt-BR/v2/scrape/proxy/python.mdx";
import ProxyNode from "/snippets/pt-BR/v2/scrape/proxy/js.mdx";
import ProxyCURL from "/snippets/pt-BR/v2/scrape/proxy/curl.mdx";
import ProxyRetryPython from "/snippets/pt-BR/v2/scrape/proxy-retry/python.mdx";
import ProxyRetryNode from "/snippets/pt-BR/v2/scrape/proxy-retry/js.mdx";
import ProxyRetryCURL from "/snippets/pt-BR/v2/scrape/proxy-retry/curl.mdx";

A Firecrawl oferece diferentes tipos de proxy para ajudar você a extrair dados de sites com variados níveis de complexidade. O tipo de proxy pode ser especificado usando o parâmetro `proxy`.


<div id="proxy-types">
  ### Tipos de proxy
</div>

Firecrawl oferece suporte a três tipos de proxy:

- **basic**: Proxies para fazer scraping da maioria dos sites. Rápido e geralmente funciona.
- **Enhanced**: Proxies Enhanced para fazer scraping de sites complexos, mantendo a privacidade. Mais lento, mas mais confiável em certos sites.
- **auto**: O Firecrawl tentará automaticamente novamente com proxies Enhanced se o proxy basic falhar. Se a nova tentativa com Enhanced for bem-sucedida, 5 créditos serão cobrados pelo scraping. Se a primeira tentativa com basic for bem-sucedida, apenas o custo regular será cobrado.

Se você não especificar um proxy, o Firecrawl usará auto por padrão.

<div id="using-Enhanced-mode">
  ### Usando o modo Enhanced
</div>

Ao fazer scraping de sites complexos, você pode usar o modo Enhanced para aumentar a taxa de sucesso, mantendo a privacidade.

<CodeGroup>

<ProxyPython />

<ProxyNode />

<ProxyCURL />

</CodeGroup>

**Observação:** Requisições com proxy Enhanced custam 5 créditos cada vez que forem usadas.

<div id="using-Enhanced-as-a-retry-mechanism">
  ## Usando o modo Enhanced como mecanismo de nova tentativa
</div>

Um padrão comum é primeiro tentar o scraping com as configurações padrão de proxy e, em seguida, refazer a tentativa no modo Enhanced se você receber códigos de status de erro específicos (401, 403 ou 500) no campo `metadata.statusCode` da resposta. Esses códigos podem indicar que o site está bloqueando sua solicitação.

<CodeGroup>

<ProxyRetryPython />

<ProxyRetryNode />

<ProxyRetryCURL />

</CodeGroup>

Essa abordagem ajuda a otimizar o uso de créditos, utilizando o modo Enhanced apenas quando necessário.