---
title: "Scrape"
description: "Transforme qualquer URL em dados prontos"
og:title: "Scrape | Firecrawl"
og:description: "Transforme qualquer URL em dados prontos"
---

import InstallationPython from "/snippets/pt-BR/v2/installation/python.mdx";
import InstallationNode from "/snippets/pt-BR/v2/installation/js.mdx";
import ScrapePython from "/snippets/v2/scrape/base/python.mdx";
import ScrapeNode from "/snippets/v2/scrape/base/js.mdx";
import ScrapeCURL from "/snippets/v2/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/v2/scrape/base/output.mdx";
import ExtractCURL from "/snippets/v2/scrape/json/base/curl.mdx";
import ExtractPython from "/snippets/v2/scrape/json/base/python.mdx";
import ExtractNode from "/snippets/v2/scrape/json/base/js.mdx";
import ExtractOutput from "/snippets/v2/scrape/json/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/v2/scrape/json/no-schema/curl.mdx";
import ExtractNoSchemaPython from "/snippets/v2/scrape/json/no-schema/python.mdx";
import ExtractNoSchemaNode from "/snippets/v2/scrape/json/no-schema/js.mdx";
import ExtractNoSchemaOutput from "/snippets/v2/scrape/json/no-schema/output.mdx";
import ScrapeActionsPython from "/snippets/v2/scrape/actions/python.mdx";
import ScrapeActionsNode from "/snippets/v2/scrape/actions/js.mdx";
import ScrapeActionsCURL from "/snippets/v2/scrape/actions/curl.mdx";
import ScrapeActionsOutput from "/snippets/v2/scrape/actions/output.mdx";
import BatchScrapePython from "/snippets/pt-BR/v2/batch-scrape/short/python.mdx";
import BatchScrapeNode from "/snippets/pt-BR/v2/batch-scrape/short/js.mdx";
import BatchScrapeCURL from "/snippets/pt-BR/v2/batch-scrape/short/curl.mdx";
import BatchScrapeOutput from "/snippets/pt-BR/v2/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/pt-BR/v2/batch-scrape/base/async-output.mdx";
import ScrapeLocationPython from "/snippets/v2/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/v2/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/v2/scrape/location/curl.mdx";

Firecrawl converte páginas da web em markdown, ideal para aplicativos com LLMs.

* Cuida das complexidades: proxies, cache, limites de taxa, conteúdo bloqueado por JS
* Lida com conteúdo dinâmico: sites dinâmicos, páginas renderizadas em JS, PDFs, imagens
* Gera markdown limpo, dados estruturados, capturas de tela ou HTML.

Para mais detalhes, consulte a [referência da API do endpoint /scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="scraping-a-url-with-firecrawl">
  ## Extraindo dados de uma URL com o Firecrawl
</div>

<div id="scrape-endpoint">
  ### endpoint /scrape
</div>

Usado para extrair o conteúdo de uma URL.

<div id="installation">
  ### Instalação
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />
</CodeGroup>

<div id="usage">
  ### Uso
</div>

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeCURL />
</CodeGroup>

Para mais detalhes sobre os parâmetros, consulte a [Referência da API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="response">
  ### Resposta
</div>

Os SDKs retornarão o objeto de dados diretamente. O cURL retornará o payload exatamente como mostrado abaixo.

<ScrapeResponse />

<div id="scrape-formats">
  ## Formatos de Scrape
</div>

Agora você pode escolher em quais formatos deseja o seu resultado. Você pode especificar vários formatos de saída. Os formatos compatíveis são:

* Markdown (`markdown`)
* Resumo (`summary`)
* HTML (`html`)
* HTML bruto (`rawHtml`) (sem modificações)
* Captura de tela (`screenshot`, com opções como `fullPage`, `quality`, `viewport`)
* Links (`links`)
* JSON (`json`) - saída estruturada

As chaves de saída corresponderão ao formato que você escolher.

<div id="extract-structured-data">
  ## Extraia dados estruturados
</div>

<div id="scrape-with-json-endpoint">
  ### endpoint /scrape (com json)
</div>

Usado para extrair dados estruturados de páginas extraídas.

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

Resultado:

<ExtractOutput />

<div id="extracting-without-schema">
  ### Extraindo sem esquema
</div>

Agora é possível extrair sem um esquema, bastando enviar um `prompt` para o endpoint. O LLM escolhe a estrutura dos dados.

<CodeGroup>
  <ExtractNoSchemaPython />

  <ExtractNoSchemaNode />

  <ExtractNoSchemaCURL />
</CodeGroup>

Resultado:

<ExtractNoSchemaOutput />

<div id="json-format-options">
  ### Opções do formato JSON
</div>

Ao usar o formato `json`, passe um objeto dentro de `formats` com os seguintes parâmetros:

* `schema`: JSON Schema para a saída estruturada.
* `prompt`: Prompt opcional para orientar a extração quando houver um schema ou quando você preferir uma orientação leve.

<div id="interacting-with-the-page-with-actions">
  ## Interagindo com a página com ações
</div>

O Firecrawl permite executar várias ações em uma página da web antes de fazer o scraping do conteúdo. Isso é especialmente útil para interagir com conteúdo dinâmico, navegar entre páginas ou acessar conteúdo que exige interação do usuário.

Veja um exemplo de como usar ações para acessar google.com, pesquisar por Firecrawl, clicar no primeiro resultado e fazer uma captura de tela.

É importante, quase sempre, usar a ação `wait` antes/depois de executar outras ações para dar tempo suficiente para a página carregar.

<div id="example">
  ### Exemplo
</div>

<CodeGroup>
  <ScrapeActionsPython />

  <ScrapeActionsNode />

  <ScrapeActionsCURL />
</CodeGroup>

<div id="output">
  ### Saída
</div>

<CodeGroup>
  <ScrapeActionsOutput />
</CodeGroup>

Para mais detalhes sobre os parâmetros de ações, consulte a [referência da API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="location-and-language">
  ## Localização e idioma
</div>

Especifique o país e os idiomas preferidos para obter conteúdo relevante com base no seu local de destino e nas suas preferências de idioma.

<div id="how-it-works">
  ### Como funciona
</div>

Quando você define as configurações de localização, o Firecrawl usará um proxy apropriado, se disponível, e emulará as configurações correspondentes de idioma e fuso horário. Por padrão, a localização é definida como “US” se não for especificada.

<div id="usage">
  ### Uso
</div>

Para usar as configurações de localização e idioma, inclua o objeto `location` no corpo da sua requisição com as seguintes propriedades:

* `country`: Código de país ISO 3166-1 alpha-2 (por exemplo, &#39;US&#39;, &#39;AU&#39;, &#39;DE&#39;, &#39;JP&#39;). O padrão é &#39;US&#39;.
* `languages`: Uma lista (array) de idiomas e localidades preferidos para a requisição, em ordem de prioridade. O padrão é o idioma da localização especificada.

<CodeGroup>
  <ScrapeLocationPython />

  <ScrapeLocationNode />

  <ScrapeLocationCURL />
</CodeGroup>

Para mais detalhes sobre as localizações compatíveis, consulte a [documentação de proxies](/pt-BR/features/proxies).

<div id="caching-and-maxage">
  ## Cache e maxAge
</div>

Para acelerar as requisições, o Firecrawl retorna resultados do cache por padrão quando há uma cópia recente disponível.

* **Janela de frescor padrão**: `maxAge = 172800000` ms (2 dias). Se a página em cache for mais recente do que isso, ela é retornada instantaneamente; caso contrário, a página é coletada novamente e então armazenada em cache.
* **Desempenho**: Pode acelerar as coletas em até 5x quando os dados não precisam estar ultra recentes.
* **Sempre buscar conteúdo novo**: Defina `maxAge` como `0`.
* **Evitar armazenamento**: Defina `storeInCache` como `false` se você não quiser que o Firecrawl armazene em cache os resultados desta requisição.

Exemplo (forçar conteúdo novo):

<CodeGroup>
  ```python Python
  from firecrawl import Firecrawl
  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  doc = firecrawl.scrape(url='https://example.com', maxAge=0, formats=['markdown'])
  print(doc)
  ```

  ```js Node
  import Firecrawl from '@mendable/firecrawl-js';

  const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

  const doc = await firecrawl.scrape('https://example.com', { maxAge: 0, formats: ['markdown'] });
  console.log(doc);
  ```

  ```bash cURL
  curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
    -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "url": "https://example.com",
      "maxAge": 0,
      "formats": ["markdown"]
    }'
  ```
</CodeGroup>

Exemplo (usar uma janela de cache de 10 minutos):

<CodeGroup>
  ```python Python
  from firecrawl import Firecrawl
  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  doc = firecrawl.scrape(url='https://example.com', maxAge=600000, formats=['markdown', 'html'])
  print(doc)
  ```

  ```js Node

  const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

  const doc = await firecrawl.scrape('https://example.com', { maxAge: 600000, formats: ['markdown', 'html'] });
  console.log(doc);
  ```

  ```bash cURL
  curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
    -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "url": "https://example.com",
      "maxAge": 600000,
      "formats": ["markdown", "html"]
    }'
  ```
</CodeGroup>

<div id="batch-scraping-multiple-urls">
  ## Scraping em lote de várias URLs
</div>

Agora é possível fazer scraping em lote de várias URLs ao mesmo tempo. A função recebe as URLs iniciais e parâmetros opcionais como argumentos. O parâmetro params permite definir opções adicionais para a tarefa de scraping em lote, como os formatos de saída.

<div id="how-it-works">
  ### Como funciona
</div>

Funciona de forma muito semelhante ao endpoint `/crawl`. Ele cria um job de raspagem em lote e retorna um ID do job para você acompanhar o status da raspagem em lote.

O SDK oferece 2 métodos: síncrono e assíncrono. O método síncrono retorna os resultados do job de raspagem em lote, enquanto o método assíncrono retorna um ID do job que você pode usar para verificar o status da raspagem em lote.

<div id="usage">
  ### Como usar
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Resposta
</div>

Se você estiver usando os métodos síncronos dos SDKs, eles retornarão os resultados do job de scraping em lote. Caso contrário, será retornado um ID de job que você pode usar para verificar o status do scraping em lote.

<div id="synchronous">
  #### Sincronamente
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### Assíncrono
</div>

Você pode usar o ID da tarefa para verificar o status do batch scrape chamando o endpoint `/batch/scrape/{id}`. Este endpoint deve ser usado enquanto a tarefa ainda estiver em execução ou logo após sua conclusão, **pois as tarefas de batch scrape expiram após 24 horas**.

<BatchScrapeAsyncOutput />

<div id="stealth-mode">
  ## Modo stealth
</div>

Para sites com proteção avançada contra bots, o Firecrawl oferece um modo de proxy stealth que aumenta as taxas de sucesso ao extrair dados de páginas desafiadoras.

Saiba mais sobre o [modo stealth](/pt-BR/features/stealth-mode).