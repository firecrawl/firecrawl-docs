---
title: "Raspar"
description: "Transforme qualquer URL em dados limpos"
og:title: "Raspar | Firecrawl"
og:description: "Transforme qualquer URL em dados limpos"
---

import InstallationPython from "/snippets/pt-BR/v2/installation/python.mdx";
import InstallationNode from "/snippets/pt-BR/v2/installation/js.mdx";
import ScrapePython from "/snippets/pt-BR/v2/scrape/base/python.mdx";
import ScrapeNode from "/snippets/pt-BR/v2/scrape/base/js.mdx";
import ScrapeCURL from "/snippets/pt-BR/v2/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/pt-BR/v2/scrape/base/output.mdx";
import ExtractCURL from "/snippets/pt-BR/v2/scrape/json/base/curl.mdx";
import ExtractPython from "/snippets/pt-BR/v2/scrape/json/base/python.mdx";
import ExtractNode from "/snippets/pt-BR/v2/scrape/json/base/js.mdx";
import ExtractOutput from "/snippets/pt-BR/v2/scrape/json/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/pt-BR/v2/scrape/json/no-schema/curl.mdx";
import ExtractNoSchemaPython from "/snippets/pt-BR/v2/scrape/json/no-schema/python.mdx";
import ExtractNoSchemaNode from "/snippets/pt-BR/v2/scrape/json/no-schema/js.mdx";
import ExtractNoSchemaOutput from "/snippets/pt-BR/v2/scrape/json/no-schema/output.mdx";
import ScrapeActionsPython from "/snippets/pt-BR/v2/scrape/actions/python.mdx";
import ScrapeActionsNode from "/snippets/pt-BR/v2/scrape/actions/js.mdx";
import ScrapeActionsCURL from "/snippets/pt-BR/v2/scrape/actions/curl.mdx";
import ScrapeActionsOutput from "/snippets/pt-BR/v2/scrape/actions/output.mdx";
import BatchScrapePython from "/snippets/pt-BR/v2/batch-scrape/short/python.mdx";
import BatchScrapeNode from "/snippets/pt-BR/v2/batch-scrape/short/js.mdx";
import BatchScrapeCURL from "/snippets/pt-BR/v2/batch-scrape/short/curl.mdx";
import BatchScrapeOutput from "/snippets/pt-BR/v2/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/pt-BR/v2/batch-scrape/base/async-output.mdx";
import ScrapeLocationPython from "/snippets/pt-BR/v2/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/pt-BR/v2/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/pt-BR/v2/scrape/location/curl.mdx";
import ScrapeBrandingPython from "/snippets/pt-BR/v2/scrape/branding/base/python.mdx";
import ScrapeBrandingNode from "/snippets/pt-BR/v2/scrape/branding/base/js.mdx";
import ScrapeBrandingCURL from "/snippets/pt-BR/v2/scrape/branding/base/curl.mdx";
import ScrapeBrandingOutput from "/snippets/pt-BR/v2/scrape/branding/base/output.mdx";
import ScrapeBrandingCombinedPython from "/snippets/pt-BR/v2/scrape/branding/combined/python.mdx";
import ScrapeBrandingCombinedNode from "/snippets/pt-BR/v2/scrape/branding/combined/js.mdx";
import ScrapeBrandingCombinedCURL from "/snippets/pt-BR/v2/scrape/branding/combined/curl.mdx";

Firecrawl converte páginas da web em Markdown, ideal para aplicações com LLMs.

* Gerencia complexidades: proxies, cache, limites de taxa, conteúdo bloqueado por JS
* Lida com conteúdo dinâmico: sites dinâmicos, sites renderizados em JS, PDFs, imagens
* Gera Markdown limpo, dados estruturados, capturas de tela ou HTML.

Para mais detalhes, consulte a [Referência da API do endpoint Scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="scraping-a-url-with-firecrawl">
  ## Extraindo dados de uma URL com o Firecrawl
</div>

<div id="scrape-endpoint">
  ### endpoint /scrape
</div>

Usado para extrair o conteúdo de uma URL.

<div id="installation">
  ### Instalação
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />
</CodeGroup>

### Uso

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeCURL />
</CodeGroup>

Para mais detalhes sobre os parâmetros, consulte a [Referência da API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="response">
  ### Resposta
</div>

Os SDKs retornarão o objeto de dados diretamente. O cURL retornará o payload exatamente como mostrado abaixo.

<ScrapeResponse />

<div id="scrape-formats">
  ## Formatos de Scrape
</div>

Agora você pode escolher em quais formatos deseja o seu resultado. Você pode especificar vários formatos de saída. Os formatos suportados são:

* Markdown (`markdown`)
* Resumo (`summary`)
* HTML (`html`)
* HTML bruto (`rawHtml`) (sem modificações)
* Captura de tela (`screenshot`, com opções como `fullPage`, `quality`, `viewport`)
* Links (`links`)
* JSON (`json`) — saída estruturada
* Imagens (`images`) — extrair todas as URLs de imagens da página
* Branding (`branding`) — extrair identidade de marca e sistema de design

As chaves de saída corresponderão ao formato que você escolher.

<div id="extract-brand-identity">
  ## Extrair identidade de marca
</div>

<div id="scrape-with-branding-endpoint">
  ### endpoint /scrape (com branding)
</div>

O formato de branding extrai informações completas sobre a identidade de marca de uma página da web, incluindo cores, fontes, tipografia, espaçamento, componentes de UI e mais. Isso é útil para análise de design systems, monitoramento de marca ou para criar ferramentas que precisam compreender a identidade visual de um site.

<CodeGroup>
  <ScrapeBrandingPython />

  <ScrapeBrandingNode />

  <ScrapeBrandingCURL />
</CodeGroup>

### Resposta

O formato de branding retorna um objeto `BrandingProfile` completo com a seguinte estrutura:

<ScrapeBrandingOutput />

<div id="branding-profile-structure">
  ### Estrutura do Perfil de Branding
</div>

O objeto `branding` contém as seguintes propriedades:

* `colorScheme`: Esquema de cores detectado (`"light"` ou `"dark"`)
* `logo`: URL do logotipo principal
* `colors`: Objeto com as cores da marca:
  * `primary`, `secondary`, `accent`: Cores principais da marca
  * `background`, `textPrimary`, `textSecondary`: Cores de UI
  * `link`, `success`, `warning`, `error`: Cores semânticas
* `fonts`: Lista (array) de famílias tipográficas usadas na página
* `typography`: Informações detalhadas de tipografia:
  * `fontFamilies`: Famílias tipográficas primária, de títulos e de código
  * `fontSizes`: Definições de tamanho para títulos e corpo do texto
  * `fontWeights`: Definições de espessura (leve, regular, média, negrito)
  * `lineHeights`: Valores de altura de linha para diferentes tipos de texto
* `spacing`: Informações de espaçamento e layout:
  * `baseUnit`: Unidade base de espaçamento em pixels
  * `borderRadius`: Raio de borda padrão
  * `padding`, `margins`: Valores de espaçamento
* `components`: Estilos de componentes de UI:
  * `buttonPrimary`, `buttonSecondary`: Estilos de botões
  * `input`: Estilos de campos de entrada
* `icons`: Informações de estilo de ícones
* `images`: Imagens da marca (logo, favicon, og:image)
* `animations`: Configurações de animação e transição
* `layout`: Configuração de layout (grid, alturas de cabeçalho/rodapé)
* `personality`: Traços de personalidade da marca (tom, energia, público-alvo)

<div id="combining-with-other-formats">
  ### Combinando com outros formatos
</div>

Você pode combinar o formato de branding com outros formatos para obter dados completos da página:

<CodeGroup>
  <ScrapeBrandingCombinedPython />

  <ScrapeBrandingCombinedNode />

  <ScrapeBrandingCombinedCURL />
</CodeGroup>

<div id="extract-structured-data">
  ## Extraia dados estruturados
</div>

<div id="scrape-with-json-endpoint">
  ### endpoint /scrape (com json)
</div>

Usado para extrair dados estruturados de páginas extraídas.

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

Resultado:

<ExtractOutput />

<div id="extracting-without-schema">
  ### Extraindo sem esquema
</div>

Agora é possível extrair sem um esquema, bastando enviar um `prompt` para o endpoint. O LLM escolhe a estrutura dos dados.

<CodeGroup>
  <ExtractNoSchemaPython />

  <ExtractNoSchemaNode />

  <ExtractNoSchemaCURL />
</CodeGroup>

Resultado:

<ExtractNoSchemaOutput />

<div id="json-format-options">
  ### Opções do formato JSON
</div>

Ao usar o formato `json`, passe um objeto dentro de `formats` com os seguintes parâmetros:

* `schema`: JSON Schema para a saída estruturada.
* `prompt`: Prompt opcional para orientar a extração quando houver um schema ou quando você preferir uma orientação leve.

<div id="interacting-with-the-page-with-actions">
  ## Interagindo com a página com ações
</div>

O Firecrawl permite executar várias ações em uma página da web antes de fazer o scraping do conteúdo. Isso é especialmente útil para interagir com conteúdo dinâmico, navegar entre páginas ou acessar conteúdo que exige interação do usuário.

Veja um exemplo de como usar ações para acessar google.com, pesquisar por Firecrawl, clicar no primeiro resultado e fazer uma captura de tela.

É importante, quase sempre, usar a ação `wait` antes/depois de executar outras ações para dar tempo suficiente para a página carregar.

<div id="example">
  ### Exemplo
</div>

<CodeGroup>
  <ScrapeActionsPython />

  <ScrapeActionsNode />

  <ScrapeActionsCURL />
</CodeGroup>

<div id="output">
  ### Saída
</div>

<CodeGroup>
  <ScrapeActionsOutput />
</CodeGroup>

Para mais detalhes sobre os parâmetros de ações, consulte a [referência da API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="location-and-language">
  ## Localização e idioma
</div>

Especifique o país e os idiomas preferidos para obter conteúdo relevante com base no seu local de destino e nas suas preferências de idioma.

<div id="how-it-works">
  ### Como funciona
</div>

Quando você define as configurações de localização, o Firecrawl usará um proxy apropriado, se disponível, e emulará as configurações correspondentes de idioma e fuso horário. Por padrão, a localização é definida como “US” se não for especificada.

### Uso

Para usar as configurações de localização e idioma, inclua o objeto `location` no corpo da sua requisição com as seguintes propriedades:

* `country`: Código de país ISO 3166-1 alpha-2 (por exemplo, &#39;US&#39;, &#39;AU&#39;, &#39;DE&#39;, &#39;JP&#39;). O padrão é &#39;US&#39;.
* `languages`: Uma lista (array) de idiomas e localidades preferidos para a requisição, em ordem de prioridade. O padrão é o idioma da localização especificada.

<CodeGroup>
  <ScrapeLocationPython />

  <ScrapeLocationNode />

  <ScrapeLocationCURL />
</CodeGroup>

Para mais detalhes sobre as localizações compatíveis, consulte a [documentação de proxies](/pt-BR/features/proxies).

<div id="caching-and-maxage">
  ## Cache e maxAge
</div>

Para acelerar as requisições, o Firecrawl retorna resultados do cache por padrão quando há uma cópia recente disponível.

* **Janela de frescor padrão**: `maxAge = 172800000` ms (2 dias). Se a página em cache for mais recente do que isso, ela é retornada instantaneamente; caso contrário, a página é coletada novamente e então armazenada em cache.
* **Desempenho**: Pode acelerar as coletas em até 5x quando os dados não precisam estar ultra recentes.
* **Sempre buscar conteúdo novo**: Defina `maxAge` como `0`.
* **Evitar armazenamento**: Defina `storeInCache` como `false` se você não quiser que o Firecrawl armazene em cache os resultados desta requisição.

Exemplo (forçar conteúdo novo):

<CodeGroup>
  ```python Python
  from firecrawl import Firecrawl
  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  doc = firecrawl.scrape(url='https://example.com', maxAge=0, formats=['markdown'])
  print(doc)
  ```

  ```js Node
  import Firecrawl from '@mendable/firecrawl-js';

  const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

  const doc = await firecrawl.scrape('https://example.com', { maxAge: 0, formats: ['markdown'] });
  console.log(doc);
  ```

  ```bash cURL
  curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
    -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "url": "https://example.com",
      "maxAge": 0,
      "formats": ["markdown"]
    }'
  ```
</CodeGroup>

Exemplo (usar uma janela de cache de 10 minutos):

<CodeGroup>
  ```python Python
  from firecrawl import Firecrawl
  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  doc = firecrawl.scrape(url='https://example.com', maxAge=600000, formats=['markdown', 'html'])
  print(doc)
  ```

  ```js Node

  const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

  const doc = await firecrawl.scrape('https://example.com', { maxAge: 600000, formats: ['markdown', 'html'] });
  console.log(doc);
  ```

  ```bash cURL
  curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
    -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "url": "https://example.com",
      "maxAge": 600000,
      "formats": ["markdown", "html"]
    }'
  ```
</CodeGroup>

<div id="batch-scraping-multiple-urls">
  ## Scraping em lote de várias URLs
</div>

Agora é possível fazer scraping em lote de várias URLs ao mesmo tempo. A função recebe as URLs iniciais e parâmetros opcionais como argumentos. O parâmetro params permite definir opções adicionais para a tarefa de scraping em lote, como os formatos de saída.

<div id="how-it-works">
  ### Como funciona
</div>

Funciona de forma muito semelhante ao endpoint `/crawl`. Ele cria um job de raspagem em lote e retorna um ID do job para você acompanhar o status da raspagem em lote.

O SDK oferece 2 métodos: síncrono e assíncrono. O método síncrono retorna os resultados do job de raspagem em lote, enquanto o método assíncrono retorna um ID do job que você pode usar para verificar o status da raspagem em lote.

<div id="usage">
  ### Como usar
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Resposta
</div>

Se você estiver usando os métodos síncronos dos SDKs, eles retornarão os resultados do job de scraping em lote. Caso contrário, será retornado um ID de job que você pode usar para verificar o status do scraping em lote.

<div id="synchronous">
  #### Sincronamente
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### Assíncrono
</div>

Você pode usar o ID da tarefa para verificar o status do batch scrape chamando o endpoint `/batch/scrape/{id}`. Este endpoint deve ser usado enquanto a tarefa ainda estiver em execução ou logo após sua conclusão, **pois as tarefas de batch scrape expiram após 24 horas**.

<BatchScrapeAsyncOutput />

<div id="stealth-mode">
  ## Modo furtivo
</div>

Para sites com proteção avançada contra bots, o Firecrawl oferece um modo de proxy furtivo que aumenta as taxas de sucesso na extração de dados de sites desafiadores.

Saiba mais sobre o [Modo Furtivo](/pt-BR/features/stealth-mode).