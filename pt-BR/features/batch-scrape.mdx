---
title: "Raspagem em lote"
description: "Raspe várias URLs em lote"
og:title: "Raspagem em lote | Firecrawl"
og:description: "Raspe várias URLs em lote"
---

import BatchScrapePython from "/snippets/pt-BR/v2/batch-scrape/base/python.mdx";
import BatchScrapeNode from "/snippets/pt-BR/v2/batch-scrape/base/js.mdx";
import BatchScrapeCURL from "/snippets/pt-BR/v2/batch-scrape/base/curl.mdx";
import BatchScrapeOutput from "/snippets/pt-BR/v2/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/pt-BR/v2/batch-scrape/base/async-output.mdx";
import BatchScrapeExtractPython from "/snippets/pt-BR/v2/batch-scrape/json/python.mdx";
import BatchScrapeExtractNode from "/snippets/pt-BR/v2/batch-scrape/json/js.mdx";
import BatchScrapeExtractCURL from "/snippets/pt-BR/v2/batch-scrape/json/curl.mdx";
import BatchScrapeExtractOutput from "/snippets/pt-BR/v2/batch-scrape/json/output.mdx";
import BatchScrapeExtractAsyncOutput from "/snippets/pt-BR/v2/batch-scrape/json/async-output.mdx";
import BatchScrapeWebhookCURL from "/snippets/pt-BR/v1/batch-scrape-webhook/base/curl.mdx";

<div id="batch-scraping-multiple-urls">
  ## Coleta em lote de várias URLs
</div>

Agora você pode coletar em lote várias URLs simultaneamente. A função recebe as URLs iniciais e parâmetros opcionais como argumentos. O argumento params permite especificar opções adicionais para a tarefa de coleta em lote, como os formatos de saída.

<div id="how-it-works">
  ### Como funciona
</div>

Funciona de forma muito semelhante ao endpoint `/crawl`. Você pode iniciar o lote e aguardar a conclusão ou iniciá-lo e lidar com a conclusão por conta própria.

* `batchScrape` (JS) / `batch_scrape` (Python): inicia um job em lote e aguarda a conclusão, retornando os resultados.
* `startBatchScrape` (JS) / `start_batch_scrape` (Python): inicia um job em lote e retorna o ID do job para que você possa fazer polling ou usar webhooks.

<div id="usage">
  ### Uso
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Resposta
</div>

* Chamar `batchScrape`/`batch_scrape` retorna os resultados completos quando o lote for concluído.

<BatchScrapeOutput />

`

* Chamar `startBatchScrape`/`start_batch_scrape` retorna um ID de tarefa que você pode acompanhar por `getBatchScrapeStatus`/`get_batch_scrape_status`, pelo endpoint da API `/batch/scrape/{id}` ou por webhooks. Este endpoint se destina a verificações em andamento ou imediatamente após a conclusão, **pois tarefas em lote expiram após 24 horas**.

<BatchScrapeAsyncOutput />

<div id="batch-scrape-with-structured-extraction">
  ## Coleta em lote com extração estruturada
</div>

Você também pode usar o endpoint de coleta em lote para extrair dados estruturados das páginas. Isso é útil se você quiser obter os mesmos dados estruturados de uma lista de URLs.

<CodeGroup>
  <BatchScrapeExtractPython />

  <BatchScrapeExtractNode />

  <BatchScrapeExtractCURL />
</CodeGroup>

<div id="response">
  ### Resposta
</div>

* `batchScrape`/`batch_scrape` retorna os resultados completos:

<BatchScrapeExtractOutput />

* `startBatchScrape`/`start_batch_scrape` retorna um ID de tarefa:

<BatchScrapeExtractAsyncOutput />

<div id="batch-scrape-with-webhooks">
  ## Extração em lote com webhooks
</div>

Você pode configurar webhooks para receber notificações em tempo real à medida que cada URL do seu lote é raspada. Isso permite processar os resultados imediatamente, em vez de esperar a conclusão de todo o lote.

<BatchScrapeWebhookCURL />

Para documentação completa sobre webhooks, incluindo tipos de eventos, estrutura do payload e exemplos de implementação, consulte a [documentação de webhooks](/pt-BR/features/webhooks).

<div id="quick-reference">
  ### Referência rápida
</div>

**Tipos de eventos:**

* `batch_scrape.started` - Quando a raspagem em lote é iniciada
* `batch_scrape.page` - Para cada URL raspada com sucesso
* `batch_scrape.completed` - Quando todas as URLs forem processadas
* `batch_scrape.failed` - Se a raspagem em lote encontrar um erro

**Payload básico:**

```json
{
  "success": true,
  "type": "batch_scrape.page",
  "id": "batch-job-id",
  "data": [...], // Dados da página para eventos "page"
  "metadata": {}, // Metadados personalizados
  "error": null
}
```

<Note>
  Para ver configurações detalhadas de webhooks, práticas recomendadas de segurança e dicas de solução de problemas, acesse a [documentação de webhooks](/pt-BR/features/webhooks).
</Note>
