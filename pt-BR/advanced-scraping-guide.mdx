---
title: "Guia Avançado de Scraping"
description: "Aprenda a aprimorar seu scraping no Firecrawl com opções avançadas."
og:title: "Guia Avançado de Scraping | Firecrawl"
og:description: "Aprenda a aprimorar seu scraping no Firecrawl com opções avançadas."
---

Este guia apresenta os diferentes endpoints do Firecrawl e como utilizá-los ao máximo, com todos os seus parâmetros.

<div id="basic-scraping-with-firecrawl">
  ## Raspagem básica com o Firecrawl
</div>

Para raspar uma única página e obter conteúdo em Markdown limpo, use o endpoint `/scrape`.

<CodeGroup>

```python Python
# pip install firecrawl-py

from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key="fc-YOUR-API-KEY")

doc = firecrawl.scrape("https://firecrawl.dev")

print(doc.markdown)
```

```JavaScript JavaScript
// npm install @mendable/firecrawl-js

import { Firecrawl } from 'firecrawl-js';

const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR-API-KEY' });

const doc = await firecrawl.scrape('https://firecrawl.dev');

console.log(doc.markdown);
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-YOUR-API-KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

</CodeGroup>

<div id="scraping-pdfs">
  ## Extração de PDFs
</div>

O Firecrawl oferece suporte a PDFs. Use a opção `parsers` (por exemplo, `parsers: ["pdf"]`) quando quiser garantir a extração de PDFs.

<div id="scrape-options">
  ## Opções de scraping
</div>

Ao usar o endpoint /scrape, você pode personalizar a coleta com as opções abaixo.

<div id="formats-formats">
  ### Formatos (`formats`)
</div>

- **Tipo**: `array`
- **Strings**: `["markdown", "links", "html", "rawHtml", "summary", "images"]`
- **Formatos de objeto**:
  - JSON: `{ type: "json", prompt, schema }`
  - Captura de tela: `{ type: "screenshot", fullPage?, quality?, viewport? }`
  - Rastreio de mudanças: `{ type: "changeTracking", modes?, prompt?, schema?, tag? }` (requer `markdown`)
- **Padrão**: `["markdown"]`

<div id="full-page-content-vs-main-content-onlymaincontent">
  ### Conteúdo da página inteira vs conteúdo principal (`onlyMainContent`)
</div>

- **Type**: `boolean`
- **Description**: Por padrão, o scraper retorna apenas o conteúdo principal. Defina como `false` para retornar o conteúdo da página inteira.
- **Default**: `true`

<div id="include-tags-includetags">
  ### Incluir tags (`includeTags`)
</div>

- **Tipo**: `array`
- **Descrição**: Tags/classes/IDs HTML a serem incluídas na extração.

<div id="exclude-tags-excludetags">
  ### Excluir tags (`excludeTags`)
</div>

- **Tipo**: `array`
- **Descrição**: Tags/classes/IDs HTML a serem excluídos da extração.

<div id="wait-for-page-readiness-waitfor">
  ### Aguardar preparo da página (`waitFor`)
</div>

- **Tipo**: `integer`
- **Descrição**: Tempo extra de espera, em milissegundos, antes do scraping (use com moderação). Esse tempo de espera é adicional ao recurso de espera inteligente do Firecrawl.
- **Padrão**: `0`

<div id="freshness-and-cache-maxage">
  ### Atualidade e cache (`maxAge`)
</div>

- **Tipo**: `integer` (milissegundos)
- **Descrição**: Se houver uma versão em cache da página mais recente do que `maxAge`, o Firecrawl a retorna imediatamente; caso contrário, faz uma nova raspagem e atualiza o cache. Defina `0` para sempre buscar conteúdo novo.
- **Padrão**: `172800000` (2 dias)

<div id="request-timeout-timeout">
  ### Tempo de requisição (`timeout`)
</div>

- **Tipo**: `integer`
- **Descrição**: Duração máxima, em milissegundos, antes de abortar.
- **Padrão**: `30000` (30 segundos)

<div id="pdf-parsing-parsers">
  ### Processamento de PDF (`parsers`)
</div>

- **Tipo**: `array`
- **Descrição**: Controla o comportamento de processamento. Para processar PDFs, defina `parsers: ["pdf"]`.
- **Custo**: O processamento de PDFs custa 1 crédito por página. Para ignorar o processamento de PDF e receber o arquivo em base64 (custo fixo de 1 crédito), defina `parsers: []`.
- **Limitar páginas**: Para limitar o processamento de PDF a um número específico de páginas, use `parsers: [{"type": "pdf", "maxPages": 10}]`.

<div id="actions-actions">
  ### Actions (`actions`)
</div>

Ao usar o endpoint /scrape, o Firecrawl permite executar várias ações em uma página da web antes de extrair seu conteúdo. Isso é particularmente útil para interagir com conteúdo dinâmico, navegar entre páginas ou acessar conteúdo que requer interação do usuário.

- **Tipo**: `array`
- **Descrição**: Sequência de etapas do navegador a serem executadas antes do scraping.
- **Ações suportadas**:
    - `wait` - Aguarda o carregamento da página: `{ type: "wait", milliseconds: number }` ou `{ type: "wait", selector: string }`
    - `click` - Clica em um elemento: `{ type: "click", selector: string, all?: boolean }`
    - `write` - Digita texto em um campo: `{ type: "write", text: string }` (o elemento deve ser focado antes com `click`)
    - `press` - Pressiona uma tecla do teclado: `{ type: "press", key: string }`
    - `scroll` - Rola a página: `{ type: "scroll", direction: "up" | "down", selector?: string }`
    - `screenshot` - Captura uma captura de tela: `{ type: "screenshot", fullPage?: boolean, quality?: number, viewport?: { width: number, height: number } }`
    - `scrape` - Faz scrape de um subelemento da página: `{ type: "scrape" }`
    - `executeJavascript` - Executa código JS: `{ type: "executeJavascript", script: string }`
    - `pdf` - Gera um PDF: `{ type: "pdf", format?: string, landscape?: boolean, scale?: number }`

<CodeGroup>

```python Python
from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key='fc-YOUR-API-KEY')

doc = firecrawl.scrape('https://example.com', {
  'actions': [
    { 'type': 'wait', 'milliseconds': 1000 },
    { 'type': 'click', 'selector': '#accept' },
    { 'type': 'scroll', 'direction': 'down' },
    { 'type': 'click', 'selector': '#q' },
    { 'type': 'write', 'text': 'firecrawl' },
    { 'type': 'press', 'key': 'Enter' },
    { 'type': 'wait', 'milliseconds': 2000 }
  ],
  'formats': ['markdown']
})

print(doc.markdown)
```

```js Node
import { Firecrawl } from 'firecrawl-js';

const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR-API-KEY' });

const doc = await firecrawl.scrape('https://example.com', {
  actions: [
    { type: 'wait', milliseconds: 1000 },
    { type: 'click', selector: '#accept' },
    { type: 'scroll', direction: 'down' },
    { type: 'click', selector: '#q' },
    { type: 'write', text: 'firecrawl' },
    { type: 'press', key: 'Enter' },
    { type: 'wait', milliseconds: 2000 }
  ],
  formats: ['markdown']
});

console.log(doc.markdown);
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "url": "https://example.com",
    "actions": [
      { "type": "wait", "milliseconds": 1000 },
      { "type": "click", "selector": "#accept" },
      { "type": "scroll", "direction": "down" },
      { "type": "click", "selector": "#q" },
      { "type": "write", "text": "firecrawl" },
      { "type": "press", "key": "Enter" },
      { "type": "wait", "milliseconds": 2000 }
    ],
    "formats": ["markdown"]
  }'
```

</CodeGroup>

<div id="action-execution-notes">
  ### Notas sobre a execução de ações
</div>

- **Write action**: Você deve primeiro colocar o foco no elemento usando uma ação de `click` antes de usar `write`. O texto é digitado caractere por caractere para simular a entrada pelo teclado.
- **Scroll selector**: Se você quiser rolar um elemento específico em vez da página inteira, forneça o parâmetro `selector` para `scroll`.
- **Wait with selector**: Você pode esperar até que um elemento específico esteja visível usando `wait` com um parâmetro `selector`, ou esperar por uma duração fixa usando `milliseconds`.
- **Actions are sequential**: As ações são executadas em ordem, e o Firecrawl espera que as interações da página sejam concluídas antes de passar para a próxima ação.
- **Actions are not supported for PDFs**: As ações exigem um motor de navegador e não podem ser usadas em URLs que retornam conteúdo PDF (incluindo URLs que redirecionam para PDFs). Se você incluir `actions` na sua requisição e a URL resultar em um PDF, a requisição falhará. Remova `actions` da sua requisição ao fazer scraping de PDFs.

<div id="advanced-action-examples">
  ### Exemplos de Ações Avançadas
</div>

**Capturando a tela:**

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "url": "https://example.com",
    "actions": [
      { "type": "click", "selector": "#load-more" },
      { "type": "wait", "milliseconds": 1000 },
      { "type": "screenshot", "fullPage": true, "quality": 80 }
    ]
  }'
```

**Clicar em vários elementos:**

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "url": "https://example.com",
    "actions": [
      { "type": "click", "selector": ".expand-button", "all": true },
      { "type": "wait", "milliseconds": 500 }
    ],
    "formats": ["markdown"]
  }'
```

**Gerar um PDF:**

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "url": "https://example.com",
    "actions": [
      { "type": "pdf", "format": "A4", "landscape": false }
    ]
  }'
```


<div id="example-usage">
  ### Exemplo de uso
</div>

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer fc-SUA-API-KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "formats": [
        "markdown",
        "links",
        "html",
        "rawHtml",
        { "type": "screenshot", "fullPage": true, "quality": 80 }
      ],
      "includeTags": ["h1", "p", "a", ".main-content"],
      "excludeTags": ["#ad", "#footer"],
      "onlyMainContent": false,
      "waitFor": 1000,
      "timeout": 15000,
      "parsers": ["pdf"]
    }'
```

Neste exemplo, o scraper vai:

* Retornar o conteúdo completo da página em markdown.
* Incluir o markdown, o HTML bruto, o HTML, os links e uma captura de tela na resposta.
* Incluir apenas as tags HTML `<h1>`, `<p>`, `<a>` e elementos com a classe `.main-content`, excluindo quaisquer elementos com os IDs `#ad` e `#footer`.
* Aguardar 1000 milissegundos (1 segundo) antes de iniciar o scraping para permitir o carregamento da página.
* Definir a duração máxima da solicitação de scraping em 15000 milissegundos (15 segundos).
* Analisar PDFs explicitamente via `parsers: ["pdf"]`.

Aqui está a referência da API: [Documentação do endpoint /scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape)

<div id="json-extraction-via-formats">
  ## Extração de JSON via formatos
</div>

Use o objeto de formato JSON em `formats` para extrair dados estruturados em uma única chamada:

```bash
curl -X POST https://api.firecrawl.dev/v2/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "url": "https://firecrawl.dev",
    "formats": [{
      "type": "json",
      "prompt": "Extract the features of the product",
      "schema": {"type": "object", "properties": {"features": {"type": "object"}}, "required": ["features"]}
    }]
  }'
```


<div id="extract-endpoint">
  ## Endpoint de extração
</div>

Use a API dedicada de jobs de extração quando quiser extração assíncrona com verificação de status (polling).

<CodeGroup>

```js Node
import Firecrawl from '@mendable/firecrawl-js';

const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR-API-KEY' });

// Iniciar job de extração
const started = await firecrawl.startExtract({
  urls: ['https://docs.firecrawl.dev'],
  prompt: 'Extract title',
  schema: { type: 'object', properties: { title: { type: 'string' } }, required: ['title'] }
});

// Verificar status
const status = await firecrawl.getExtractStatus(started.id);
console.log(status.status, status.data);
```

```python Python
from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key='fc-YOUR-API-KEY')

started = firecrawl.start_extract(
    urls=["https://docs.firecrawl.dev"],
    prompt="Extract title",
    schema={"type": "object", "properties": {"title": {"type": "string"}}, "required": ["title"]}
)
status = firecrawl.get_extract_status(started.id)
print(status.get("status"), status.get("data"))
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/extract \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "urls": ["https://docs.firecrawl.dev"],
    "prompt": "Extract title",
    "schema": {"type": "object", "properties": {"title": {"type": "string"}}, "required": ["title"]}
  }'
```
</CodeGroup>

<div id="crawling-multiple-pages">
  ## Rastreando várias páginas
</div>

Para rastrear várias páginas, use o endpoint `/v2/crawl`.

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-YOUR-API-KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

Retorna um ID

```json
{ "id": "1234-5678-9101" }
```

<div id="check-crawl-job">
  ### Verificar job de crawl
</div>

Usado para verificar o status de um job de crawl e obter o resultado.

```bash cURL
curl -X GET https://api.firecrawl.dev/v2/crawl/1234-5678-9101 \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY'
```


<div id="paginationnext-url">
  #### Paginação/Próxima URL
</div>

Se o conteúdo tiver mais de 10 MB ou se a tarefa de crawl ainda estiver em execução, a resposta poderá incluir um parâmetro `next`, uma URL para a próxima página de resultados.

<div id="crawl-prompt-and-params-preview">
  ### Prévia do prompt e dos parâmetros de crawl
</div>

Você pode fornecer um `prompt` em linguagem natural para o Firecrawl deduzir as configurações de crawl. Veja a prévia delas primeiro:

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/crawl/params-preview \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "url": "https://docs.firecrawl.dev",
    "prompt": "Extrair documentação e blog"
  }'
```

<div id="crawler-options">
  ### Opções do crawler
</div>

Ao usar o endpoint `/v2/crawl`, você pode configurar o comportamento do crawler com:

<div id="includepaths">
  #### includePaths
</div>

- **Tipo**: `array`
- **Descrição**: Padrões de regex a incluir. Por padrão, eles são aplicados apenas ao **pathname** da URL (não aos parâmetros de consulta). Para corresponder à URL completa, incluindo query strings, use `regexOnFullURL: true`.
- **Exemplo**: `["^/blog/.*$", "^/docs/.*$"]`

<div id="excludepaths">
  #### excludePaths
</div>

- **Tipo**: `array`
- **Descrição**: Padrões de regex a serem excluídos. Por padrão, eles são aplicados apenas ao **pathname** da URL (não aos parâmetros de consulta). Para aplicar à URL completa, incluindo query strings, use `regexOnFullURL: true`.
- **Exemplo**: `["^/admin/.*$", "^/private/.*$"]`

<div id="regexonfullurl">
  #### regexOnFullURL
</div>

- **Type**: `boolean`
- **Description**: Quando `true`, os padrões de `includePaths` e `excludePaths` são aplicados à URL completa (incluindo parâmetros de consulta) em vez de apenas ao caminho (pathname). Útil para filtrar URLs por parâmetros de consulta.
- **Default**: `false`

<div id="maxdiscoverydepth">
  #### maxDiscoveryDepth
</div>

- **Tipo**: `integer`
- **Descrição**: Profundidade máxima de descoberta para encontrar novas URLs.

<div id="limit">
  #### limit
</div>

- **Type**: `integer`
- **Description**: Número máximo de páginas a rastrear.
- **Default**: `10000`

<div id="crawlentiredomain">
  #### crawlEntireDomain
</div>

- **Tipo**: `boolean`
- **Descrição**: Explora por páginas relacionadas (irmãs/pais) para cobrir todo o domínio.
- **Padrão**: `false`

<div id="allowexternallinks">
  #### allowExternalLinks
</div>

- **Tipo**: `boolean`
- **Descrição**: Segue links para domínios externos.
- **Padrão**: `false`

<div id="allowsubdomains">
  #### allowSubdomains
</div>

- **Tipo**: `boolean`
- **Descrição**: Segue subdomínios do domínio principal.
- **Padrão**: `false`

<div id="delay">
  #### delay
</div>

- **Tipo**: `number`
- **Descrição**: Atraso, em segundos, entre as extrações.
- **Padrão**: `undefined`

<div id="scrapeoptions">
  #### scrapeOptions
</div>

- **Tipo**: `object`
- **Descrição**: Opções para o scraper (veja Formatos acima).
- **Exemplo**: `{ "formats": ["markdown", "links", {"type": "screenshot", "fullPage": true}], "includeTags": ["h1", "p", "a", ".main-content"], "excludeTags": ["#ad", "#footer"], "onlyMainContent": false, "waitFor": 1000, "timeout": 15000}`
- **Padrões**: `formats: ["markdown"]`, cache ativado por padrão (maxAge ~ 2 dias)

<div id="example-usage">
  ### Exemplo de uso
</div>

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-SUA-API-KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "includePaths": ["^/blog/.*$", "^/docs/.*$"],
      "excludePaths": ["^/admin/.*$", "^/private/.*$"],
      "maxDiscoveryDepth": 2,
      "limit": 1000
    }'
```

<div id="mapping-website-links">
  ## Mapeamento de links de sites
</div>

O endpoint `/v2/map` identifica URLs relacionadas a um determinado site.

<div id="usage">
  ### Uso
</div>

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/map \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-SUA-API-KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

<div id="map-options">
  ### Opções de mapeamento
</div>

<div id="search">
  #### search
</div>

- **Tipo**: `string`
- **Descrição**: Filtra links que contêm determinado texto.

<div id="limit">
  #### limit
</div>

- **Tipo**: `integer`
- **Descrição**: Número máximo de links a serem retornados.
- **Padrão**: `100`

<div id="sitemap">
  #### sitemap
</div>

- **Tipo**: `"only" | "include" | "skip"`
- **Descrição**: Controla o uso do sitemap durante o mapeamento.
- **Padrão**: `"include"`

<div id="includesubdomains">
  #### includeSubdomains
</div>

- **Tipo**: `boolean`
- **Descrição**: Inclui os subdomínios do site.
- **Padrão**: `true`

Aqui está a referência da API correspondente: [Documentação do endpoint /map](https://docs.firecrawl.dev/api-reference/endpoint/map)

<div id="whitelisting-firecrawl">
  ## Adicionando o Firecrawl à lista de permissões
</div>

<div id="allowing-firecrawl-to-scrape-your-website">
  ### Como permitir que o Firecrawl faça scraping do seu site
</div>

Se você quiser que o Firecrawl faça scraping do seu próprio site e precisar adicionar o crawler à lista de permissões:

- **User Agent**: o Firecrawl se identifica com o user agent `FirecrawlAgent`. Permita essa string de user agent no seu firewall ou nas suas regras de segurança.
- **Endereços IP**: o Firecrawl não usa um conjunto fixo de endereços IP para requisições de scraping de saída.

<div id="allowing-your-application-to-call-the-firecrawl-api">
  ### Permitindo que sua aplicação faça chamadas à API do Firecrawl
</div>

Se o seu firewall bloquear requisições de saída da sua aplicação para serviços externos, adicione `35.245.250.27` à lista de permissões para permitir chamadas à API do Firecrawl.