---
title: "Guia Avançado de Scraping"
description: "Aprenda a aprimorar seu scraping no Firecrawl com opções avançadas."
og:title: "Guia Avançado de Scraping | Firecrawl"
og:description: "Aprenda a aprimorar seu scraping no Firecrawl com opções avançadas."
---

Este guia apresenta os diferentes endpoints do Firecrawl e como utilizá-los ao máximo, com todos os seus parâmetros.

<div id="basic-scraping-with-firecrawl">
  ## Raspagem básica com o Firecrawl
</div>

Para raspar uma única página e obter conteúdo em Markdown limpo, use o endpoint `/scrape`.

<CodeGroup>

```python Python
# pip install firecrawl-py

from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key="fc-YOUR-API-KEY")

doc = firecrawl.scrape("https://firecrawl.dev")

print(doc.markdown)
```

```JavaScript JavaScript
// npm install @mendable/firecrawl-js

import { Firecrawl } from 'firecrawl-js';

const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR-API-KEY' });

const doc = await firecrawl.scrape('https://firecrawl.dev');

console.log(doc.markdown);
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-YOUR-API-KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

</CodeGroup>

<div id="scraping-pdfs">
  ## Extração de PDFs
</div>

O Firecrawl oferece suporte a PDFs. Use a opção `parsers` (por exemplo, `parsers: ["pdf"]`) quando quiser garantir a extração de PDFs.

<div id="scrape-options">
  ## Opções de scraping
</div>

Ao usar o endpoint /scrape, você pode personalizar a coleta com as opções abaixo.

<div id="formats-formats">
  ### Formatos (`formats`)
</div>

- **Tipo**: `array`
- **Strings**: `["markdown", "links", "html", "rawHtml", "summary"]`
- **Formatos de objeto**:
  - JSON: `{ type: "json", prompt, schema }`
  - Captura de tela: `{ type: "screenshot", fullPage?, quality?, viewport? }`
  - Rastreamento de alterações: `{ type: "changeTracking", modes?, prompt?, schema?, tag? }` (requer `markdown`)
- **Padrão**: `["markdown"]`

<div id="full-page-content-vs-main-content-onlymaincontent">
  ### Conteúdo da página inteira vs conteúdo principal (`onlyMainContent`)
</div>

- **Type**: `boolean`
- **Description**: Por padrão, o scraper retorna apenas o conteúdo principal. Defina como `false` para retornar o conteúdo da página inteira.
- **Default**: `true`

<div id="include-tags-includetags">
  ### Incluir tags (`includeTags`)
</div>

- **Tipo**: `array`
- **Descrição**: Tags/classes/IDs HTML a serem incluídas na extração.

<div id="exclude-tags-excludetags">
  ### Excluir tags (`excludeTags`)
</div>

- **Tipo**: `array`
- **Descrição**: Tags/classes/IDs HTML a serem excluídos da extração.

<div id="wait-for-page-readiness-waitfor">
  ### Aguardar a página ficar pronta (`waitFor`)
</div>

- **Tipo**: `integer`
- **Descrição**: Milissegundos para esperar antes de fazer o scraping (use com moderação).
- **Padrão**: `0`

<div id="freshness-and-cache-maxage">
  ### Atualidade e cache (`maxAge`)
</div>

- **Tipo**: `integer` (milissegundos)
- **Descrição**: Se houver uma versão em cache da página mais recente do que `maxAge`, o Firecrawl a retorna imediatamente; caso contrário, faz uma nova raspagem e atualiza o cache. Defina `0` para sempre buscar conteúdo novo.
- **Padrão**: `172800000` (2 dias)

<div id="request-timeout-timeout">
  ### Tempo de requisição (`timeout`)
</div>

- **Tipo**: `integer`
- **Descrição**: Duração máxima, em milissegundos, antes de abortar.
- **Padrão**: `30000` (30 segundos)

<div id="pdf-parsing-parsers">
  ### Processamento de PDF (`parsers`)
</div>

- **Tipo**: `array`
- **Descrição**: Controla o comportamento de processamento. Para processar PDFs, defina `parsers: ["pdf"]`.

<div id="actions-actions">
  ### Ações (`actions`)
</div>

Ao usar o endpoint /scrape, o Firecrawl permite executar várias ações em uma página da web antes de extrair seu conteúdo. Isso é especialmente útil para interagir com conteúdo dinâmico, navegar entre páginas ou acessar conteúdo que exige interação do usuário.

- Tipo: `array`
- Descrição: Sequência de etapas do navegador a serem executadas antes da extração.
- Ações compatíveis:
    - `wait` `{ milliseconds }`
    - `click` `{ selector }`
    - `write` `{ selector, text }`
    - `press` `{ key }`
    - `scroll` `{ direction: "up" | "down" }`
    - `scrape` `{ selector }` (extrair um subelemento)
    - `executeJavascript` `{ script }`
    - `pdf` (aciona a renderização em PDF em alguns fluxos)

<CodeGroup>

```python Python
from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key='fc-YOUR-API-KEY')

doc = firecrawl.scrape('https://example.com', {
  "actions": [
    {"type": "wait", "milliseconds": 1000},
    {"type": "click", "selector": "#accept"},
    {"type": "scroll", "direction": "down"},
    {"type": "write", "selector": "#q", "text": "firecrawl"},
    {"type": "press", "key": "Enter"}
  ],
  "formats": ['markdown']
})

print(doc.markdown)
```

```js Node
import { Firecrawl } from 'firecrawl-js';

const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR-API-KEY' });

const doc = await firecrawl.scrape('https://example.com', {
  actions: [
    { type: 'wait', milliseconds: 1000 },
    { type: 'click', selector: '#accept' },
    { type: 'scroll', direction: 'down' },
    { type: 'write', selector: '#q', text: 'firecrawl' },
    { type: 'press', key: 'Enter' }
  ],
  formats: ['markdown']
});

console.log(doc.markdown);
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "url": "https://example.com",
    "actions": [
      { "type": "wait", "milliseconds": 1000 },
      { "type": "click", "selector": "#accept" },
      { "type": "scroll", "direction": "down" },
      { "type": "write", "selector": "#q", "text": "firecrawl" },
      { "type": "press", "key": "Enter" }
    ],
    "formats": ["markdown"]
  }'
```

</CodeGroup>

<div id="example-usage">
  ### Exemplo de uso
</div>

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer fc-SUA-API-KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "formats": [
        "markdown",
        "links",
        "html",
        "rawHtml",
        { "type": "screenshot", "fullPage": true, "quality": 80 }
      ],
      "includeTags": ["h1", "p", "a", ".main-content"],
      "excludeTags": ["#ad", "#footer"],
      "onlyMainContent": false,
      "waitFor": 1000,
      "timeout": 15000,
      "parsers": ["pdf"]
    }'
```

Neste exemplo, o scraper vai:

* Retornar o conteúdo completo da página em markdown.
* Incluir o markdown, o HTML bruto, o HTML, os links e uma captura de tela na resposta.
* Incluir apenas as tags HTML `<h1>`, `<p>`, `<a>` e elementos com a classe `.main-content`, excluindo quaisquer elementos com os IDs `#ad` e `#footer`.
* Aguardar 1000 milissegundos (1 segundo) antes de iniciar o scraping para permitir o carregamento da página.
* Definir a duração máxima da solicitação de scraping em 15000 milissegundos (15 segundos).
* Analisar PDFs explicitamente via `parsers: ["pdf"]`.

Aqui está a referência da API: [Documentação do endpoint /scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape)


<div id="json-extraction-via-formats">
  ## Extração em JSON via formatos
</div>

Use o objeto de formato JSON em `formats` para extrair dados estruturados de uma só vez:

```bash
curl -X POST https://api.firecrawl.dev/v2/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "url": "https://firecrawl.dev",
    "formats": [{
      "type": "json",
      "prompt": "Extraia as funcionalidades do produto",
      "schema": {"type": "object", "properties": {"features": {"type": "object"}}, "required": ["features"]}
    }]
  }'
```


<div id="extract-endpoint">
  ## Endpoint /extract
</div>

Use a API dedicada de jobs de extração quando precisar de extração assíncrona com verificação de status (polling).

<CodeGroup>

```js Node
import Firecrawl from '@mendable/firecrawl-js';

const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR-API-KEY' });

// Iniciar job de extração
const started = await firecrawl.startExtract({
  urls: ['https://docs.firecrawl.dev'],
  prompt: 'Extrair título',
  schema: { type: 'object', properties: { title: { type: 'string' } }, required: ['title'] }
});

// Consultar status
const status = await firecrawl.getExtractStatus(started.id);
console.log(status.status, status.data);
```

```python Python
from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key='fc-YOUR-API-KEY')

started = firecrawl.start_extract(
    urls=["https://docs.firecrawl.dev"],
    prompt="Extrair título",
    schema={"type": "object", "properties": {"title": {"type": "string"}}, "required": ["title"]}
)
status = firecrawl.get_extract_status(started.id)
print(status.get("status"), status.get("data"))
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/extract \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "urls": ["https://docs.firecrawl.dev"],
    "prompt": "Extrair título",
    "schema": {"type": "object", "properties": {"title": {"type": "string"}}, "required": ["title"]}
  }'
```
</CodeGroup>

<div id="crawling-multiple-pages">
  ## Rastreando várias páginas
</div>

Para rastrear várias páginas, use o endpoint `/v2/crawl`.

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-YOUR-API-KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

Retorna um ID

```json
{ "id": "1234-5678-9101" }
```


<div id="check-crawl-job">
  ### Verificar job de rastreamento
</div>

Usado para conferir o status de um job de rastreamento e obter seu resultado.

```bash cURL
curl -X GET https://api.firecrawl.dev/v2/crawl/1234-5678-9101 \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-SUA-API-KEY'
```


<div id="paginationnext-url">
  #### Paginação/Próxima URL
</div>

Se o conteúdo tiver mais de 10 MB ou se a tarefa de crawl ainda estiver em execução, a resposta pode incluir o parâmetro `next`, uma URL para a próxima página de resultados.

<div id="crawl-prompt-and-params-preview">
  ### Prévia do prompt e dos parâmetros de crawl
</div>

Você pode fornecer um `prompt` em linguagem natural para o Firecrawl deduzir as configurações de crawl. Veja a prévia delas primeiro:

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/crawl/params-preview \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer fc-YOUR-API-KEY' \
  -d '{
    "url": "https://docs.firecrawl.dev",
    "prompt": "Extrair documentação e blog"
  }'
```


<div id="crawler-options">
  ### Opções do crawler
</div>

Ao usar o endpoint `/v2/crawl`, você pode personalizar o comportamento do rastreamento com:

<div id="includepaths">
  #### includePaths
</div>

- **Tipo**: `array`
- **Descrição**: Padrões de regex a incluir.
- **Exemplo**: `["^/blog/.*$", "^/docs/.*$"]`

<div id="excludepaths">
  #### excludePaths
</div>

- **Tipo**: `array`
- **Descrição**: Padrões de regex a serem excluídos.
- **Exemplo**: `["^/admin/.*$", "^/private/.*$"]`

<div id="maxdiscoverydepth">
  #### maxDiscoveryDepth
</div>

- **Tipo**: `integer`
- **Descrição**: Profundidade máxima de descoberta para encontrar novas URLs.

<div id="limit">
  #### limit
</div>

- **Type**: `integer`
- **Description**: Número máximo de páginas a rastrear.
- **Default**: `10000`

<div id="crawlentiredomain">
  #### crawlEntireDomain
</div>

- **Tipo**: `boolean`
- **Descrição**: Explora por páginas relacionadas (irmãs/pais) para cobrir todo o domínio.
- **Padrão**: `false`

<div id="allowexternallinks">
  #### allowExternalLinks
</div>

- **Tipo**: `boolean`
- **Descrição**: Segue links para domínios externos.
- **Padrão**: `false`

<div id="allowsubdomains">
  #### allowSubdomains
</div>

- **Tipo**: `boolean`
- **Descrição**: Segue subdomínios do domínio principal.
- **Padrão**: `false`

<div id="delay">
  #### delay
</div>

- **Tipo**: `number`
- **Descrição**: Atraso, em segundos, entre as extrações.
- **Padrão**: `undefined`

<div id="scrapeoptions">
  #### scrapeOptions
</div>

- **Tipo**: `object`
- **Descrição**: Opções para o scraper (veja Formatos acima).
- **Exemplo**: `{ "formats": ["markdown", "links", {"type": "screenshot", "fullPage": true}], "includeTags": ["h1", "p", "a", ".main-content"], "excludeTags": ["#ad", "#footer"], "onlyMainContent": false, "waitFor": 1000, "timeout": 15000}`
- **Padrões**: `formats: ["markdown"]`, cache ativado por padrão (maxAge ~ 2 dias)

<div id="example-usage">
  ### Exemplo de uso
</div>

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-SUA-API-KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "includePaths": ["^/blog/.*$", "^/docs/.*$"],
      "excludePaths": ["^/admin/.*$", "^/private/.*$"],
      "maxDiscoveryDepth": 2,
      "limit": 1000
    }'
```


<div id="mapping-website-links">
  ## Mapeando links do site
</div>

O endpoint `/v2/map` identifica URLs relacionadas a um site específico.

<div id="usage">
  ### Uso
</div>

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/map \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer fc-SUA-API-KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```


<div id="map-options">
  ### Opções de mapa
</div>

<div id="search">
  #### search
</div>

- **Tipo**: `string`
- **Descrição**: Filtra links que contêm determinado texto.

<div id="limit">
  #### limit
</div>

- **Tipo**: `integer`
- **Descrição**: Número máximo de links a serem retornados.
- **Padrão**: `100`

<div id="sitemap">
  #### sitemap
</div>

- **Tipo**: `"only" | "include" | "skip"`
- **Descrição**: Controla o uso do sitemap durante o mapeamento.
- **Padrão**: `"include"`

<div id="includesubdomains">
  #### includeSubdomains
</div>

- **Tipo**: `boolean`
- **Descrição**: Inclui os subdomínios do site.
- **Padrão**: `true`

Aqui está a referência da API correspondente: [Documentação do endpoint /map](https://docs.firecrawl.dev/api-reference/endpoint/map)

Obrigado pela leitura!
