---
title: Bem-vindo ao V1
description: "O Firecrawl permite transformar sites inteiros em Markdown pronto para LLMs"
og:title: "Bem-vindo ao V1 | Firecrawl"
og:description: "O Firecrawl permite transformar sites inteiros em Markdown pronto para LLMs"
---

import InstallationPython from "/snippets/pt-BR/v1/installation/python.mdx";
import InstallationNode from "/snippets/pt-BR/v1/installation/js.mdx";
import InstallationGo from "/snippets/pt-BR/v1/installation/go.mdx";
import InstallationRust from "/snippets/pt-BR/v1/installation/rust.mdx";
import ScrapePython from "/snippets/pt-BR/v1/scrape/base/python.mdx";
import ScrapeNode from "/snippets/pt-BR/v1/scrape/base/js.mdx";
import ScrapeGo from "/snippets/pt-BR/v1/scrape/base/go.mdx";
import ScrapeRust from "/snippets/pt-BR/v1/scrape/base/rust.mdx";
import ScrapeCURL from "/snippets/pt-BR/v1/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/pt-BR/v1/scrape/base/output.mdx";
import MapPython from "/snippets/pt-BR/v1/map/base/python.mdx";
import MapJavaScript from "/snippets/pt-BR/v1/map/base/js.mdx";
import MapGo from "/snippets/pt-BR/v1/map/base/go.mdx";
import MapRust from "/snippets/pt-BR/v1/map/base/rust.mdx";
import MapCURL from "/snippets/pt-BR/v1/map/base/curl.mdx";
import MapResponse from "/snippets/pt-BR/v1/map/base/output.mdx";
import CrawlWebSocketPythonBase from "/snippets/pt-BR/v1/crawl-websocket/base/python.mdx";
import CrawlWebSocketNodeBase from "/snippets/pt-BR/v1/crawl-websocket/base/js.mdx";
import ExtractCURL from "/snippets/pt-BR/v1/llm-extract/base/curl.mdx";
import ExtractPython from "/snippets/pt-BR/v1/llm-extract/base/python.mdx";
import ExtractNode from "/snippets/pt-BR/v1/llm-extract/base/js.mdx";
import ExtractOutput from "/snippets/pt-BR/v1/llm-extract/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/pt-BR/v1/llm-extract/no-schema/curl.mdx";
import ExtractNoSchemaOutput from "/snippets/pt-BR/v1/llm-extract/no-schema/output.mdx";
import CrawlWebhookCURL from "/snippets/pt-BR/v1/crawl-webhook/base/curl.mdx";

Firecrawl V1 chegou! Com isso, apresentamos uma API mais confiável e amigável para desenvolvedores.

Veja o que há de novo:

* Formatos de saída para `/scrape`. Escolha em quais formatos você quer o resultado.
* Novo [`endpoint /map`](/pt-BR/features/map) para obter a maioria das URLs de uma página da web.
* API mais amigável para desenvolvedores para o status de `/crawl/{id}`.
* Limites de taxa 2x para todos os planos.
* [SDK de Go](/pt-BR/sdks/go) e [SDK de Rust](/pt-BR/sdks/rust)
* Suporte a equipes
* Gerenciamento de chaves de API no painel.
* `onlyMainContent` agora é `true` por padrão.
* Webhooks e suporte a WebSocket para `/crawl`.

<div id="scrape-formats">
  ## Formatos de Scrape
</div>

Agora você pode escolher os formatos do resultado. É possível especificar vários formatos de saída. Os formatos compatíveis são:

* Markdown (markdown)
* HTML (html)
* HTML bruto (rawHtml) (sem modificações)
* Captura de tela (screenshot ou screenshot@fullPage)
* Links (links)
* JSON (json) — saída estruturada

As chaves do resultado corresponderão ao formato escolhido.

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeGo />

  <ScrapeRust />

  <ScrapeCURL />
</CodeGroup>

<div id="response">
  ### Resposta
</div>

Os SDKs retornarão o objeto de dados diretamente. O cURL retornará o payload exatamente como mostrado abaixo.

<ScrapeResponse />

<div id="introducing-map-alpha">
  ## Apresentando o endpoint /map (Alpha)
</div>

A maneira mais fácil de transformar uma única URL em um mapa de todo o site.

<div id="usage">
  ### Uso
</div>

<CodeGroup>
  <MapPython />

  <MapJavaScript />

  <MapGo />

  <MapRust />

  <MapCURL />
</CodeGroup>

<div id="response">
  ### Resposta
</div>

Os SDKs retornarão o objeto de dados diretamente. O cURL retornará o payload exatamente como mostrado abaixo.

<MapResponse />

<div id="websockets">
  ## WebSockets
</div>

Para rastrear um site com WebSockets, use o método `Crawl URL and Watch`.

<CodeGroup>
  <CrawlWebSocketPythonBase />

  <CrawlWebSocketNodeBase />
</CodeGroup>

<div id="json-format">
  ## Formato JSON
</div>

A extração via LLM agora está disponível na v1 no formato `json`. Para extrair dados estruturados de uma página, você pode passar um schema para o endpoint ou simplesmente fornecer um prompt.

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

Saída:

<ExtractOutput />

<div id="extracting-without-schema-new">
  ### Extração sem schema (Novo)
</div>

Agora é possível extrair sem usar um schema, apenas passando um `prompt` para o endpoint. O LLM escolhe a estrutura dos dados.

<CodeGroup>
  <ExtractNoSchemaCURL />
</CodeGroup>

Resultado:

<ExtractNoSchemaOutput />

<div id="new-crawl-webhook">
  ## Novo webhook de crawl
</div>

Agora é possível passar o parâmetro `webhook` para o endpoint `/crawl`. Isso enviará uma solicitação POST para a URL especificada quando o crawl for iniciado, atualizado e concluído.

O webhook agora será acionado para cada página rastreada, e não apenas para o resultado completo ao final.

<CrawlWebhookCURL />

<div id="webhook-events">
  ### Eventos de Webhook
</div>

Agora há 4 tipos de eventos:

* `crawl.started` - Acionado quando o rastreamento é iniciado.
* `crawl.page` - Acionado para cada página rastreada.
* `crawl.completed` - Acionado quando o rastreamento é concluído, informando que terminou.
* `crawl.failed` - Acionado quando o rastreamento falha.

<div id="webhook-response">
  ### Resposta do webhook
</div>

* `success` - Indica se o webhook conseguiu rastrear a página corretamente.
* `type` - O tipo de evento que ocorreu.
* `id` - O ID do rastreamento.
* `data` - Os dados extraídos (array). Só estará preenchido em `crawl.page` e conterá 1 item se a página tiver sido extraída com sucesso. A resposta é a mesma do endpoint `/scrape`.
* `error` - Se o webhook falhar, conterá a mensagem de erro.

<div id="migrating-from-v0">
  ## Migrando do V0
</div>

> ⚠️ **Aviso de descontinuação**: Os endpoints V0 serão descontinuados em 1º de abril de 2025. Migre para os endpoints V1 antes dessa data para garantir a continuidade do serviço.

<div id="scrape-endpoint">
  ## endpoint /scrape
</div>

O endpoint `/scrape` foi atualizado e redesenhado para oferecer mais confiabilidade e facilidade de uso. A estrutura do novo corpo da requisição para `/scrape` é a seguinte:

```json
{
  "url": "<string>",
  "formats": ["markdown", "html", "rawHtml", "links", "screenshot", "json"],
  "includeTags": ["<string>"],
  "excludeTags": ["<string>"],
  "headers": { "<key>": "<value>" },
  "waitFor": 123,
  "timeout": 123
}
```

<div id="formats">
  ### Formatos
</div>

Agora você pode escolher em quais formatos deseja seu resultado. É possível especificar vários formatos de saída. Os formatos compatíveis são:

* Markdown (markdown)
* HTML (html)
* HTML bruto (rawHtml) (sem modificações)
* Captura de tela (screenshot ou screenshot@fullPage)
* Links (links)
* JSON (json)

Por padrão, a saída incluirá apenas o formato markdown.

<div id="details-on-the-new-request-body">
  ### Detalhes sobre o novo corpo da requisição
</div>

A tabela abaixo apresenta as alterações nos parâmetros do corpo da requisição para o endpoint `/scrape` na V1.

| Parâmetro | Mudança | Descrição |
| --------- | ------ | ----------- |
| `onlyIncludeTags` | Movido e renomeado | Movido para o nível raiz e renomeado para `includeTags`. |
| `removeTags` | Movido e renomeado | Movido para o nível raiz e renomeado para `excludeTags`. |
| `onlyMainContent`| Movido | Movido para o nível raiz. `true` por padrão. |
| `waitFor`| Movido | Movido para o nível raiz. |
| `headers`| Movido | Movido para o nível raiz. |
| `parsePDF`| Movido | Movido para o nível raiz. |
| `extractorOptions`| Sem mudança ||
| `timeout`| Sem mudança ||
| `pageOptions` | Removido | O parâmetro `pageOptions` não é mais necessário. As opções de scrape foram movidas para o nível raiz. |
| `replaceAllPathsWithAbsolutePaths` | Removido | `replaceAllPathsWithAbsolutePaths` não é mais necessário. Todos os caminhos agora são absolutos por padrão. |
| `includeHtml`| Removido | adicione &quot;html&quot; a `formats` em vez disso. |
| `includeRawHtml`| Removido | adicione &quot;rawHtml&quot; a `formats` em vez disso. |
| `screenshot`| Removido | adicione &quot;screenshot&quot; a `formats` em vez disso. |
| `fullPageScreenshot`| Removido | adicione &quot;screenshot@fullPage&quot; a `formats` em vez disso. |
| `extractorOptions` | Removido | Use o formato &quot;json&quot; com o objeto `jsonOptions`. |

O novo formato `json` é descrito na seção [llm-extract](/pt-BR/features/extract).

<div id="crawl-endpoint">
  ## endpoint /crawl
</div>

Também atualizamos o endpoint `/crawl` na v1. Confira o corpo da requisição aprimorado abaixo:

```json
{
  "url": "<string>",
  "excludePaths": ["<string>"],
  "includePaths": ["<string>"],
  "maxDepth": 2,
  "ignoreSitemap": true,
  "limit": 10,
  "allowBackwardLinks": true,
  "allowExternalLinks": true,
  "scrapeOptions": {
    // mesmas opções do endpoint /scrape
    "formats": ["markdown", "html", "rawHtml", "screenshot", "links"]
    "headers": { "<key>": "<value>" },
    "includeTags": ["<string>"],
    "excludeTags": ["<string>"],
    "onlyMainContent": true
    "waitFor": 123
  }
}
```

### Detalhes do novo corpo da requisição

A tabela abaixo apresenta as mudanças nos parâmetros do corpo da requisição para o endpoint `/crawl` na V1.

| Parâmetro | Mudança | Descrição |
| --------- | ------ | ----------- |
| `pageOptions` | Renomeado | Renomeado para `scrapeOptions`. |
| `includes` | Movido e renomeado | Movido para o nível raiz. Renomeado para `includePaths`. |
| `excludes` | Movido e renomeado | Movido para o nível raiz. Renomeado para `excludePaths`. |
| `allowBackwardCrawling` | Movido e renomeado | Movido para o nível raiz. Renomeado para `allowBackwardLinks`. |
| `allowExternalLinks` | Movido | Movido para o nível raiz. |
| `maxDepth` | Movido | Movido para o nível raiz. |
| `ignoreSitemap` | Movido | Movido para o nível raiz. |
| `limit` | Movido | Movido para o nível raiz. |
| `crawlerOptions` | Removido | O parâmetro `crawlerOptions` não é mais necessário. As opções de crawl foram movidas para o nível raiz. |
| `timeout` | Removido | Use `timeout` em `scrapeOptions`. |