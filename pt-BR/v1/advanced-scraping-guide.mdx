---
title: "Guia Avançado de Scraping"
description: "Aprenda a aprimorar seu scraping no Firecrawl com opções avançadas."
og:title: "Guia Avançado de Scraping | Firecrawl"
og:description: "Aprenda a aprimorar seu scraping no Firecrawl com opções avançadas."
---

Este guia apresenta os diferentes endpoints do Firecrawl e como utilizá-los plenamente com todos os seus parâmetros.

<div id="basic-scraping-with-firecrawl-scrape">
  ## Raspagem básica com o Firecrawl (/scrape)
</div>

Para extrair o conteúdo em markdown limpo de uma única página, use o endpoint `/scrape`.

<CodeGroup>

```python Python
# pip install firecrawl-py

from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR_API_KEY")

content = app.scrape_url("https://docs.firecrawl.dev")
```

```JavaScript JavaScript
// npm install @mendable/firecrawl-js

import { FirecrawlApp } from 'firecrawl-js';

const app = new FirecrawlApp({ apiKey: 'YOUR_API_KEY' });

const content = await app.scrapeUrl('https://docs.firecrawl.dev');
```

```go Go
// go get github.com/mendableai/firecrawl-go

import (
  "fmt"
  "log"

  "github.com/mendableai/firecrawl-go"
)

func main() {
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
    log.Fatalf("Failed to initialize FirecrawlApp: %v", err)
  }

  content, err := app.ScrapeURL("docs.firecrawl.dev", nil)
  if err != nil {
    log.Fatalf("Failed)
  }
}
```

```rust Rust
// Instale o crate firecrawl_rs com o Cargo

use firecrawl_rs::FirecrawlApp;
#[tokio::main]
async fn main() {
  // Inicialize o FirecrawlApp com a chave de API
  let api_key = "YOUR_API_KEY";
  let api_url = "https://api.firecrawl.dev";
  let app = FirecrawlApp::new(api_key, api_url).expect("Failed to initialize FirecrawlApp");

  let scrape_result = app.scrape_url("https://docs.firecrawl.dev", None).await;
  match scrape_result {
    Ok(data) => println!("Resultado da raspagem:\n{}", data["markdown"]),
    Err(e) => eprintln!("Falha na raspagem: {}", e),
  }
}
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

</CodeGroup>

<div id="scraping-pdfs">
  ## Extraindo PDFs
</div>

**O Firecrawl oferece suporte à extração de PDFs por padrão.** Você pode usar o endpoint `/scrape` para extrair um link de PDF e obter o conteúdo em texto do PDF. É possível desativar isso definindo `parsePDF` como `false`.

<div id="scrape-options">
  ## Opções de Scrape
</div>

Ao usar o endpoint `/scrape`, você pode personalizar o comportamento da captura com diversos parâmetros. Estas são as opções disponíveis:

<div id="setting-the-content-formats-on-response-with-formats">
  ### Definindo os formatos de conteúdo na resposta com `formats`
</div>

- **Tipo**: `array`
- **Enum**: `["markdown", "links", "html", "rawHtml", "screenshot", "json"]`
- **Descrição**: Especifique os formatos a serem incluídos na resposta. As opções incluem:
  - `markdown`: Retorna o conteúdo extraído em formato Markdown.
  - `links`: Inclui todos os links encontrados na página.
  - `html`: Fornece o conteúdo em formato HTML.
  - `rawHtml`: Entrega o HTML bruto, sem qualquer processamento.
  - `screenshot`: Inclui uma captura de tela da página como exibida no navegador.
  - `json`: Extrai informações estruturadas da página usando o LLM.
- **Padrão**: `["markdown"]`

<div id="getting-the-full-page-content-as-markdown-with-onlymaincontent">
  ### Obtendo o conteúdo completo da página em markdown com `onlyMainContent`
</div>

- **Tipo**: `boolean`
- **Descrição**: Por padrão, o scraper retornará apenas o conteúdo principal da página, excluindo cabeçalhos, barras de navegação, rodapés etc. Defina como `false` para retornar o conteúdo completo da página.
- **Padrão**: `true`

<div id="setting-the-tags-to-include-with-includetags">
  ### Definindo as tags a serem incluídas com `includeTags`
</div>

- **Tipo**: `array`
- **Descrição**: Especifique as tags, classes e IDs de HTML a serem incluídas na resposta.
- **Padrão**: indefinido

<div id="setting-the-tags-to-exclude-with-excludetags">
  ### Definindo as tags a serem excluídas com `excludeTags`
</div>

- **Tipo**: `array`
- **Descrição**: Especifique as tags, classes e IDs de HTML a serem excluídas da resposta.
- **Padrão**: indefinido

<div id="waiting-for-the-page-to-load-with-waitfor">
  ### Aguardando o carregamento da página com `waitFor`
</div>

- **Tipo**: `integer`
- **Descrição**: Use apenas como último recurso. Aguarde um número específico de milissegundos para a página carregar antes de obter o conteúdo.
- **Padrão**: `0`

<div id="setting-the-maximum-timeout">
  ### Definindo o `timeout` máximo
</div>

- **Tipo**: `integer`
- **Descrição**: Define a duração máxima, em milissegundos, que o scraper aguardará pela resposta da página antes de abortar a operação.
- **Padrão**: `30000` (30 segundos)

<div id="parsing-pdf-files-with-parsepdf">
  ### Processando arquivos PDF com `parsePDF`
</div>

- **Tipo**: `boolean`
- **Descrição**: Define como os arquivos PDF são processados durante a raspagem. Quando `true`, o conteúdo do PDF é extraído e convertido para o formato markdown, com cobrança baseada no número de páginas (1 crédito por página). Quando `false`, o arquivo PDF é retornado em base64, com uma tarifa fixa de 1 crédito no total.
- **Padrão**: `true`

<div id="example-usage">
  ### Exemplo de uso
</div>

```bash
curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer SUA_CHAVE_DE_API' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "formats": ["markdown", "links", "html", "rawHtml", "screenshot"],
      "includeTags": ["h1", "p", "a", ".main-content"],
      "excludeTags": ["#ad", "#footer"],
      "onlyMainContent": false,
      "waitFor": 1000,
      "timeout": 15000,
      "parsePDF": false
    }'
```

Neste exemplo, o scraper vai:

* Retornar o conteúdo completo da página em markdown.
* Incluir o markdown, raw HTML, HTML, links e screenshot na resposta.
* A resposta incluirá apenas as tags HTML `<h1>`, `<p>`, `<a>` e elementos com a classe `.main-content`, enquanto exclui qualquer elemento com os IDs `#ad` e `#footer`.
* Aguardar 1000 milissegundos (1 segundo) para a página carregar antes de obter o conteúdo.
* Definir a duração máxima da requisição de scraping em 15000 milissegundos (15 segundos).
* Retornar arquivos PDF em base64 em vez de convertê-los para markdown.

Aqui está a referência da API para isso: [Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)


<div id="extractor-options">
  ## Opções do Extrator
</div>

Ao usar o endpoint `/scrape`, você pode definir opções para **extrair informações estruturadas** do conteúdo da página usando o parâmetro `extract`. Aqui estão as opções disponíveis:

<div id="using-the-llm-extraction">
  ### Usando a extração com LLM
</div>

<div id="schema">
  ### schema
</div>

- **Type**: `object`
- **Required**: Falso se um prompt for fornecido
- **Description**: O esquema dos dados a serem extraídos. Define a estrutura dos dados extraídos.

<div id="system-prompt">
  ### prompt do sistema
</div>

- **Tipo**: `string`
- **Obrigatório**: Não
- **Descrição**: Prompt do sistema para o LLM.

<div id="prompt">
  ### prompt
</div>

- **Type**: `string`
- **Required**: Falso se um schema for fornecido
- **Description**: Um prompt para o LLM extrair os dados no formato/estrutura corretos.
- **Example**: `"Extract the features of the product"`

<div id="example-usage">
  ### Exemplo de uso
</div>

```bash
curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://firecrawl.dev",
      "formats": ["markdown", "json"],
      "json": {
        "prompt": "Extraia as funcionalidades do produto"
      }
    }'
```

```json
{
  "success": true,
  "data": {
    "content": "Conteúdo bruto",
    "metadata": {
      "title": "Mendable",
      "description": "A Mendable permite criar aplicativos de chat com IA com facilidade. Faça a ingestão, personalize e implante com uma única linha de código, onde quiser. Oferecido pela SideGuide",
      "robots": "follow, index",
      "ogTitle": "Mendable",
      "ogDescription": "A Mendable permite criar aplicativos de chat com IA com facilidade. Faça a ingestão, personalize e implante com uma única linha de código, onde quiser. Oferecido pela SideGuide",
      "ogUrl": "https://docs.firecrawl.dev/",
      "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",
      "ogLocaleAlternate": [],
      "ogSiteName": "Mendable",
      "sourceURL": "https://docs.firecrawl.dev/",
      "statusCode": 200
    },
    "extract": {
      "product": "Firecrawl",
      "features": {
        "general": {
          "description": "Transforme sites em dados prontos para LLMs.",
          "openSource": true,
          "freeCredits": 500,
          "useCases": [
            "Aplicativos de IA",
            "Ciência de dados",
            "Pesquisa de mercado",
            "Agregação de conteúdo",
          ]
        },
        "crawlingAndScraping": {
          "crawlAllAccessiblePages": true,
          "noSitemapRequired": true,
          "dynamicContentHandling": true,
          "dataCleanliness": {
            "process": "Algoritmos avançados",
            "outputFormat": "Markdown"
          }
        },
        ...
      }
    }
  }
}
```


<div id="actions">
  ## Ações
</div>

Ao usar o endpoint `/scrape`, o Firecrawl permite executar várias ações em uma página da web antes de extrair seu conteúdo. Isso é especialmente útil para interagir com conteúdo dinâmico, navegar por páginas ou acessar conteúdo que exige interação do usuário.

<div id="available-actions">
  ### Ações disponíveis
</div>

<div id="wait">
  #### wait
</div>

- **Tipo**: `object`
- **Descrição**: Espera por uma quantidade especificada de milissegundos.
- **Propriedades**:
  - `type`: `"wait"`
  - `milliseconds`: Número de milissegundos a esperar.
- **Exemplo**:
  ```json
  {
    "type": "wait",
    "milliseconds": 2000
  }
  ```

<div id="screenshot">
  #### screenshot
</div>

- **Tipo**: `object`
- **Descrição**: Captura uma imagem da tela.
- **Propriedades**:
  - `type`: `"screenshot"`
  - `fullPage`: A captura deve ser de página inteira ou do tamanho da área visível (viewport)? (padrão: `false`)
- **Exemplo**:
  ```json
  {
    "type": "screenshot",
    "fullPage": true
  }
  ```

<div id="click">
  #### click
</div>

- **Tipo**: `object`
- **Descrição**: Clica em um elemento.
- **Propriedades**:
  - `type`: `"click"`
  - `selector`: Seletor (CSS) usado para localizar o elemento.
- **Exemplo**:
  ```json
  {
    "type": "click",
    "selector": "#load-more-button"
  }
  ```

<div id="write">
  #### write
</div>

- **Tipo**: `object`
- **Descrição**: Escreve texto em um campo de entrada.
- **Propriedades**:
  - `type`: `"write"`
  - `text`: Texto a ser digitado.
  - `selector`: Seletor para o campo de entrada.
- **Exemplo**:
  ```json
  {
    "type": "write",
    "text": "Hello, world!",
    "selector": "#search-input"
  }
  ```

<div id="press">
  #### press
</div>

- **Tipo**: `object`
- **Descrição**: Pressiona uma tecla na página.
- **Propriedades**:
  - `type`: `"press"`
  - `key`: Tecla a ser pressionada.
- **Exemplo**:
  ```json
  {
    "type": "press",
    "key": "Enter"
  }
  ```

<div id="scroll">
  #### scroll
</div>

- **Tipo**: `object`
- **Descrição**: Rola a página.
- **Propriedades**:
  - `type`: `"scroll"`
  - `direction`: Direção de rolagem (`"up"` ou `"down"`).
  - `amount`: Quantidade de pixels a rolar.
- **Exemplo**:
  ```json
  {
    "type": "scroll",
    "direction": "down",
    "amount": 500
  }
  ```

<div id="scrape">
  #### scrape
</div>

- **Tipo**: `object`
- **Descrição**: Extrai (scrape) o conteúdo da página atual, retornando a URL e o HTML. O conteúdo extraído será retornado no array `actions.scrapes` da resposta.
- **Propriedades**:
  - `type`: `"scrape"`
- **Exemplo**:
  ```json
  {
    "type": "scrape"
  }
  ```

<div id="pdf">
  #### pdf
</div>

- **Tipo**: `object`
- **Descrição**: Gera um PDF da página atual. O PDF será retornado no array `actions.pdfs` da resposta.
- **Propriedades**:
  - `type`: `"pdf"`
  - `format`: O tamanho de página do PDF gerado (padrão: `"Letter"`)
  - `landscape`: Define se o PDF será gerado na orientação paisagem (padrão: `false`)
  - `scale`: O fator de escala do PDF gerado (padrão: `1`)
- **Exemplo**:
  ```json
  {
    "type": "pdf",
    "format": "A4",
    "landscape": true,
    "scale": 0.8
  }
  ```

<div id="executejavascript">
  #### executeJavascript
</div>

- **Tipo**: `object`
- **Descrição**: Executa código JavaScript na página. Os valores retornados estarão no array `actions.javascriptReturns` da resposta.
- **Propriedades**:
  - `type`: `"executeJavascript"`
  - `script`: Código JavaScript a ser executado.
- **Exemplo**:
  ```json
  {
    "type": "executeJavascript",
    "script": "document.querySelector('.button').click();"
  }
  ```

Para mais detalhes sobre os parâmetros de ações, consulte a [Referência da API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="crawling-multiple-pages">
  ## Rastreamento de várias páginas
</div>

Para rastrear várias páginas, você pode usar o endpoint `/crawl`. Esse endpoint permite especificar uma URL base que você deseja rastrear, e todas as subpáginas acessíveis serão rastreadas.

```bash
curl -X POST https://api.firecrawl.dev/v1/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer SEU_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

Retorna um ID

```json
{ "id": "1234-5678-9101" }
```


<div id="check-crawl-job">
  ### Verificar tarefa de crawl
</div>

Usado para checar o status de uma tarefa de crawl e obter seu resultado.

```bash
curl -X GET https://api.firecrawl.dev/v1/crawl/1234-5678-9101 \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer SUA_CHAVE_DE_API'
```


<div id="paginationnext-url">
  #### Paginação/Próxima URL
</div>

Se o conteúdo for maior que 10 MB ou se a tarefa de rastreamento ainda estiver em execução, a resposta incluirá um parâmetro `next`. Esse parâmetro é uma URL para a próxima página de resultados. Você pode usá-lo para obter a próxima página de resultados.

<div id="crawler-options">
  ### Opções do crawler
</div>

Ao usar o endpoint `/crawl`, você pode personalizar o comportamento do rastreamento com parâmetros no corpo da requisição. Aqui estão as opções disponíveis:

<div id="includepaths">
  #### `includePaths`
</div>

- **Tipo**: `array`
- **Descrição**: Padrões de regex a incluir no crawl. Apenas URLs que corresponderem a esses padrões serão rastreadas. Por exemplo, `^/blog/.*` corresponderá a qualquer URL que comece com `/blog/`.
- **Exemplo**: `["^/blog/.*$", "^/docs/.*$"]`

<div id="excludepaths">
  #### `excludePaths`
</div>

- **Tipo**: `array`
- **Descrição**: Padrões de regex a serem excluídos do crawl. URLs que corresponderem a esses padrões serão puladas. Por exemplo, `^/admin/.*` excluirá qualquer URL que comece com `/admin/`.
- **Exemplo**: `["^/admin/.*$", "^/private/.*$"]`

<div id="maxdepth">
  #### `maxDepth`
</div>

- **Tipo**: `integer`
- **Descrição**: Profundidade absoluta máxima a ser rastreada a partir da base da URL informada. Por exemplo, se o caminho da URL for `/features/feature-1`, nenhum resultado será retornado a menos que `maxDepth` seja pelo menos 2.
- **Exemplo**: `2`

<div id="limit">
  #### `limit`
</div>

- **Tipo**: `integer`
- **Descrição**: Número máximo de páginas a rastrear.
- **Padrão**: `10000`

<div id="allowbackwardlinks">
  #### `allowBackwardLinks`
</div>

- **Tipo**: `boolean`
- **Descrição**: Permite que o crawler siga links internos para URLs irmãs ou superiores (pais), não apenas caminhos filhos.
  - **false**: Faz crawl apenas de URLs mais profundas (filhas).
    - ex.: /features/feature-1 → /features/feature-1/tips ✅
    - Não seguirá /pricing ou / ❌
  - **true**: Faz crawl de quaisquer links internos, incluindo irmãs e superiores (pais).
    - ex.: /features/feature-1 → /pricing, /, etc. ✅
  - Use true para uma cobertura interna mais ampla além de caminhos aninhados.
- **Padrão**: `false`

<div id="allowexternallinks">
  ### `allowExternalLinks`
</div>

- **Tipo**: `boolean`
- **Descrição**: Esta opção permite que o crawler siga links que apontam para domínios externos. Use com cautela, pois isso pode fazer com que a varredura seja limitada apenas pelos valores de `limit` e `maxDepth`.
- **Padrão**: `false`

<div id="allowsubdomains">
  ### `allowSubdomains`
</div>

- **Tipo**: `boolean`
- **Descrição**: Permite que o rastreador siga links para subdomínios do domínio principal. Por exemplo, ao rastrear `example.com`, isso permite seguir links para `blog.example.com` ou `api.example.com`.
- **Padrão**: `false`

<div id="delay">
  ### `delay`
</div>

- **Type**: `number`
- **Description**: Intervalo, em segundos, entre coletas. Ajuda a respeitar os limites de taxa do site e a evitar sobrecarregar o site de destino. Se não for definido, o crawler pode usar o crawl delay do robots.txt, se disponível.
- **Default**: `undefined`

<div id="scrapeoptions">
  #### scrapeOptions
</div>

Como parte das opções do crawler, você também pode especificar o parâmetro `scrapeOptions`. Esse parâmetro permite personalizar o comportamento de scraping para cada página.

- **Tipo**: `object`
- **Descrição**: Opções para o scraper.
- **Exemplo**: `{"formats": ["markdown", "links", "html", "rawHtml", "screenshot"], "includeTags": ["h1", "p", "a", ".main-content"], "excludeTags": ["#ad", "#footer"], "onlyMainContent": false, "waitFor": 1000, "timeout": 15000}`
- **Padrão**: `{ "formats": ["markdown"] }`
- **Veja também**: [Opções de scraping](#setting-the-content-formats-on-response-with-formats)

<div id="example-usage">
  ### Exemplo de uso
</div>

```bash
curl -X POST https://api.firecrawl.dev/v1/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer SUA_CHAVE_DE_API' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "includePaths": ["^/blog/.*$", "^/docs/.*$"],
      "excludePaths": ["^/admin/.*$", "^/private/.*$"],
      "maxDepth": 2,
      "limit": 1000
    }'
```

Neste exemplo, o crawler vai:

* Rastrear apenas URLs que correspondam aos padrões `^/blog/.*$` e `^/docs/.*$`.
* Ignorar URLs que correspondam aos padrões `^/admin/.*$` e `^/private/.*$`.
* Retornar os dados completos do documento de cada página.
* Rastrear até uma profundidade máxima de 2.
* Rastrear no máximo 1000 páginas.


<div id="mapping-website-links-with-map">
  ## Mapeando links de sites com `/map`
</div>

O endpoint `/map` é especializado em identificar URLs contextualmente relacionadas a um determinado site. Esse recurso é essencial para compreender o ecossistema de links de um site, o que pode contribuir significativamente para a análise estratégica e o planejamento de navegação.

<div id="usage">
  ### Uso
</div>

Para usar o endpoint `/map`, você precisa enviar uma solicitação GET com a URL da página que deseja mapear. Veja um exemplo usando `curl`:

```bash
curl -X POST https://api.firecrawl.dev/v1/map \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer SEU_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

Isso retornará um objeto JSON contendo links contextualmente relacionados ao URL.


<div id="example-response">
  ### Exemplo de resposta
</div>

```json
  {
    "sucesso":true,
    "links":[
      "https://docs.firecrawl.dev",
      "https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete",
      "https://docs.firecrawl.dev/api-reference/endpoint/crawl-get",
      "https://docs.firecrawl.dev/api-reference/endpoint/crawl-post",
      "https://docs.firecrawl.dev/api-reference/endpoint/map",
      "https://docs.firecrawl.dev/api-reference/endpoint/scrape",
      "https://docs.firecrawl.dev/api-reference/introduction",
      "https://docs.firecrawl.dev/articles/search-announcement",
      ...
    ]
  }
```


<div id="map-options">
  ### Opções de mapeamento
</div>

<div id="search">
  #### `search`
</div>

- **Tipo**: `string`
- **Descrição**: Busca links que contenham um texto específico.
- **Exemplo**: `"blog"`

<div id="limit">
  #### `limit`
</div>

- **Tipo**: `integer`
- **Descrição**: Número máximo de links a serem retornados.
- **Padrão**: `100`

<div id="ignoresitemap">
  #### `ignoreSitemap`
</div>

- **Tipo**: `boolean`
- **Descrição**: Ignora o sitemap do site durante a varredura
- **Padrão**: `true`

<div id="includesubdomains">
  #### `includeSubdomains`
</div>

- **Tipo**: `boolean`
- **Descrição**: Inclui subdomínios do site
- **Padrão**: `true`

Aqui está a referência da API sobre isso: [Documentação do endpoint /map](https://docs.firecrawl.dev/api-reference/endpoint/map)

Obrigado por ler!