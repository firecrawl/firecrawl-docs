---
title: "Scrape"
description: "Transforme qualquer URL em dados limpos"
og:title: "Scrape | Firecrawl"
og:description: "Transforme qualquer URL em dados limpos"
---

import InstallationPython from "/snippets/pt-BR/v1/installation/python.mdx";
import InstallationNode from "/snippets/pt-BR/v1/installation/js.mdx";
import InstallationGo from "/snippets/pt-BR/v1/installation/go.mdx";
import InstallationRust from "/snippets/pt-BR/v1/installation/rust.mdx";
import ScrapePython from "/snippets/pt-BR/v1/scrape/base/python.mdx";
import ScrapeNode from "/snippets/pt-BR/v1/scrape/base/js.mdx";
import ScrapeGo from "/snippets/pt-BR/v1/scrape/base/go.mdx";
import ScrapeRust from "/snippets/pt-BR/v1/scrape/base/rust.mdx";
import ScrapeCURL from "/snippets/pt-BR/v1/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/pt-BR/v1/scrape/base/output.mdx";
import ExtractCURL from "/snippets/pt-BR/v1/llm-extract/base/curl.mdx";
import ExtractPython from "/snippets/pt-BR/v1/llm-extract/base/python.mdx";
import ExtractNode from "/snippets/pt-BR/v1/llm-extract/base/js.mdx";
import ExtractOutput from "/snippets/pt-BR/v1/llm-extract/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/pt-BR/v1/llm-extract/no-schema/curl.mdx";
import ExtractNoSchemaOutput from "/snippets/pt-BR/v1/llm-extract/no-schema/output.mdx";
import ScrapeActionsPython from "/snippets/pt-BR/v1/scrape/actions/python.mdx";
import ScrapeActionsNode from "/snippets/pt-BR/v1/scrape/actions/js.mdx";
import ScrapeActionsCURL from "/snippets/pt-BR/v1/scrape/actions/curl.mdx";
import ScrapeActionsOutput from "/snippets/pt-BR/v1/scrape/actions/output.mdx";
import BatchScrapePython from "/snippets/pt-BR/v1/batch-scrape/base/python.mdx";
import BatchScrapeNode from "/snippets/pt-BR/v1/batch-scrape/base/js.mdx";
import BatchScrapeCURL from "/snippets/pt-BR/v1/batch-scrape/base/curl.mdx";
import BatchScrapeOutput from "/snippets/pt-BR/v1/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/pt-BR/v1/batch-scrape/base/async-output.mdx";
import ScrapeLocationPython from "/snippets/pt-BR/v1/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/pt-BR/v1/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/pt-BR/v1/scrape/location/curl.mdx";

Firecrawl converte páginas da web em markdown, ideal para aplicações com LLMs.

* Gerencia as complexidades: proxies, cache, limites de taxa, conteúdo bloqueado por JS
* Lida com conteúdo dinâmico: sites dinâmicos, páginas renderizadas em JS, PDFs, imagens
* Gera markdown limpo, dados estruturados, capturas de tela ou HTML.

Para mais detalhes, consulte a [referência da API do endpoint /scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="scraping-a-url-with-firecrawl">
  ## Fazendo scraping de uma URL com o Firecrawl
</div>

<div id="scrape-endpoint">
  ### endpoint /scrape
</div>

Usado para raspar uma URL e obter seu conteúdo.

<div id="installation">
  ### Instalação
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationGo />

  <InstallationRust />
</CodeGroup>

<div id="usage">
  ### Como usar
</div>

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeGo />

  <ScrapeRust />

  <ScrapeCURL />
</CodeGroup>

Para mais detalhes sobre os parâmetros, consulte a [referência da API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="response">
  ### Resposta
</div>

Os SDKs retornarão o objeto de dados diretamente. O cURL retornará o payload exatamente como mostrado abaixo.

<ScrapeResponse />

<div id="scrape-formats">
  ## Formatos de Scrape
</div>

Agora você pode escolher em quais formatos deseja a saída. É possível especificar vários formatos de saída. Os formatos compatíveis são:

* Markdown (markdown)
* HTML (html)
* HTML bruto (rawHtml) (sem modificações)
* Captura de tela (screenshot ou screenshot@fullPage)
* Links (links)
* JSON (json) - saída estruturada

As chaves da saída corresponderão ao formato escolhido.

<div id="extract-structured-data">
  ## Extraia dados estruturados
</div>

<div id="scrape-with-json-endpoint">
  ### endpoint /scrape (com JSON)
</div>

Usado para extrair dados estruturados de páginas extraídas.

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

Saída:

<ExtractOutput />

<div id="extracting-without-schema-new">
  ### Extração sem schema (Novo)
</div>

Agora você pode extrair sem um schema apenas passando um `prompt` para o endpoint. O LLM escolhe a estrutura dos dados.

<CodeGroup>
  <ExtractNoSchemaCURL />
</CodeGroup>

Resultado:

<ExtractNoSchemaOutput />

<div id="json-options-object">
  ### Objeto de opções JSON
</div>

O objeto `jsonOptions` aceita os seguintes parâmetros:

* `schema`: O esquema usado na extração.
* `systemPrompt`: O prompt do sistema usado na extração.
* `prompt`: O prompt usado na extração sem um esquema.

<div id="interacting-with-the-page-with-actions">
  ## Interagindo com a página usando ações
</div>

O Firecrawl permite executar várias ações em uma página da web antes de extrair seu conteúdo. Isso é especialmente útil para interagir com conteúdo dinâmico, navegar entre páginas ou acessar conteúdo que exige interação do usuário.

A seguir está um exemplo de como usar ações para acessar google.com, pesquisar por Firecrawl, clicar no primeiro resultado e tirar uma captura de tela.

É importante quase sempre usar a ação `wait` antes/depois de executar outras ações para dar tempo suficiente para a página carregar.

<div id="example">
  ### Exemplo
</div>

<CodeGroup>
  <ScrapeActionsPython />

  <ScrapeActionsNode />

  <ScrapeActionsCURL />
</CodeGroup>

<div id="output">
  ### Saída
</div>

<CodeGroup>
  <ScrapeActionsOutput />
</CodeGroup>

Para mais detalhes sobre os parâmetros de ações, consulte a [referência da API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="location-and-language">
  ## Localização e idioma
</div>

Especifique o país e os idiomas preferidos para obter conteúdo relevante com base no seu local de destino e nas suas preferências de idioma.

<div id="how-it-works">
  ### Como funciona
</div>

Ao definir as configurações de localização, o Firecrawl usará um proxy apropriado, se disponível, e emulará o idioma e o fuso horário correspondentes. Por padrão, a localização é definida como “US” se nada for especificado.

<div id="usage">
  ### Uso
</div>

Para usar as configurações de localização e idioma, inclua o objeto `location` no corpo da requisição com as seguintes propriedades:

* `country`: código de país ISO 3166-1 alpha-2 (por exemplo, &#39;US&#39;, &#39;AU&#39;, &#39;DE&#39;, &#39;JP&#39;). Padrão: &#39;US&#39;.
* `languages`: uma lista de idiomas e localidades preferidos para a requisição, em ordem de prioridade. Padrão: o idioma da localização especificada.

<CodeGroup>
  <ScrapeLocationPython />

  <ScrapeLocationNode />

  <ScrapeLocationCURL />
</CodeGroup>

<div id="batch-scraping-multiple-urls">
  ## Coleta em lote de várias URLs
</div>

Agora você pode coletar várias URLs em lote ao mesmo tempo. A função recebe as URLs iniciais e parâmetros opcionais como argumentos. O argumento params permite especificar opções adicionais para a tarefa de coleta em lote, como os formatos de saída.

<div id="how-it-works">
  ### Como funciona
</div>

É muito parecido com o funcionamento do endpoint `/crawl`. Ele envia um trabalho de scraping em lote e retorna um ID de trabalho para verificar o status do processamento em lote.

O SDK oferece 2 métodos: síncrono e assíncrono. O método síncrono retorna os resultados do trabalho de scraping em lote, enquanto o método assíncrono retorna um ID de trabalho que você pode usar para acompanhar o status do processamento em lote.

<div id="usage">
  ### Uso
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Resposta
</div>

Se você estiver usando os métodos síncronos dos SDKs, serão retornados os resultados da raspagem em lote. Caso contrário, será retornado um ID de tarefa que você pode usar para verificar o status da raspagem em lote.

<div id="synchronous">
  #### Sincrônico
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### Assíncrono
</div>

Você pode usar o ID da tarefa para verificar o status do batch scrape chamando o endpoint `/batch/scrape/{id}`. Este endpoint deve ser usado enquanto a tarefa ainda estiver em execução ou logo após a sua conclusão, **pois tarefas de batch scrape expiram após 24 horas**.

<BatchScrapeAsyncOutput />

<div id="stealth-mode">
  ## Modo Stealth
</div>

Para sites com proteção anti-bot avançada, o Firecrawl oferece um modo de proxy stealth que aumenta as taxas de sucesso na extração de dados de sites complexos.

Saiba mais sobre o [Modo Stealth](/pt-BR/features/stealth-mode).

<div id="using-fire-1-with-scrape">
  ## Usando o FIRE-1 com Scrape
</div>

Você pode usar o FIRE-1 (Agente) com o endpoint `/scrape` para aplicar navegação inteligente antes de extrair o conteúdo final.

Ativar o FIRE-1 é simples. Basta incluir um objeto `agent` na sua requisição à API de scrape ou extract:

```json
"agent": {
  "model": "FIRE-1",
  "prompt": "Insira aqui suas instruções detalhadas de navegação."
}
```

*Nota:* O campo `prompt` é obrigatório em solicitações ao endpoint /scrape e instrui o FIRE-1 exatamente como interagir com a página da web.

<div id="example-usage-with-scrape-endpoint">
  ### Exemplo de uso com o endpoint /scrape
</div>

Aqui está um exemplo rápido usando o FIRE-1 (Agente) com o endpoint /scrape para obter as empresas no segmento de consumo do Y Combinator:

```bash
curl -X POST https://api.firecrawl.dev/v1/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer SUA_CHAVE_DE_API' \
  -d '{
    "url": "https://ycombinator.com/companies",
    "formats": ["markdown"],
    "agent": {
      "model": "FIRE-1",
      "prompt": "Obtenha as empresas da W22 no segmento de consumo clicando nos respectivos botões"
    }
  }'
```
