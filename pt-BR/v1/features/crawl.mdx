---
title: 'Crawl'
description: 'O Firecrawl pode percorrer recursivamente os subdomínios de uma URL e coletar o conteúdo'
og:title: 'Crawl | Firecrawl'
og:description: 'O Firecrawl pode percorrer recursivamente os subdomínios de uma URL e coletar o conteúdo'
---

import InstallationPython from '/snippets/pt-BR/v1/installation/python.mdx';
import InstallationNode from '/snippets/pt-BR/v1/installation/js.mdx';
import InstallationGo from '/snippets/pt-BR/v1/installation/go.mdx';
import InstallationRust from '/snippets/pt-BR/v1/installation/rust.mdx';
import CrawlPython from '/snippets/pt-BR/v1/crawl/base/python.mdx';
import CrawlNode from '/snippets/pt-BR/v1/crawl/base/js.mdx';
import CrawlGo from '/snippets/pt-BR/v1/crawl/base/go.mdx';
import CrawlRust from '/snippets/pt-BR/v1/crawl/base/rust.mdx';
import CrawlCURL from '/snippets/pt-BR/v1/crawl/base/curl.mdx';
import AsyncCrawlOutput from '/snippets/pt-BR/v1/crawl-async/base/output.mdx';
import CheckCrawlJobPython from '/snippets/pt-BR/v1/crawl-status/short/python.mdx';
import CheckCrawlJobNode from '/snippets/pt-BR/v1/crawl-status/short/js.mdx';
import CheckCrawlJobGo from '/snippets/pt-BR/v1/crawl-status/short/go.mdx';
import CheckCrawlJobRust from '/snippets/pt-BR/v1/crawl-status/short/rust.mdx';
import CheckCrawlJobCURL from '/snippets/pt-BR/v1/crawl-status/short/curl.mdx';
import CheckCrawlJobOutputScraping from '/snippets/pt-BR/v1/crawl-status/base/output-scraping.mdx';
import CheckCrawlJobOutputCompleted from '/snippets/pt-BR/v1/crawl-status/base/output-completed.mdx';
import CrawlWebSocketPython from '/snippets/pt-BR/v1/crawl-websocket/base/python.mdx';
import CrawlWebSocketNode from '/snippets/pt-BR/v1/crawl-websocket/base/js.mdx';
import CrawlWebhookCURL from '/snippets/pt-BR/v1/crawl-webhook/base/curl.mdx';
import PythonCrawlExample from '/snippets/pt-BR/v1/crawl/sdk-example/python.mdx';
import NodeCrawlExample from '/snippets/pt-BR/v1/crawl/sdk-example/js.mdx';
import PythonCrawlExampleResponse from '/snippets/pt-BR/v1/crawl/sdk-example/python-response.mdx';
import NodeCrawlExampleResponse from '/snippets/pt-BR/v1/crawl/sdk-example/js-response.mdx';
import FastCrawlPython from '/snippets/pt-BR/v1/crawl/fast/python.mdx';
import FastCrawlNode from '/snippets/pt-BR/v1/crawl/fast/js.mdx';
import FastCrawlGo from '/snippets/pt-BR/v1/crawl/fast/go.mdx';
import FastCrawlRust from '/snippets/pt-BR/v1/crawl/fast/rust.mdx';
import FastCrawlCURL from '/snippets/pt-BR/v1/crawl/fast/curl.mdx';

Firecrawl rastreia sites com eficiência para extrair dados abrangentes, contornando bloqueios. O processo:

1. **Análise de URL:** Analisa o sitemap e rastreia o site para identificar links
2. **Navegação:** Segue links de forma recursiva para localizar todas as subpáginas
3. **Scraping:** Extrai o conteúdo de cada página, lidando com JS e limites de requisição
4. **Saída:** Converte os dados em Markdown limpo ou em um formato estruturado

Isso garante uma coleta de dados completa a partir de qualquer URL de partida.

<div id="crawling">
  ## Rastreamento
</div>

<div id="crawl-endpoint">
  ### endpoint /crawl
</div>

Usado para rastrear uma URL e todas as subpáginas acessíveis. Isso envia um job de rastreamento e retorna um ID de job para verificar o status do rastreamento.

<Warning>
  Por padrão, o crawl ignorará sublinks de uma página se eles não forem filhos da
  URL que você fornecer. Assim, website.com/other-parent/blog-1 não será
  retornado se você rastrear website.com/blogs/. Se você quiser
  website.com/other-parent/blog-1, use o parâmetro `crawlEntireDomain`. Para
  rastrear subdomínios como blog.website.com ao rastrear website.com, use o
  parâmetro `allowSubdomains`.
</Warning>

<div id="installation">
  ### Instalação
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationGo />

  <InstallationRust />
</CodeGroup>

<div id="usage">
  ### Uso
</div>

<CodeGroup>
  <CrawlPython />

  <CrawlNode />

  <CrawlGo />

  <CrawlRust />

  <CrawlCURL />
</CodeGroup>

<div id="api-response">
  ### Resposta da API
</div>

Se você estiver usando cURL ou as funções `async crawl` nos SDKs, será retornado um `ID` que você pode usar para verificar o status do crawl.

<Note>
  Se você estiver usando o SDK, confira a seção de resposta do SDK
  [abaixo](#sdk-response).
</Note>

<AsyncCrawlOutput />

<div id="check-crawl-job">
  ### Verificar Job de Rastreamento
</div>

Usado para checar o status de um job de rastreamento e obter seu resultado.

<Note>
  Este endpoint só funciona para rastreamentos que estão em andamento ou que foram
  concluídos recentemente.{' '}
</Note>

<CodeGroup>
  <CheckCrawlJobPython />

  <CheckCrawlJobNode />

  <CheckCrawlJobGo />

  <CheckCrawlJobRust />

  <CheckCrawlJobCURL />
</CodeGroup>

<div id="response-handling">
  #### Manipulação de respostas
</div>

A resposta varia conforme o status do crawl.

Para respostas não concluídas ou para respostas grandes que excedam 10 MB, é fornecido um parâmetro de URL `next`. Você deve fazer uma solicitação para essa URL para obter os próximos 10 MB de dados. Se o parâmetro `next` não estiver presente, isso indica o fim dos dados do crawl.

O parâmetro skip define o número máximo de resultados retornados em cada bloco de resultados.

<Info>
  Os parâmetros skip e next só são relevantes ao acessar a API diretamente.
  Se você estiver usando o SDK, nós cuidamos disso para você e retornaremos todos os
  resultados de uma vez.
</Info>

<CodeGroup>
  <CheckCrawlJobOutputScraping />

  <CheckCrawlJobOutputCompleted />
</CodeGroup>

<div id="sdk-response">
  ### Resposta do SDK
</div>

O SDK oferece duas maneiras de rastrear URLs:

1. **Rastreamento síncrono** (`crawl_url`/`crawlUrl`):
   * Aguarda a conclusão do rastreamento e retorna a resposta completa
   * Trata a paginação automaticamente
   * Recomendado para a maioria dos casos de uso

<CodeGroup>
  <PythonCrawlExample />

  <NodeCrawlExample />
</CodeGroup>

A resposta inclui o status do rastreamento e todos os dados extraídos:

<CodeGroup>
  <PythonCrawlExampleResponse />

  <NodeCrawlExampleResponse />
</CodeGroup>

2. **Rastreamento assíncrono** (`async_crawl_url`/`asyncCrawlUrl`):
   * Retorna imediatamente com um ID de rastreamento
   * Permite verificar o status manualmente
   * Útil para rastreamentos longos ou lógica de polling personalizada

<CodeGroup>
  <AsyncCrawlPython />

  <AsyncCrawlNode />
</CodeGroup>

<div id="faster-crawling">
  ## Rastreamento mais rápido
</div>

Acelere seus rastreamentos em 500% quando não precisar dos dados mais recentes. Adicione `maxAge` às suas `scrapeOptions` para usar dados em cache da página quando disponíveis.

<CodeGroup>
  <FastCrawlPython />

  <FastCrawlNode />

  <FastCrawlGo />

  <FastCrawlRust />

  <FastCrawlCURL />
</CodeGroup>

**Como funciona:**

* Cada página do seu rastreamento verifica se há dados em cache mais novos do que `maxAge`
* Se houver, retorna instantaneamente do cache (500% mais rápido)
* Se não houver, faz a raspagem atualizada da página e armazena o resultado em cache
* Perfeito para rastrear sites de documentação, catálogos de produtos ou outros conteúdos relativamente estáticos

Para mais detalhes sobre o uso de `maxAge`, consulte a documentação de [Raspagem mais rápida](/pt-BR/features/fast-scraping).

<div id="crawl-websocket">
  ## WebSocket de Crawl
</div>

O método do Firecrawl baseado em WebSocket, `Crawl URL and Watch`, permite extrair e monitorar dados em tempo real. Inicie um crawl com uma URL e personalize-o com opções como limite de páginas, domínios permitidos e formatos de saída — ideal para processamento de dados imediato.

<CodeGroup>
  <CrawlWebSocketPython />

  <CrawlWebSocketNode />
</CodeGroup>

<div id="crawl-webhook">
  ## Webhook de Crawler
</div>

Você pode configurar webhooks para receber notificações em tempo real conforme o rastreamento avança. Isso permite processar páginas à medida que são coletadas, em vez de esperar a conclusão de todo o rastreamento.

<CrawlWebhookCURL />

Para uma documentação completa sobre webhooks, incluindo tipos de eventos, estrutura do payload e exemplos de implementação, consulte a [documentação de Webhooks](/pt-BR/webhooks/overview).

<div id="quick-reference">
  ### Referência rápida
</div>

**Tipos de eventos:**

* `crawl.started` - Quando o rastreamento começa
* `crawl.page` - Para cada página rastreada com sucesso
* `crawl.completed` - Quando o rastreamento termina
* `crawl.failed` - Se o rastreamento encontrar um erro

**Payload básico:**

```json
{
  "success": true,
  "type": "crawl.page",
  "id": "crawl-job-id",
  "data": [...], // Dados da página para eventos 'page'
  "metadata": {}, // Seus metadados customizados
  "error": null
}
```

<Note>
  Para obter detalhes sobre configuração de webhooks, práticas recomendadas de segurança e
  solução de problemas, consulte a [documentação de Webhooks](/pt-BR/webhooks/overview).
</Note>
