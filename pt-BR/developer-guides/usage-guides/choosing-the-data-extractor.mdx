---
title: "Escolhendo o Extrator de Dados"
description: "Compare /agent, /extract e /scrape (modo JSON) para escolher a ferramenta ideal para extração de dados estruturados"
og:title: "Escolhendo o Extrator de Dados | Firecrawl"
og:description: "Compare /agent, /extract e /scrape (modo JSON) para escolher a ferramenta ideal para extração de dados estruturados"
sidebarTitle: "Escolhendo o Extrator de Dados"
---

import AgentWithSchemaPython from "/snippets/pt-BR/v2/agent/with-schema/python.mdx";
import AgentWithSchemaJS from "/snippets/pt-BR/v2/agent/with-schema/js.mdx";
import AgentWithSchemaCURL from "/snippets/pt-BR/v2/agent/with-schema/curl.mdx";
import ExtractPython from "/snippets/pt-BR/v2/extract/base/python.mdx";
import ExtractNode from "/snippets/pt-BR/v2/extract/base/js.mdx";
import ExtractCURL from "/snippets/pt-BR/v2/extract/base/curl.mdx";
import ScrapeJsonPython from "/snippets/pt-BR/v2/scrape/json/base/python.mdx";
import ScrapeJsonNode from "/snippets/pt-BR/v2/scrape/json/base/js.mdx";
import ScrapeJsonCURL from "/snippets/pt-BR/v2/scrape/json/base/curl.mdx";

Firecrawl oferece três abordagens para extrair dados estruturados de páginas da web. Cada uma atende a casos de uso diferentes, com níveis variados de automação e controle.

<div id="quick-comparison">
  ## Comparação rápida
</div>

| Feature | `/agent` | `/extract` | `/scrape` (modo JSON) |
|---------|----------|------------|----------------------|
| **Status** | Ativo | Use `/agent` em vez disso | Ativo |
| **URL Required** | Não (opcional) | Sim (curingas suportados) | Sim (URL única) |
| **Scope** | Descoberta em toda a Web | Múltiplas páginas/domínios | Página única |
| **URL Discovery** | Busca autônoma na web | Rastreia a partir das URLs fornecidas | Nenhuma |
| **Processing** | Assíncrono | Assíncrono | Síncrono |
| **Schema Required** | Não (prompt ou schema) | Não (prompt ou schema) | Não (prompt ou schema) |
| **Pricing** | Dinâmico (5 execuções grátis/dia) | Baseado em tokens (1 crédito = 15 tokens) | 1 crédito/página |
| **Best For** | Pesquisa, descoberta, coleta complexa de dados | Extração em múltiplas páginas (quando você já tem as URLs) | Extração de página única já conhecida |

<div id="1-agent-endpoint">
  ## 1. `/agent` Endpoint
</div>

O endpoint `/agent` é o recurso mais avançado do Firecrawl — o sucessor de `/extract`. Ele usa agentes de IA para pesquisar, navegar e coletar dados de forma autônoma em toda a web.

<div id="key-characteristics">
  ### Características principais
</div>

* **URLs opcionais**: Basta descrever o que você precisa via `prompt`; o uso de URLs é totalmente opcional
* **Navegação autônoma**: O agente pesquisa e navega profundamente em sites para encontrar seus dados
* **Pesquisa profunda na web**: Descobre informações de forma autônoma em vários domínios e páginas
* **Processamento paralelo**: Processa múltiplas fontes simultaneamente para resultados mais rápidos
* **Modelos disponíveis**: `spark-1-mini` (padrão, 60% mais barato) e `spark-1-pro` (maior precisão)

<div id="example">
  ### Exemplo
</div>

<CodeGroup>
  <AgentWithSchemaPython />

  <AgentWithSchemaJS />

  <AgentWithSchemaCURL />
</CodeGroup>

<div id="best-use-case-autonomous-research-discovery">
  ### Melhor caso de uso: Pesquisa e descoberta autônomas
</div>

**Cenário**: Você precisa encontrar informações sobre startups de IA que captaram uma rodada Série A, incluindo seus fundadores e os valores investidos.

**Por que `/agent`**: Você não sabe em quais sites essas informações estão. O agente vai pesquisar autonomamente na web, navegar por fontes relevantes (Crunchbase, sites de notícias, páginas das empresas) e compilar os dados estruturados para você.

Para mais detalhes, consulte a [documentação do agente](/pt-BR/features/agent).

***

<div id="2-extract-endpoint">
  ## 2. Endpoint `/extract`
</div>

<Note>
  **Use `/agent` instead**: Recomendamos migrar para [`/agent`](/pt-BR/features/agent) — é mais rápido, mais confiável, não requer URLs e atende a todos os casos de uso de `/extract`, além de outros.
</Note>

O endpoint `/extract` coleta dados estruturados das URLs especificadas ou de domínios inteiros usando extração baseada em LLM.

<div id="key-characteristics">
  ### Características principais
</div>

* **URLs normalmente obrigatórias**: Forneça pelo menos uma URL (suporta curingas como `example.com/*`)
* **Rastreamento de domínio**: Pode rastrear e analisar todas as URLs descobertas em um domínio
* **Aprimoramento via busca na web**: `enableWebSearch` opcional para seguir links fora dos domínios especificados
* **Esquema opcional**: Suporta JSON Schema estrito OU prompts em linguagem natural
* **Processamento assíncrono**: Retorna um ID de tarefa para verificação de status

<div id="the-url-limitation">
  ### A limitação de URLs
</div>

O desafio fundamental com `/extract` é que, em geral, você precisa conhecer as URLs antecipadamente:

1. **Lacuna na descoberta**: para tarefas como &quot;encontrar empresas da YC W24&quot;, você não sabe quais URLs contêm os dados. É preciso uma etapa de busca separada antes de chamar `/extract`.
2. **Busca na web pouco prática**: embora `enableWebSearch` exista, ele é limitado a iniciar a partir de URLs que você fornece — um fluxo de trabalho pouco prático para tarefas de descoberta.
3. **Por que `/agent` foi criado**: `/extract` é bom para extrair de locais conhecidos, mas menos eficaz para descobrir onde os dados estão.

<div id="example">
  ### Exemplo
</div>

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

<div id="best-use-case-targeted-multi-page-extraction">
  ### Melhor caso de uso: extração direcionada em várias páginas
</div>

**Cenário**: você tem a URL da documentação do seu concorrente e quer extrair todos os endpoints de API deles de `docs.competitor.com/*`.

**Por que `/extract` funcionou aqui**: você conhecia o domínio exato. Mas, mesmo assim, `/agent` com URLs fornecidas normalmente produz resultados melhores hoje em dia.

Para mais detalhes, veja a [documentação do Extract](/pt-BR/features/extract).

***

<div id="3-scrape-endpoint-with-json-mode">
  ## 3. Endpoint `/scrape` com modo JSON
</div>

O endpoint `/scrape` com modo JSON é a abordagem mais controlada — ele extrai dados estruturados de uma única URL conhecida usando um LLM para transformar o conteúdo da página no schema que você especificar.

<div id="key-characteristics">
  ### Características principais
</div>

* **Apenas uma URL**: Projetado para extrair dados de uma única página por vez
* **URL exata obrigatória**: É preciso conhecer a URL exata que contém os dados
* **Schema opcional**: Pode usar um JSON schema OU apenas um prompt (o LLM define a estrutura)
* **Síncrono**: Retorna os dados imediatamente (sem necessidade de polling de jobs)
* **Formatos adicionais**: Pode combinar extração em JSON com markdown, HTML e capturas de tela em uma única requisição

<div id="example">
  ### Exemplo
</div>

<CodeGroup>
  <ScrapeJsonPython />

  <ScrapeJsonNode />

  <ScrapeJsonCURL />
</CodeGroup>

<div id="best-use-case-single-page-precision-extraction">
  ### Melhor caso de uso: extração precisa em uma única página
</div>

**Cenário**: Você está criando uma ferramenta de monitoramento de preços e precisa extrair o preço, o status de estoque e os detalhes do produto de uma página de produto específica cuja URL você já tem.

**Por que usar `/scrape` com modo JSON**: Você sabe exatamente qual página contém os dados, precisa de extração precisa em uma única página e quer resultados síncronos sem a sobrecarga de gerenciar tarefas.

Para mais detalhes, veja a [documentação do modo JSON](/pt-BR/features/llm-extract).

***

<div id="decision-guide">
  ## Guia de decisão
</div>

**Você sabe a(s) URL(s) exata(s) que contém seus dados?**

* **NÃO** → Use `/agent` (descoberta autônoma na web)
* **SIM**
  * **Página única?** → Use `/scrape` com modo JSON
  * **Múltiplas páginas?** → Use `/agent` com URLs (ou `/scrape` em lote)

<div id="recommendations-by-scenario">
  ### Recomendações por cenário
</div>

| Cenário | Endpoint recomendado |
|----------|---------------------|
| &quot;Encontrar todas as startups de IA e seus financiamentos&quot; | `/agent` |
| &quot;Extrair dados desta página específica de produto&quot; | `/scrape` (modo JSON) |
| &quot;Obter todas as postagens de blog de competitor.com&quot; | `/agent` com URL |
| &quot;Monitorar preços em várias URLs conhecidas&quot; | `/scrape` com processamento em lote |
| &quot;Pesquisar empresas em um setor específico&quot; | `/agent` |
| &quot;Extrair informações de contato de 50 páginas de empresas conhecidas&quot; | `/scrape` com processamento em lote |

***

<div id="pricing">
  ## Preços
</div>

| Endpoint | Custo | Observações |
|----------|------|-------|
| `/scrape` (modo JSON) | 1 crédito/página | Fixo, previsível |
| `/extract` | Baseado em tokens (1 crédito = 15 tokens) | Variável com base no conteúdo |
| `/agent` | Dinâmico | 5 execuções grátis por dia; varia de acordo com a complexidade |

<div id="example-find-the-founders-of-firecrawl">
  ### Exemplo: &quot;Encontre os fundadores da Firecrawl&quot;
</div>

| Endpoint | Como funciona | Créditos consumidos |
|----------|--------------|--------------|
| `/scrape` | Você localiza a URL manualmente e faz o scrape de 1 página | ~1 crédito |
| `/extract` | Você fornece uma ou mais URLs, e ele extrai dados estruturados | Variável (com base em tokens) |
| `/agent` | Apenas envie o prompt — o agente encontra e extrai | ~15 créditos |

**Trade-off**: `/scrape` é o mais barato, mas exige que você saiba a URL. `/agent` custa mais, mas faz a descoberta automaticamente.

Para detalhes de preços, consulte [Firecrawl Pricing](https://firecrawl.dev/pricing).

***

<div id="migration-extract-agent">
  ## Migração: `/extract` → `/agent`
</div>

Se você estiver usando `/extract` atualmente, a migração é direta:

**Antes (extract):**

```python
result = app.extract(
    urls=["https://example.com/*"],
    prompt="Extract product information",
    schema=schema
)
```

**Depois (agente):**

```python
result = app.agent(
    urls=["https://example.com"],  # Opcional - pode ser omitido completamente
    prompt="Extract product information from example.com",
    schema=schema,
    model="spark-1-mini"  # or "spark-1-pro" for higher accuracy
)
```

A principal vantagem é que, com `/agent`, você pode simplesmente deixar de lado as URLs e apenas descrever o que precisa.

***

<div id="key-takeaways">
  ## Principais pontos
</div>

1. **Sabe a URL exata?** Use `/scrape` com modo JSON — é a opção mais barata (1 crédito/página), mais rápida (síncrona) e mais previsível.

2. **Precisa de pesquisa autônoma?** Use `/agent` — ele cuida da descoberta automaticamente, com 5 execuções gratuitas por dia e depois preço dinâmico baseado na complexidade.

3. **Migre de `/extract`** para `/agent` em novos projetos — `/agent` é o sucessor, com recursos mais avançados.

4. **Equilíbrio entre custo e conveniência**: `/scrape` é mais econômico quando você já sabe as URLs; `/agent` custa mais, mas elimina a descoberta manual de URLs.

***

<div id="further-reading">
  ## Leitura complementar
</div>

* [Documentação do agente](/pt-BR/features/agent)
* [Modelos de agente](/pt-BR/features/models)
* [Documentação do modo JSON](/pt-BR/features/llm-extract)
* [Documentação de extração](/pt-BR/features/extract)
* [Raspagem em lote](/pt-BR/features/batch-scrape)