---
title: 'Go'
description: 'O Firecrawl Go SDK é um wrapper da Firecrawl API para ajudar você a transformar sites em markdown com facilidade.'
icon: 'golang'
og:title: "Go SDK | Firecrawl"
og:description: "O Firecrawl Go SDK é um wrapper da Firecrawl API para ajudar você a transformar sites em markdown com facilidade."
---

> Observação: isto utiliza a [versão v0 da Firecrawl API](/pt-BR/v0/introduction), que está sendo descontinuada. Recomendamos migrar para a [v1](/pt-BR/sdks/go).

<div id="installation">
  ## Instalação
</div>

Para instalar o SDK do Firecrawl para Go, use go get:

```bash
go get github.com/firecrawl/firecrawl-go
```

<div id="usage">
  ## Uso
</div>

1. Obtenha uma chave de API em [firecrawl.dev](https://firecrawl.dev)
2. Defina a chave de API como uma variável de ambiente chamada `FIRECRAWL_API_KEY` ou passe-a como parâmetro para a struct `FirecrawlApp`.

Veja um exemplo de como usar o SDK com tratamento de erros:

```go
import (
  "fmt"
  "log"

  "github.com/firecrawl/firecrawl-go"
)

func main() {
  // Inicialize o FirecrawlApp com sua chave de API
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
    log.Fatalf("Falha ao inicializar o FirecrawlApp: %v", err)
  }

  // Fazer scraping de uma única URL
  scrapedData, err := app.ScrapeURL("docs.firecrawl.dev", nil)
  if err != nil {
    log.Fatalf("Erro ao fazer scraping: %v", err)
  }
  fmt.Println(scrapedData)

  // Fazer crawling de um site
  params := map[string]any{
    "pageOptions": map[string]any{
      "onlyMainContent": true,
    },
  }

  crawlResult, err := app.CrawlURL("docs.firecrawl.dev", params)
  if err != nil {
    log.Fatalf("Erro ao fazer crawling: %v", err)
  }
  fmt.Println(crawlResult)
}
```

<div id="scraping-a-url">
  ### Fazer scraping de uma URL
</div>

Para realizar o scraping de uma única URL com tratamento de erros, use o método `ScrapeURL`. Ele recebe a URL como parâmetro e retorna os dados coletados como um dicionário.

```go
scrapedData, err := app.ScrapeURL("docs.firecrawl.dev", nil)
if err != nil {
  log.Fatalf("Falha ao coletar a URL: %v", err)
}
fmt.Println(scrapedData)
```

<div id="crawling-a-website">
  ### Rastreando um site
</div>

Para rastrear um site, use o método `CrawlUrl`. Ele recebe a URL inicial e parâmetros opcionais. O parâmetro `params` permite definir opções adicionais para a tarefa de rastreamento, como o número máximo de páginas a rastrear, domínios permitidos e o formato de saída.

```go
crawlParams := map[string]any{
  "crawlerOptions": map[string]any{
    "excludes": []string{"blog/*"},
    "includes": []string{}, // deixe em branco para incluir todas as páginas
    "limit": 1000,
  },
  "pageOptions": map[string]any{
    "onlyMainContent": true,
  },
}
crawlResult, err := app.CrawlURL("docs.firecrawl.dev", crawlParams, true, 2, idempotencyKey)
if err != nil {
  log.Fatalf("Falha ao fazer o crawl da URL: %v", err)
}
fmt.Println(crawlResult)
```

<div id="checking-crawl-status">
  ### Verificando o status do crawl
</div>

Para verificar o status de um job de crawl, use o método `CheckCrawlStatus`. Ele recebe o ID do job como parâmetro e retorna o status atual do crawl.

```go
status, err := app.CheckCrawlStatus(jobId)
if err != nil {
  log.Fatalf("Falha ao verificar o status do crawl: %v", err)
}
fmt.Println(status)
```

<div id="canceling-a-crawl-job">
  ### Cancelando um job de crawl
</div>

Para cancelar um job de crawl, use o método `CancelCrawlJob`. Ele recebe o ID do job como parâmetro e retorna o status do cancelamento.

```go
canceled, err := app.CancelCrawlJob(jobId)
if err != nil {
  log.Fatalf("Falha ao cancelar o job de crawl: %v", err)
}
fmt.Println(canceled)
```

<div id="extracting-structured-data-from-a-url">
  ### Extraindo dados estruturados de uma URL
</div>

Com a extração via LLM, você pode extrair facilmente dados estruturados de qualquer URL. Veja como usar:

```go
jsonSchema := map[string]any{
  "type": "object",
  "properties": map[string]any{
    "top": map[string]any{
      "type": "array",
      "items": map[string]any{
        "type": "object",
        "properties": map[string]any{
          "title":       map[string]string{"type": "string"},
          "points":      map[string]string{"type": "number"},
          "by":          map[string]string{"type": "string"},
          "commentsURL": map[string]string{"type": "string"},
        },
        "required": []string{"title", "points", "by", "commentsURL"},
      },
      "minItems":    5,
      "maxItems":    5,
      "description": "Top 5 notícias no Hacker News",
    },
  },
  "required": []string{"top"},
}

llmExtractionParams := map[string]any{
  "extractorOptions": firecrawl.ExtractorOptions{
    ExtractionSchema: jsonSchema,
  },
}

scrapeResult, err := app.ScrapeURL("https://news.ycombinator.com", llmExtractionParams)
if err != nil {
  log.Fatalf("Falha ao executar a extração via LLM: %v", err)
}
fmt.Println(scrapeResult)
```

<div id="search-for-a-query">
  ### Pesquisar por uma consulta
</div>

Para pesquisar na web, obter os resultados mais relevantes, fazer o scraping de cada página e retornar o markdown, use o método `Search`. O método recebe a consulta como parâmetro e retorna os resultados da pesquisa.

```go
query := "O que é o Firecrawl?"
searchResult, err := app.Search(query)
if err != nil {
  log.Fatalf("Falha ao realizar a pesquisa: %v", err)
}
fmt.Println(searchResult)
```

<div id="error-handling">
  ## Tratamento de erros
</div>

O SDK trata os erros retornados pela API do Firecrawl e gera as exceções apropriadas. Se ocorrer um erro durante uma requisição, uma exceção será lançada com uma mensagem de erro descritiva.