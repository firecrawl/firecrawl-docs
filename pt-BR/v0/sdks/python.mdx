---
title: 'Python'
description: 'O Firecrawl Python SDK é um wrapper da Firecrawl API que ajuda você a converter sites em markdown com facilidade.'
icon: 'python'
og:title: "Python SDK | Firecrawl"
og:description: "O Firecrawl Python SDK é um wrapper da Firecrawl API que ajuda você a converter sites em markdown com facilidade."
---

> Observação: isto utiliza a [versão v0 da Firecrawl API](/pt-BR/v0/introduction), que está sendo descontinuada. Recomendamos mudar para a [v1](/pt-BR/sdks/python).

<div id="installation">
  ## Instalação
</div>

Para instalar o SDK do Firecrawl para Python, você pode usar o pip:

```bash
pip install firecrawl-py==0.0.16
```

<div id="usage">
  ## Uso
</div>

1. Obtenha uma chave de API em [firecrawl.dev](https://firecrawl.dev)
2. Defina a chave de API como uma variável de ambiente chamada `FIRECRAWL_API_KEY` ou passe-a como parâmetro para a classe `FirecrawlApp`.

Veja um exemplo de como usar o SDK:

```python
from firecrawl import FirecrawlApp

# Inicialize o FirecrawlApp com sua chave de API
app = FirecrawlApp(api_key='your_api_key')

# Extraia dados de uma única URL
url = 'https://docs.firecrawl.dev'
scraped_data = app.scrape_url(url)

# Rastreie um site
crawl_url = 'https://docs.firecrawl.dev'
params = {
    'pageOptions': {
        'onlyMainContent': True
    }
}
crawl_result = app.crawl_url(crawl_url, params=params)
```

<div id="scraping-a-url">
  ### Extraindo dados de uma URL
</div>

Para extrair dados de uma única URL, use o método `scrape_url`. Ele recebe a URL como parâmetro e retorna os dados extraídos como um dicionário.

```python
url = 'https://exemplo.com'
scraped_data = app.scrape_url(url)
```

<div id="extracting-structured-data-from-a-url">
  ### Extraindo dados estruturados de uma URL
</div>

Com a extração via LLM, você pode extrair facilmente dados estruturados de qualquer URL. Temos suporte a esquemas Pydantic para facilitar seu uso. Veja como utilizar:

```python
class ArticleSchema(BaseModel):
    title: str
    points: int 
    by: str
    commentsURL: str

class TopArticlesSchema(BaseModel):
    top: List[ArticleSchema] = Field(..., max_items=5, description="Top 5 notícias")

data = app.scrape_url('https://news.ycombinator.com', {
    'extractorOptions': {
        'extractionSchema': TopArticlesSchema.model_json_schema(),
        'mode': 'llm-extraction'
    },
    'pageOptions':{
        'onlyMainContent': True
    }
})
print(data["llm_extraction"])
```

<div id="crawling-a-website">
  ### Rastreamento de um site
</div>

Para rastrear um site, use o método `crawl_url`. Ele recebe a URL inicial e parâmetros opcionais como argumentos. O argumento `params` permite especificar opções adicionais para a tarefa de rastreamento, como o número máximo de páginas a rastrear, os domínios permitidos e o formato de saída.

O parâmetro `wait_until_done` determina se o método deve aguardar a conclusão da tarefa de rastreamento antes de retornar o resultado. Se definido como `True`, o método verificará periodicamente o status da tarefa até que seja concluída ou até que o `timeout` especificado (em segundos) seja atingido. Se definido como `False`, o método retornará imediatamente com o ID da tarefa, e você poderá verificar manualmente o status usando o método `check_crawl_status`.

```python
crawl_url = 'https://example.com'
params = {
    'crawlerOptions': {
        'excludes': ['blog/*'],
        'includes': [], # deixe vazio para todas as páginas
        'limit': 1000,
    },
    'pageOptions': {
        'onlyMainContent': True
    }
}
crawl_result = app.crawl_url(crawl_url, params=params, wait_until_done=True, timeout=5)
```

Se `wait_until_done` estiver definido como `True`, o método `crawl_url` retornará o resultado do crawl quando a tarefa for concluída. Se a tarefa falhar ou for interrompida, uma exceção será gerada.

<div id="checking-crawl-status">
  ### Verificando o status do crawl
</div>

Para consultar o status de um job de crawl, use o método `check_crawl_status`. Ele recebe o ID do job como parâmetro e retorna o status atual do crawl.

```python
job_id = crawl_result['jobId']
status = app.check_crawl_status(job_id)
```

<div id="search-for-a-query">
  ### Pesquisar por uma consulta
</div>

Usado para pesquisar na web, obter os resultados mais relevantes, fazer o scraping de cada página e retornar o markdown.

```python
query = 'o que é a Mendable?'
search_result = app.search(query)
```

<div id="error-handling">
  ## Tratamento de erros
</div>

O SDK trata os erros retornados pela API do Firecrawl e lança as exceções apropriadas. Se ocorrer um erro durante uma requisição, uma exceção será lançada com uma mensagem de erro descritiva.