---
title: "Guia Avançado de Scraping"
description: "Aprenda a aprimorar seu scraping no Firecrawl com opções avançadas."
og:title: "Guia Avançado de Scraping | Firecrawl"
og:description: "Aprenda a aprimorar seu scraping no Firecrawl com opções avançadas."
---

Este guia apresenta os diferentes endpoints do Firecrawl e como utilizá-los plenamente, explorando todos os seus parâmetros.

<div id="basic-scraping-with-firecrawl-scrape">
  ## Coleta básica com o Firecrawl (/scrape)
</div>

Para extrair o conteúdo em markdown de uma única página, use o endpoint `/scrape`.

<CodeGroup>

```python Python
# pip install firecrawl-py

from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR_API_KEY")

content = app.scrape_url("https://docs.firecrawl.dev")
```

```JavaScript JavaScript
// npm install @mendable/firecrawl-js

import { FirecrawlApp } from 'firecrawl-js';

const app = new FirecrawlApp({ apiKey: 'YOUR_API_KEY' });

const content = await app.scrapeUrl('https://docs.firecrawl.dev');
```

```go Go
// go get github.com/mendableai/firecrawl-go

import (
  "fmt"
  "log"

  "github.com/mendableai/firecrawl-go"
)

func main() {
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
    log.Fatalf("Failed to initialize FirecrawlApp: %v", err)
  }

  content, err := app.ScrapeURL("docs.firecrawl.dev", nil)
  if err != nil {
    log.Fatalf("Failed")
  }
}
```

```rust Rust
// Instale o crate firecrawl_rs com o Cargo

use firecrawl_rs::FirecrawlApp;
#[tokio::main]
async fn main() {
  // Inicialize o FirecrawlApp com a chave de API
  let api_key = "YOUR_API_KEY";
  let api_url = "https://api.firecrawl.dev";
  let app = FirecrawlApp::new(api_key, api_url).expect("Failed to initialize FirecrawlApp");

  let scrape_result = app.scrape_url("https://docs.firecrawl.dev", None).await;
  match scrape_result {
    Ok(data) => println!("Resultado da extração:\n{}", data["markdown"]),
    Err(e) => eprintln!("Falha na extração: {}", e),
  }
}
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

</CodeGroup>

<div id="scraping-pdfs">
  ## Extraindo PDFs
</div>

**O Firecrawl oferece suporte à extração de PDFs por padrão.** Você pode usar o endpoint `/scrape` para extrair um link de PDF e obter o conteúdo em texto do PDF. É possível desativar isso definindo `pageOptions.parsePDF` como `false`.

<div id="page-options">
  ## Opções de página
</div>

Ao usar o endpoint `/scrape`, você pode personalizar o comportamento de scraping com o parâmetro `pageOptions`. Veja as opções disponíveis:

<div id="getting-cleaner-content-with-onlymaincontent">
  ### Obtendo conteúdo mais limpo com `onlyMainContent`
</div>

- **Tipo**: `boolean`
- **Descrição**: Retorna somente o conteúdo principal da página, excluindo cabeçalhos, barras de navegação, rodapés etc.
- **Padrão**: `false`

<div id="getting-the-html-with-includehtml">
  ### Obtendo o HTML com `includeHtml`
</div>

- **Tipo**: `boolean`
- **Descrição**: Inclui o conteúdo da página em HTML. Isso adicionará a chave `html` na resposta.
- **Padrão**: `false`

<div id="getting-the-raw-html-with-includerawhtml">
  ### Obtendo o HTML bruto com `includeRawHtml`
</div>

- **Tipo**: `boolean`
- **Descrição**: Inclui o HTML bruto da página. Isso adicionará a chave `rawHtml` na resposta.
- **Padrão**: `false`

<div id="getting-a-screenshot-of-the-page-with-screenshot">
  ### Obtendo uma captura de tela da página com `screenshot`
</div>

- **Tipo**: `boolean`
- **Descrição**: Inclui uma captura de tela da parte superior da página que você está extraindo.
- **Padrão**: `false`

<div id="waiting-for-the-page-to-load-with-waitfor">
  ### Aguardando o carregamento da página com `waitFor`
</div>

- **Tipo**: `integer`
- **Descrição**: Use apenas como último recurso. Aguarde um número específico de milissegundos para a página carregar antes de obter o conteúdo.
- **Padrão**: `0`

<div id="example-usage">
  ### Exemplo de uso
</div>

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer SUA_CHAVE_DE_API' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "pageOptions": {
        "onlyMainContent": true,
        "includeHtml": true,
        "includeRawHtml":true,
        "screenshot": true,
        "waitFor": 5000
      }
    }'
```

Neste exemplo, o scraper vai:

* Retornar apenas o conteúdo principal da página.
* Incluir o HTML bruto na resposta, na chave `html`.
* Aguardar 5000 milissegundos (5 segundos) para a página carregar antes de obter o conteúdo.

Aqui está a referência da API correspondente: [Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)


<div id="extractor-options">
  ## Opções do Extrator
</div>

Ao usar o endpoint `/scrape`, você pode definir opções para **extrair informações estruturadas** do conteúdo da página usando o parâmetro `extractorOptions`. Estas são as opções disponíveis:

<div id="mode">
  ### mode
</div>

- **Type**: `string`
- **Enum**: `["llm-extraction", "llm-extraction-from-raw-html"]`
- **Description**: O modo de extração a ser utilizado.

  - `llm-extraction`: Extrai informações do conteúdo limpo e processado.
  - `llm-extraction-from-raw-html`: Extrai informações diretamente do HTML cru.

- **Type**: `string`
- **Description**: Um prompt que descreve quais informações devem ser extraídas da página.

<div id="extractionschema">
  ### extractionSchema
</div>

- **Tipo**: `object`
- **Descrição**: O esquema dos dados a serem extraídos. Define a estrutura dos dados extraídos.

<div id="example-usage">
  ### Exemplo de uso
</div>

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer SUA_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev/",
      "extractorOptions": {
        "mode": "llm-extraction",
        "extractionPrompt": "Com base nas informações da página, extraia os dados conforme o schema. ",
        "extractionSchema": {
          "type": "object",
          "properties": {
            "company_mission": {
                      "type": "string"
            },
            "supports_sso": {
                      "type": "boolean"
            },
            "is_open_source": {
                      "type": "boolean"
            },
            "is_in_yc": {
                      "type": "boolean"
            }
          },
          "required": [
            "company_mission",
            "supports_sso",
            "is_open_source",
            "is_in_yc"
          ]
        }
      }
    }'
```

```json
{
  "success": true,
  "data": {
    "content": "Conteúdo bruto",
    "metadata": {
      "title": "Mendable",
      "description": "A Mendable permite criar facilmente aplicativos de chat com IA. Ingira, personalize e faça o deploy com uma única linha de código em qualquer lugar que desejar. Oferecido por SideGuide",
      "robots": "seguir, indexar",
      "ogTitle": "Mendable",
      "ogDescription": "A Mendable permite criar facilmente aplicativos de chat com IA. Ingira, personalize e faça o deploy com uma única linha de código em qualquer lugar que desejar. Oferecido por SideGuide",
      "ogUrl": "https://docs.firecrawl.dev/",
      "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",
      "ogLocaleAlternate": [],
      "ogSiteName": "Mendable",
      "sourceURL": "https://docs.firecrawl.dev/"
    },
    "llm_extraction": {
      "company_mission": "Treinar uma IA segura com seus recursos técnicos para responder a perguntas de clientes e funcionários, para que sua equipe não precise fazê-lo",
      "supports_sso": true,
      "is_open_source": false,
      "is_in_yc": true
    }
  }
}
```


<div id="adjusting-timeout">
  ## Ajustando o tempo limite
</div>

Você pode ajustar o tempo limite do processo de scraping usando o parâmetro `timeout`, em milissegundos.

<div id="example-usage">
  ### Exemplo de uso
</div>

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer SUA_CHAVE_DE_API' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "timeout": 50000
    }'
```


<div id="crawling-multiple-pages">
  ## Rastreamento de múltiplas páginas
</div>

Para rastrear várias páginas, você pode usar o endpoint `/crawl`. Esse endpoint permite especificar uma URL base que você deseja rastrear, e todas as subpáginas acessíveis serão rastreadas.

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer SUA_CHAVE_DE_API' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

Retorna um jobId

```json
{ "jobId": "1234-5678-9101" }
```


<div id="check-crawl-job">
  ### Verificar Job de Crawler
</div>

Usado para verificar o status de um job de crawler e obter seu resultado.

```bash
curl -X GET https://api.firecrawl.dev/v0/crawl/status/1234-5678-9101 \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer SUA_CHAVE_DE_API'
```


<div id="crawler-options">
  ### Opções do Crawler
</div>

Ao usar o endpoint `/crawl`, você pode personalizar o comportamento do rastreamento com o parâmetro `crawlerOptions`. Aqui estão as opções disponíveis:

<div id="includes">
  #### `includes`
</div>

- **Tipo**: `array`
- **Descrição**: Padrões de URL a serem incluídos na varredura. Somente URLs que corresponderem a esses padrões serão rastreadas.
- **Exemplo**: `["/blog/*", "/products/*"]`

<div id="excludes">
  #### `excludes`
</div>

- **Tipo**: `array`
- **Descrição**: Padrões de URL a serem excluídos da varredura. URLs que corresponderem a esses padrões serão puladas.
- **Exemplo**: `["/admin/*", "/login/*"]`

<div id="returnonlyurls">
  #### `returnOnlyUrls`
</div>

- **Tipo**: `boolean`
- **Descrição**: Se definido como `true`, a resposta incluirá apenas uma lista de URLs, em vez dos dados completos do documento.
- **Padrão**: `false`

<div id="maxdepth">
  #### `maxDepth`
</div>

- **Tipo**: `integer`
- **Descrição**: Profundidade máxima de rastreamento em relação à URL informada. Um maxDepth de 0 extrai apenas a URL informada. Um maxDepth de 1 extrai a URL informada e todas as páginas a um nível de profundidade. Um maxDepth de 2 extrai a URL informada e todas as páginas até dois níveis de profundidade. Valores maiores seguem o mesmo padrão.
- **Exemplo**: `2`

<div id="mode">
  #### `mode`
</div>

- **Tipo**: `string`
- **Enum**: `["default", "fast"]`
- **Descrição**: O modo de rastreamento a ser usado. O modo `fast` rastreia sites sem sitemap 4x mais rápido, mas pode ser menos preciso e não é recomendado para sites com forte uso de JavaScript para renderização.
- **Padrão**: `default`

<div id="limit">
  #### `limit`
</div>

- **Tipo**: `integer`
- **Descrição**: Número máximo de páginas a serem rastreadas.
- **Padrão**: `10000`

<div id="example-usage">
  ### Exemplo de uso
</div>

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer SUA_CHAVE_DE_API' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "crawlerOptions": {
        "includes": ["/blog/*", "/products/*"],
        "excludes": ["/admin/*", "/login/*"],
        "returnOnlyUrls": false,
        "maxDepth": 2,
        "mode": "fast",
        "limit": 1000
      }
    }'
```

Neste exemplo, o crawler vai:

* Rastrear apenas URLs que correspondam aos padrões `/blog/*` e `/products/*`.
* Ignorar URLs que correspondam aos padrões `/admin/*` e `/login/*`.
* Retornar os dados completos do documento de cada página.
* Rastrear até a profundidade máxima de 2.
* Usar o modo de rastreamento rápido.
* Rastrear no máximo 1000 páginas.


<div id="page-options-crawler-options">
  ## Opções de Página + Opções do Crawler
</div>

Você pode combinar os parâmetros `pageOptions` e `crawlerOptions` para personalizar todo o comportamento de crawling.

<div id="example-usage">
  ### Exemplo de uso
</div>

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer SUA_CHAVE_DE_API' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "pageOptions": {
        "onlyMainContent": true,
        "includeHtml": true,
        "includeRawHtml": true,
        "screenshot": true,
        "waitFor": 5000
      },
      "crawlerOptions": {
        "includes": ["/blog/*", "/products/*"],
        "maxDepth": 2,
        "mode": "fast",
      }
    }'
```

Neste exemplo, o crawler vai:

* Retornar apenas o conteúdo principal de cada página.
* Incluir o conteúdo HTML bruto de cada página.
* Aguardar 5000 milissegundos para cada página carregar antes de obter seu conteúdo.
* Rastrear apenas URLs que correspondam aos padrões `/blog/*` e `/products/*`.
* Rastrear até uma profundidade máxima de 2.
* Usar o modo de rastreamento rápido.


<div id="extractor-options-crawler-options">
  ## Opções do Extrator + Opções do Crawler
</div>

Em breve...