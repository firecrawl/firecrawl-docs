---
title: "LlamaIndex"
description: "Utiliser Firecrawl avec LlamaIndex pour des applications RAG"
---

Intégrez Firecrawl à LlamaIndex pour créer des applications d’IA avec recherche vectorielle et embeddings, alimentées par du contenu web.

<div id="setup">
  ## Configuration
</div>

```bash
npm install llamaindex @llamaindex/openai @mendable/firecrawl-js
```

Créez le fichier `.env` :

```bash
FIRECRAWL_API_KEY=votre_clé_firecrawl
OPENAI_API_KEY=votre_clé_openai
```

> **Remarque :** Si vous utilisez Node &lt; 20, installez `dotenv` et ajoutez `import 'dotenv/config'` à votre code.

<div id="rag-with-vector-search">
  ## RAG avec recherche vectorielle
</div>

Cet exemple montre comment utiliser LlamaIndex avec Firecrawl pour explorer un site web, générer des embeddings et interroger le contenu à l’aide de la RAG.

```typescript
import Firecrawl from '@mendable/firecrawl-js';
import { Document, VectorStoreIndex, Settings } from 'llamaindex';
import { OpenAI, OpenAIEmbedding } from '@llamaindex/openai';

Settings.llm = new OpenAI({ model: "gpt-4o" });
Settings.embedModel = new OpenAIEmbedding({ model: "text-embedding-3-small" });

const firecrawl = new Firecrawl({ apiKey: process.env.FIRECRAWL_API_KEY });
const crawlResult = await firecrawl.crawl('https://firecrawl.dev', {
  limit: 10,
  scrapeOptions: { formats: ['markdown'] }
});
console.log(`${crawlResult.data.length} pages explorées`);

const documents = crawlResult.data.map((page: any, i: number) =>
  new Document({
    text: page.markdown,
    id_: `page-${i}`,
    metadata: { url: page.metadata?.sourceURL }
  })
);

const index = await VectorStoreIndex.fromDocuments(documents);
console.log('Index vectoriel créé avec les embeddings');

const queryEngine = index.asQueryEngine();
const response = await queryEngine.query({ query: 'Qu\'est-ce que Firecrawl et comment fonctionne-t-il ?' });

console.log('\nRéponse :', response.toString());
```

Pour plus d’exemples, consultez la [documentation de LlamaIndex](https://ts.llamaindex.ai/).
