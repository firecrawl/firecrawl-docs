---
title: "Choisir l’extracteur de données"
description: "Comparer /agent, /extract et /scrape (mode JSON) pour choisir le bon outil d’extraction de données structurées"
og:title: "Choisir l’extracteur de données | Firecrawl"
og:description: "Comparer /agent, /extract et /scrape (mode JSON) pour choisir le bon outil d’extraction de données structurées"
sidebarTitle: "Choisir l’extracteur de données"
---

import AgentWithSchemaPython from "/snippets/fr/v2/agent/with-schema/python.mdx";
import AgentWithSchemaJS from "/snippets/fr/v2/agent/with-schema/js.mdx";
import AgentWithSchemaCURL from "/snippets/fr/v2/agent/with-schema/curl.mdx";
import ExtractPython from "/snippets/fr/v2/extract/base/python.mdx";
import ExtractNode from "/snippets/fr/v2/extract/base/js.mdx";
import ExtractCURL from "/snippets/fr/v2/extract/base/curl.mdx";
import ScrapeJsonPython from "/snippets/fr/v2/scrape/json/base/python.mdx";
import ScrapeJsonNode from "/snippets/fr/v2/scrape/json/base/js.mdx";
import ScrapeJsonCURL from "/snippets/fr/v2/scrape/json/base/curl.mdx";

Firecrawl propose trois approches pour extraire des données structurées à partir de pages Web. Chacune répond à des cas d’usage différents, avec des niveaux variables d’automatisation et de contrôle.

<div id="quick-comparison">
  ## Comparaison rapide
</div>

| Fonctionnalité | `/agent` | `/extract` | `/scrape` (mode JSON) |
|---------|----------|------------|----------------------|
| **Statut** | Actif | Utilisez plutôt `/agent` | Actif |
| **URL requise** | Non (optionnelle) | Oui (caractères génériques pris en charge) | Oui (URL unique) |
| **Portée** | Découverte à l’échelle du web | Plusieurs pages/domaines | Page unique |
| **Découverte d’URL** | Recherche web autonome | Explore à partir des URL fournies | Aucune |
| **Traitement** | Asynchrone | Asynchrone | Synchrone |
| **Schéma requis** | Non (prompt ou schéma) | Non (prompt ou schéma) | Non (prompt ou schéma) |
| **Tarification** | Dynamique (5 exécutions gratuites par jour) | Basée sur les tokens (1 crédit = 15 tokens) | 1 crédit/page |
| **Idéal pour** | Recherche, découverte, collecte complexe | Extraction multi‑pages (quand vous connaissez les URL) | Extraction d’une page unique connue |

<div id="1-agent-endpoint">
  ## 1. `/agent` Endpoint
</div>

L’endpoint `/agent` est l’offre la plus avancée de Firecrawl, successeur de `/extract`. Il utilise des agents d’IA pour rechercher, parcourir et collecter de manière autonome des données sur l’ensemble du web.

<div id="key-characteristics">
  ### Caractéristiques principales
</div>

* **URL facultatives** : décrivez simplement ce dont vous avez besoin via le paramètre `prompt` ; les URL sont entièrement optionnelles
* **Navigation autonome** : l’agent effectue des recherches et navigue en profondeur dans les sites web pour trouver vos données
* **Recherche web approfondie** : découvre automatiquement des informations sur plusieurs domaines et pages
* **Traitement parallèle** : traite plusieurs sources simultanément pour des résultats plus rapides
* **Modèles disponibles** : `spark-1-mini` (par défaut, 60 % moins cher) et `spark-1-pro` (précision plus élevée)

<div id="example">
  ### Exemple
</div>

<CodeGroup>
  <AgentWithSchemaPython />

  <AgentWithSchemaJS />

  <AgentWithSchemaCURL />
</CodeGroup>

<div id="best-use-case-autonomous-research-discovery">
  ### Cas d’usage idéal : recherche et exploration autonomes
</div>

**Scénario** : Vous devez trouver des informations sur des startups d’IA qui ont levé un financement de série A, y compris leurs fondateurs et les montants levés.

**Pourquoi `/agent`** : Vous ne savez pas quels sites web contiennent ces informations. L’agent va explorer le web de manière autonome, naviguer vers les sources pertinentes (Crunchbase, sites d’actualités, pages d’entreprises) et compiler pour vous les données structurées.

Pour plus de détails, voir la [documentation de l’agent](/fr/features/agent).

***

<div id="2-extract-endpoint">
  ## 2. Endpoint `/extract`
</div>

<Note>
  **Utilisez plutôt `/agent`** : nous recommandons de migrer vers [`/agent`](/fr/features/agent) — il est plus rapide, plus fiable, ne nécessite pas d’URL et couvre tous les cas d’usage de `/extract`, et plus encore.
</Note>

L’endpoint `/extract` collecte des données structurées à partir d’URL spécifiées ou de domaines entiers grâce à une extraction basée sur des LLM.

<div id="key-characteristics">
  ### Caractéristiques principales
</div>

* **URL généralement nécessaires** : fournissez au moins une URL (prend en charge les caractères génériques comme `example.com/*`)
* **Exploration de domaine** : peut explorer et analyser toutes les URL découvertes dans un domaine
* **Amélioration de la recherche sur le web** : `enableWebSearch` optionnel pour suivre les liens en dehors des domaines spécifiés
* **Schéma facultatif** : prend en charge un schéma JSON strict OU des prompts en langage naturel
* **Traitement asynchrone** : renvoie un ID de tâche pour la vérification de l’état

<div id="the-url-limitation">
  ### La limitation liée aux URL
</div>

Le défi fondamental avec `/extract` est que vous devez généralement connaître les URL à l&#39;avance :

1. **Lacune de découverte** : pour des tâches comme « trouver les entreprises YC W24 », vous ne savez pas quelles URL contiennent les données. Vous devez ajouter une étape de recherche distincte avant d&#39;appeler `/extract`.
2. **Recherche web peu adaptée** : même si `enableWebSearch` existe, il est limité aux URL que vous fournissez — ce qui en fait un workflow peu adapté aux tâches de découverte.
3. **Pourquoi `/agent` a été créé** : `/extract` est efficace pour extraire des données à partir d&#39;emplacements connus, mais moins efficace pour découvrir où se trouvent les données.

<div id="example">
  ### Exemple
</div>

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

<div id="best-use-case-targeted-multi-page-extraction">
  ### Cas d&#39;utilisation idéal : extraction ciblée multi‑pages
</div>

**Scénario** : vous avez l&#39;URL de la documentation de votre concurrent et vous voulez en extraire tous ses endpoints d&#39;API à partir de `docs.competitor.com/*`.

**Pourquoi `/extract` fonctionnait bien ici** : vous connaissiez exactement le domaine. Mais même dans ce cas, l&#39;utilisation de `/agent` avec des URL fournies donnera généralement de meilleurs résultats aujourd&#39;hui.

Pour plus de détails, voir la [documentation Extract](/fr/features/extract).

***

<div id="3-scrape-endpoint-with-json-mode">
  ## 3. Endpoint `/scrape` avec mode JSON
</div>

L&#39;endpoint `/scrape` avec mode JSON est l&#39;approche la plus contrôlée : il extrait des données structurées à partir d&#39;une URL unique connue à l&#39;aide d&#39;un LLM pour analyser le contenu de la page selon le schéma que vous avez spécifié.

<div id="key-characteristics">
  ### Caractéristiques principales
</div>

* **Une seule URL uniquement** : conçu pour extraire les données d&#39;une seule page spécifique à la fois
* **URL exacte requise** : vous devez connaître l&#39;URL précise contenant les données
* **Schéma optionnel** : peut utiliser un schéma JSON OU simplement un prompt (le LLM choisit la structure)
* **Synchrone** : renvoie les données immédiatement (aucun polling de tâche nécessaire)
* **Formats supplémentaires** : peut combiner l&#39;extraction JSON avec du markdown, du HTML et des captures d&#39;écran dans une même requête

<div id="example">
  ### Exemple
</div>

<CodeGroup>
  <ScrapeJsonPython />

  <ScrapeJsonNode />

  <ScrapeJsonCURL />
</CodeGroup>

<div id="best-use-case-single-page-precision-extraction">
  ### Cas d’usage idéal : extraction précise sur une seule page
</div>

**Scénario** : Vous développez un outil de suivi des prix et devez extraire le prix, le statut de disponibilité et les détails du produit à partir d’une page produit spécifique dont vous avez déjà l’URL.

**Pourquoi `/scrape` avec le mode JSON** : Vous savez exactement quelle page contient les données, vous avez besoin d’une extraction précise sur une seule page et vous voulez des résultats synchrones sans la complexité de gestion des jobs.

Pour plus de détails, consultez la [documentation du mode JSON](/fr/features/llm-extract).

***

<div id="decision-guide">
  ## Guide de décision
</div>

**Connaissez-vous l’URL ou les URL exactes contenant vos données ?**

* **NON** → Utilisez `/agent` (découverte autonome du web)
* **OUI**
  * **Une seule page ?** → Utilisez `/scrape` avec le mode JSON
  * **Plusieurs pages ?** → Utilisez `/agent` avec des URL (ou un batch `/scrape`)

<div id="recommendations-by-scenario">
  ### Recommandations par scénario
</div>

| Scénario | Endpoint recommandé |
|----------|---------------------|
| « Trouver toutes les startups IA et leurs financements » | `/agent` |
| « Extraire les données de cette page produit spécifique » | `/scrape` (mode JSON) |
| « Récupérer tous les articles de blog de competitor.com » | `/agent` avec URL |
| « Surveiller les prix sur plusieurs URL connues » | `/scrape` avec traitement par lots |
| « Rechercher des entreprises dans un secteur spécifique » | `/agent` |
| « Extraire les coordonnées à partir de 50 pages d’entreprise connues » | `/scrape` avec traitement par lots |

***

<div id="pricing">
  ## Tarification
</div>

| Endpoint | Coût | Remarques |
|----------|------|-----------|
| `/scrape` (mode JSON) | 1 crédit/page | Fixe, prévisible |
| `/extract` | Facturation au jeton (1 crédit = 15 jetons) | Variable selon le contenu |
| `/agent` | Dynamique | 5 exécutions gratuites/jour ; varie selon la complexité |

<div id="example-find-the-founders-of-firecrawl">
  ### Exemple : « Trouver les fondateurs de Firecrawl »
</div>

| Endpoint | Fonctionnement | Crédits utilisés |
|----------|----------------|------------------|
| `/scrape` | Vous trouvez l’URL manuellement, puis effectuez le scraping d’1 page | ~1 crédit |
| `/extract` | Vous fournissez une ou plusieurs URL, il extrait des données structurées | Variable (en fonction des tokens) |
| `/agent` | Envoyez simplement le prompt — l’agent trouve et extrait | ~15 crédits |

**Compromis** : `/scrape` est le moins cher mais nécessite de connaître l’URL. `/agent` coûte plus cher mais gère la découverte automatiquement.

Pour des informations détaillées sur les tarifs, voir [Firecrawl Pricing](https://firecrawl.dev/pricing).

***

<div id="migration-extract-agent">
  ## Migration : `/extract` → `/agent`
</div>

Si vous utilisez actuellement `/extract`, la migration est directe :

**Avant (extract) :**

```python
result = app.extract(
    urls=["https://example.com/*"],
    prompt="Extract product information",
    schema=schema
)
```

**Après (agent) :**

```python
result = app.agent(
    urls=["https://example.com"],  # Optionnel - peut être omis entièrement
    prompt="Extract product information from example.com",
    schema=schema,
    model="spark-1-mini"  # ou "spark-1-pro" pour une meilleure précision
)
```

L’avantage principal : avec `/agent`, vous pouvez ignorer totalement les URL et simplement décrire ce dont vous avez besoin.

***

<div id="key-takeaways">
  ## Points clés à retenir
</div>

1. **Vous connaissez l’URL exacte ?** Utilisez `/scrape` avec le mode JSON — c’est l’option la moins chère (1 crédit/page), la plus rapide (synchrone) et la plus prévisible.

2. **Vous avez besoin de recherche autonome ?** Utilisez `/agent` — il gère la découverte automatiquement avec 5 exécutions gratuites/jour, puis une tarification dynamique en fonction de la complexité.

3. **Migrez de `/extract`** vers `/agent` pour les nouveaux projets — `/agent` en est le successeur, avec de meilleures fonctionnalités.

4. **Compromis entre coût et commodité :** `/scrape` est le plus économique lorsque vous connaissez vos URL ; `/agent` coûte plus cher mais élimine la découverte manuelle des URL.

***

<div id="further-reading">
  ## Pour aller plus loin
</div>

* [Documentation de l&#39;agent](/fr/features/agent)
* [Modèles d&#39;agent](/fr/features/models)
* [Documentation du mode JSON](/fr/features/llm-extract)
* [Documentation d&#39;Extract](/fr/features/extract)
* [Scraping par lots](/fr/features/batch-scrape)