---
title: 'Exploration'
description: 'Firecrawl peut explorer récursivement les sous-domaines d’une URL et collecter leur contenu'
og:title: "Exploration | Firecrawl"
og:description: "Firecrawl peut explorer récursivement les sous-domaines d’une URL et collecter leur contenu"
---

import InstallationPython from "/snippets/fr/v1/installation/python.mdx";
import InstallationNode from "/snippets/fr/v1/installation/js.mdx";
import InstallationGo from "/snippets/fr/v1/installation/go.mdx";
import InstallationRust from "/snippets/fr/v1/installation/rust.mdx";
import CrawlPython from "/snippets/fr/v1/crawl/base/python.mdx";
import CrawlNode from "/snippets/fr/v1/crawl/base/js.mdx";
import CrawlGo from "/snippets/fr/v1/crawl/base/go.mdx";
import CrawlRust from "/snippets/fr/v1/crawl/base/rust.mdx";
import CrawlCURL from "/snippets/fr/v1/crawl/base/curl.mdx";
import AsyncCrawlOutput from "/snippets/fr/v1/crawl-async/base/output.mdx";
import CheckCrawlJobPython from "/snippets/fr/v1/crawl-status/short/python.mdx";
import CheckCrawlJobNode from "/snippets/fr/v1/crawl-status/short/js.mdx";
import CheckCrawlJobGo from "/snippets/fr/v1/crawl-status/short/go.mdx";
import CheckCrawlJobRust from "/snippets/fr/v1/crawl-status/short/rust.mdx";
import CheckCrawlJobCURL from "/snippets/fr/v1/crawl-status/short/curl.mdx";
import CheckCrawlJobOutputScraping from "/snippets/fr/v1/crawl-status/base/output-scraping.mdx";
import CheckCrawlJobOutputCompleted from "/snippets/fr/v1/crawl-status/base/output-completed.mdx";
import CrawlWebSocketPython from "/snippets/fr/v1/crawl-websocket/base/python.mdx";
import CrawlWebSocketNode from "/snippets/fr/v1/crawl-websocket/base/js.mdx";
import CrawlWebhookCURL from "/snippets/fr/v1/crawl-webhook/base/curl.mdx";
import PythonCrawlExample from "/snippets/fr/v1/crawl/sdk-example/python.mdx";
import NodeCrawlExample from "/snippets/fr/v1/crawl/sdk-example/js.mdx";
import PythonCrawlExampleResponse from "/snippets/fr/v1/crawl/sdk-example/python-response.mdx";
import NodeCrawlExampleResponse from "/snippets/fr/v1/crawl/sdk-example/js-response.mdx";
import FastCrawlPython from "/snippets/fr/v1/crawl/fast/python.mdx";
import FastCrawlNode from "/snippets/fr/v1/crawl/fast/js.mdx";
import FastCrawlGo from "/snippets/fr/v1/crawl/fast/go.mdx";
import FastCrawlRust from "/snippets/fr/v1/crawl/fast/rust.mdx";
import FastCrawlCURL from "/snippets/fr/v1/crawl/fast/curl.mdx";

Firecrawl explore efficacement les sites web pour extraire des données complètes tout en contournant les bloqueurs. Le processus :

1. **Analyse d’URL :** Analyse le sitemap et parcourt le site pour identifier les liens
2. **Parcours :** Suit les liens de manière récursive pour trouver toutes les sous-pages
3. **Scraping :** Extrait le contenu de chaque page, gère le JavaScript et les limites de débit
4. **Résultats :** Convertit les données en Markdown propre ou en format structuré

Cela garantit une collecte de données exhaustive à partir de n’importe quelle URL de départ.

<div id="crawling">
  ## Exploration
</div>

<div id="crawl-endpoint">
  ### point de terminaison /crawl
</div>

Permet d’explorer une URL et toutes les sous-pages accessibles. Cette opération soumet un job d’exploration et renvoie un ID de job pour suivre l’état de l’exploration.

<Warning>Par défaut, l’exploration ignore les liens d’une page s’ils ne sont pas des descendants directs de l’URL fournie. Ainsi, website.com/other-parent/blog-1 ne sera pas renvoyé si vous explorez website.com/blogs/. Si vous souhaitez inclure website.com/other-parent/blog-1, utilisez le paramètre `crawlEntireDomain`. Pour explorer des sous-domaines comme blog.website.com lors de l’exploration de website.com, utilisez le paramètre `allowSubdomains`.</Warning>

<div id="installation">
  ### Installation
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationGo />

  <InstallationRust />
</CodeGroup>

<div id="usage">
  ### Utilisation
</div>

<CodeGroup>
  <CrawlPython />

  <CrawlNode />

  <CrawlGo />

  <CrawlRust />

  <CrawlCURL />
</CodeGroup>

<div id="api-response">
  ### Réponse de l’API
</div>

Si vous utilisez cURL ou les fonctions `async crawl` des SDK, un `ID` vous sera renvoyé pour vérifier l’état du crawl.

<Note>Si vous utilisez le SDK, consultez la section Réponse du SDK [ci-dessous](#sdk-response).</Note>

<AsyncCrawlOutput />

<div id="check-crawl-job">
  ### Vérifier une tâche de crawl
</div>

Permet de vérifier l’état d’une tâche de crawl et d’en récupérer le résultat.

<Note>Ce point de terminaison ne fonctionne que pour les crawls en cours ou récemment terminés.</Note>

<CodeGroup>
  <CheckCrawlJobPython />

  <CheckCrawlJobNode />

  <CheckCrawlJobGo />

  <CheckCrawlJobRust />

  <CheckCrawlJobCURL />
</CodeGroup>

<div id="response-handling">
  #### Gestion des réponses
</div>

La réponse varie selon l’état du crawl.

Pour les réponses incomplètes ou volumineuses dépassant 10 Mo, un paramètre d’URL `next` est fourni. Vous devez appeler cette URL pour récupérer les 10 Mo de données suivants. Si le paramètre `next` est absent, cela indique la fin des données du crawl.

Le paramètre skip définit le nombre maximal de résultats renvoyés pour chaque bloc de résultats.

<Info>
  Les paramètres skip et next ne sont pertinents que lorsque vous interrogez directement l’API. Si vous utilisez le SDK, nous nous en chargeons et renverrons tous les résultats en une seule fois.
</Info>

<CodeGroup>
  <CheckCrawlJobOutputScraping />

  <CheckCrawlJobOutputCompleted />
</CodeGroup>

<div id="sdk-response">
  ### Réponse du SDK
</div>

Le SDK propose deux manières d’explorer des URL :

1. **Exploration synchrone** (`crawl_url`/`crawlUrl`) :
   * Attend la fin de l’exploration et renvoie la réponse complète
   * Gère la pagination automatiquement
   * Recommandée dans la plupart des cas d’usage

<CodeGroup>
  <PythonCrawlExample />

  <NodeCrawlExample />
</CodeGroup>

La réponse inclut le statut de l’exploration et toutes les données collectées :

<CodeGroup>
  <PythonCrawlExampleResponse />

  <NodeCrawlExampleResponse />
</CodeGroup>

2. **Exploration asynchrone** (`async_crawl_url`/`asyncCrawlUrl`) :
   * Renvoie immédiatement un ID d’exploration
   * Permet de vérifier l’état manuellement
   * Utile pour les explorations longues ou une logique de sondage (polling) personnalisée

<CodeGroup>
  <AsyncCrawlPython />

  <AsyncCrawlNode />
</CodeGroup>

<div id="faster-crawling">
  ## Exploration plus rapide
</div>

Accélérez vos explorations de 500 % lorsque vous n’avez pas besoin des données les plus récentes. Ajoutez `maxAge` à vos `scrapeOptions` pour utiliser les données de page mises en cache lorsqu’elles sont disponibles.

<CodeGroup>
  <FastCrawlPython />

  <FastCrawlNode />

  <FastCrawlGo />

  <FastCrawlRust />

  <FastCrawlCURL />
</CodeGroup>

**Comment ça fonctionne :**

* Chaque page de votre exploration vérifie si nous avons des données en cache plus récentes que `maxAge`
* Le cas échéant, la réponse est renvoyée instantanément depuis le cache (jusqu’à 500 % plus rapide)
* Sinon, la page est récupérée à neuf et le résultat est mis en cache
* Idéal pour explorer des sites de documentation, des catalogues de produits ou d’autres contenus relativement statiques

Pour plus de détails sur l’utilisation de `maxAge`, consultez la documentation [Faster Scraping](/fr/features/fast-scraping).

<div id="crawl-websocket">
  ## WebSocket de crawl
</div>

La méthode basée sur WebSocket de Firecrawl, `Crawl URL and Watch`, permet l’extraction et le suivi des données en temps réel. Démarrez un crawl à partir d’une URL et personnalisez-le avec des options comme les limites de pages, les domaines autorisés et les formats, idéal pour le traitement immédiat des données.

<CodeGroup>
  <CrawlWebSocketPython />

  <CrawlWebSocketNode />
</CodeGroup>

<div id="crawl-webhook">
  ## Webhook de crawl
</div>

Vous pouvez configurer des webhooks pour recevoir des notifications en temps réel au fur et à mesure de l’avancement de votre crawl. Cela vous permet de traiter les pages dès leur extraction, plutôt que d’attendre la fin du crawl.

<CrawlWebhookCURL />

Pour une documentation complète sur les webhooks — types d’événements, structure des charges utiles et exemples d’implémentation — consultez la [documentation des webhooks](/fr/features/webhooks).

<div id="quick-reference">
  ### Référence rapide
</div>

**Types d’événements :**

* `crawl.started` - Au démarrage de l’exploration
* `crawl.page` - Pour chaque page extraite avec succès
* `crawl.completed` - À la fin de l’exploration
* `crawl.failed` - Si l’exploration échoue

**Payload de base :**

```json
{
  "success": true,
  "type": "crawl.page",
  "id": "crawl-job-id",
  "data": [...], // Données de la page pour les événements « page »
  "metadata": {}, // Vos métadonnées personnalisées
  "error": null
}
```

<Note>
  Pour une configuration détaillée des webhooks, des bonnes pratiques de sécurité et des solutions de dépannage, consultez la [documentation Webhooks](/fr/features/webhooks).
</Note>
