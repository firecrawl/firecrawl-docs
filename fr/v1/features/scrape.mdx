---
title: "Extraction"
description: "Transformez n'importe quelle URL en données propres"
og:title: "Extraction | Firecrawl"
og:description: "Transformez n'importe quelle URL en données propres"
---

import InstallationPython from "/snippets/fr/v1/installation/python.mdx";
import InstallationNode from "/snippets/fr/v1/installation/js.mdx";
import InstallationGo from "/snippets/fr/v1/installation/go.mdx";
import InstallationRust from "/snippets/fr/v1/installation/rust.mdx";
import ScrapePython from "/snippets/fr/v1/scrape/base/python.mdx";
import ScrapeNode from "/snippets/fr/v1/scrape/base/js.mdx";
import ScrapeGo from "/snippets/fr/v1/scrape/base/go.mdx";
import ScrapeRust from "/snippets/fr/v1/scrape/base/rust.mdx";
import ScrapeCURL from "/snippets/fr/v1/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/fr/v1/scrape/base/output.mdx";
import ExtractCURL from "/snippets/fr/v1/llm-extract/base/curl.mdx";
import ExtractPython from "/snippets/fr/v1/llm-extract/base/python.mdx";
import ExtractNode from "/snippets/fr/v1/llm-extract/base/js.mdx";
import ExtractOutput from "/snippets/fr/v1/llm-extract/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/fr/v1/llm-extract/no-schema/curl.mdx";
import ExtractNoSchemaOutput from "/snippets/fr/v1/llm-extract/no-schema/output.mdx";
import ScrapeActionsPython from "/snippets/fr/v1/scrape/actions/python.mdx";
import ScrapeActionsNode from "/snippets/fr/v1/scrape/actions/js.mdx";
import ScrapeActionsCURL from "/snippets/fr/v1/scrape/actions/curl.mdx";
import ScrapeActionsOutput from "/snippets/fr/v1/scrape/actions/output.mdx";
import BatchScrapePython from "/snippets/fr/v1/batch-scrape/base/python.mdx";
import BatchScrapeNode from "/snippets/fr/v1/batch-scrape/base/js.mdx";
import BatchScrapeCURL from "/snippets/fr/v1/batch-scrape/base/curl.mdx";
import BatchScrapeOutput from "/snippets/fr/v1/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/fr/v1/batch-scrape/base/async-output.mdx";
import ScrapeLocationPython from "/snippets/fr/v1/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/fr/v1/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/fr/v1/scrape/location/curl.mdx";

Firecrawl convertit des pages web en markdown, idéal pour les applications LLM.

* Il gère les aspects complexes : proxies, mise en cache, limites de débit, contenu bloqué par JS
* Prend en charge le contenu dynamique : sites dynamiques, pages rendues en JS, PDF, images
* Produit un markdown propre, des données structurées, des captures d’écran ou du HTML.

Pour plus de détails, consultez la [référence de l’API du point de terminaison /scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="scraping-a-url-with-firecrawl">
  ## Scraper une URL avec Firecrawl
</div>

<div id="scrape-endpoint">
  ### point de terminaison /scrape
</div>

Utilisé pour scraper une URL et en récupérer le contenu.

<div id="installation">
  ### Installation
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationGo />

  <InstallationRust />
</CodeGroup>

<div id="usage">
  ### Utilisation
</div>

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeGo />

  <ScrapeRust />

  <ScrapeCURL />
</CodeGroup>

Pour en savoir plus sur les paramètres, consultez la [référence de l’API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="response">
  ### Réponse
</div>

Les SDK renvoient directement l’objet de données. cURL renvoie la charge utile exactement comme ci-dessous.

<ScrapeResponse />

<div id="scrape-formats">
  ## Formats de scraping
</div>

Vous pouvez désormais choisir les formats de sortie souhaités. Vous pouvez spécifier plusieurs formats de sortie. Les formats pris en charge sont :

* Markdown (markdown)
* HTML (html)
* HTML brut (rawHtml) (sans modification)
* Capture d’écran (screenshot ou screenshot@fullPage)
* Liens (links)
* JSON (json) — sortie structurée

Les clés de sortie correspondront au format choisi.

<div id="extract-structured-data">
  ## Extraire des données structurées
</div>

<div id="scrape-with-json-endpoint">
  ### Point de terminaison /scrape (avec json)
</div>

Permet d’extraire des données structurées à partir de pages explorées.

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

Résultat :

<ExtractOutput />

<div id="extracting-without-schema-new">
  ### Extraction sans schéma (Nouveau)
</div>

Vous pouvez désormais extraire sans schéma en fournissant simplement un `prompt` au point de terminaison. Le LLM choisit la structure des données.

<CodeGroup>
  <ExtractNoSchemaCURL />
</CodeGroup>

Résultat :

<ExtractNoSchemaOutput />

<div id="json-options-object">
  ### Objet d’options JSON
</div>

L’objet `jsonOptions` accepte les paramètres suivants :

* `schema` : Schéma à utiliser pour l’extraction.
* `systemPrompt` : Invite système à utiliser pour l’extraction.
* `prompt` : Invite à utiliser pour l’extraction sans schéma.

<div id="interacting-with-the-page-with-actions">
  ## Interagir avec la page à l’aide des actions
</div>

Firecrawl vous permet d’exécuter diverses actions sur une page web avant d’en extraire le contenu. C’est particulièrement utile pour interagir avec du contenu dynamique, naviguer entre des pages ou accéder à du contenu nécessitant une interaction de l’utilisateur.

Voici un exemple montrant comment utiliser des actions pour se rendre sur google.com, rechercher Firecrawl, cliquer sur le premier résultat et prendre une capture d’écran.

Il est important d’utiliser presque toujours l’action `wait` avant et/ou après l’exécution d’autres actions afin de laisser suffisamment de temps au chargement de la page.

<div id="example">
  ### Exemple
</div>

<CodeGroup>
  <ScrapeActionsPython />

  <ScrapeActionsNode />

  <ScrapeActionsCURL />
</CodeGroup>

<div id="output">
  ### Sortie
</div>

<CodeGroup>
  <ScrapeActionsOutput />
</CodeGroup>

Pour en savoir plus sur les paramètres des actions, consultez la [référence de l’API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="location-and-language">
  ## Localisation et langue
</div>

Indiquez le pays et les langues préférées pour obtenir du contenu pertinent en fonction de votre localisation cible et de vos préférences linguistiques.

<div id="how-it-works">
  ### Fonctionnement
</div>

Lorsque vous définissez les paramètres de localisation, Firecrawl utilise, si disponible, un proxy approprié et adopte les paramètres de langue et de fuseau horaire correspondants. Par défaut, la localisation est définie sur « US » si aucun paramètre n’est précisé.

<div id="usage">
  ### Utilisation
</div>

Pour utiliser les paramètres de localisation et de langue, incluez l’objet `location` dans le corps de la requête avec les propriétés suivantes :

* `country` : code pays ISO 3166-1 alpha-2 (p. ex. « US », « AU », « DE », « JP »). Valeur par défaut : « US ».
* `languages` : tableau des langues et paramètres régionaux préférés pour la requête, par ordre de priorité. Par défaut, la langue du lieu spécifié.

<CodeGroup>
  <ScrapeLocationPython />

  <ScrapeLocationNode />

  <ScrapeLocationCURL />
</CodeGroup>

<div id="batch-scraping-multiple-urls">
  ## Extraction par lots de plusieurs URL
</div>

Vous pouvez désormais extraire par lots plusieurs URL en même temps. La fonction prend les URL de départ et des paramètres optionnels comme arguments. L’argument params vous permet de définir des options supplémentaires pour la tâche d’extraction par lots, telles que les formats de sortie.

<div id="how-it-works">
  ### Fonctionnement
</div>

C’est très similaire au comportement du point de terminaison `/crawl`. Il soumet une tâche de scraping par lot et renvoie un identifiant de tâche pour en suivre l’état.

Le SDK propose deux méthodes : synchrone et asynchrone. La méthode synchrone renvoie les résultats du scraping par lot, tandis que la méthode asynchrone renvoie un identifiant de tâche que vous pouvez utiliser pour vérifier l’état du scraping par lot.

<div id="usage">
  ### Utilisation
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Réponse
</div>

Si vous utilisez les méthodes synchrones des SDK, elles renverront les résultats de la tâche de scraping par lots. Sinon, un identifiant de tâche sera renvoyé, que vous pourrez utiliser pour vérifier l’état du scraping par lots.

<div id="synchronous">
  #### Synchrone
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### Asynchrone
</div>

Vous pouvez ensuite utiliser l’ID du job pour vérifier l’état de l’extraction par lots en appelant le point de terminaison `/batch/scrape/{id}`. Ce point de terminaison est conçu pour être utilisé pendant l’exécution du job ou juste après son achèvement, **car les jobs d’extraction par lots expirent au bout de 24 heures**.

<BatchScrapeAsyncOutput />

<div id="stealth-mode">
  ## Mode furtif
</div>

Pour les sites dotés d’une protection anti-bot avancée, Firecrawl propose un mode proxy furtif qui augmente le taux de réussite du scraping sur les sites difficiles.

En savoir plus sur le [mode furtif](/fr/features/stealth-mode).

<div id="using-fire-1-with-scrape">
  ## Utiliser FIRE-1 avec Scrape
</div>

Vous pouvez utiliser l’agent FIRE-1 avec le point de terminaison `/scrape` pour effectuer une navigation intelligente avant de récupérer le contenu final.

L’activation de FIRE-1 est simple : incluez simplement un objet `agent` dans votre requête API de scrape ou d’extract :

```json
"agent": {
  "model": "FIRE-1",
  "prompt": "Saisissez ici vos instructions de navigation détaillées."
}
```

*Remarque :* Le champ `prompt` est obligatoire pour les requêtes de scraping ; il indique précisément à FIRE-1 comment interagir avec la page web.

<div id="example-usage-with-scrape-endpoint">
  ### Exemple d’utilisation avec le point de terminaison /scrape
</div>

Voici un exemple rapide montrant l’usage de FIRE-1 avec le point de terminaison /scrape pour récupérer les entreprises du segment grand public depuis Y Combinator :

```bash
curl -X POST https://api.firecrawl.dev/v1/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer VOTRE_CLÉ_API' \
  -d '{
    "url": "https://ycombinator.com/companies",
    "formats": ["markdown"],
    "agent": {
      "model": "FIRE-1",
      "prompt": "Récupérer les entreprises W22 dans le secteur grand public en cliquant sur les boutons correspondants"
    }
  }'
```
