---
title: "Extraction en lot"
description: "Extraire plusieurs URL en lot"
og:title: "Extraction en lot | Firecrawl"
og:description: "Extraire plusieurs URL en lot"
---

import BatchScrapePython from '/snippets/fr/v1/batch-scrape/base/python.mdx';
import BatchScrapeNode from '/snippets/fr/v1/batch-scrape/base/js.mdx';
import BatchScrapeCURL from '/snippets/fr/v1/batch-scrape/base/curl.mdx';
import BatchScrapeOutput from '/snippets/fr/v1/batch-scrape/base/output.mdx';
import BatchScrapeAsyncOutput from '/snippets/fr/v1/batch-scrape/base/async-output.mdx';
import BatchScrapeExtractPython from '/snippets/fr/v1/batch-scrape/extract/python.mdx';
import BatchScrapeExtractNode from '/snippets/fr/v1/batch-scrape/extract/js.mdx';
import BatchScrapeExtractCURL from '/snippets/fr/v1/batch-scrape/extract/curl.mdx';
import BatchScrapeExtractOutput from '/snippets/fr/v1/batch-scrape/extract/output.mdx';
import BatchScrapeExtractAsyncOutput from '/snippets/fr/v1/batch-scrape/extract/async-output.mdx';
import BatchScrapeWebhookCURL from '/snippets/fr/v1/batch-scrape-webhook/base/curl.mdx';

<div id="batch-scraping-multiple-urls">
  ## Extraction par lots de plusieurs URL
</div>

Vous pouvez désormais lancer une extraction par lots de plusieurs URL simultanément. La fonction prend en arguments les URL de départ et des paramètres optionnels. L’argument params vous permet de spécifier des options supplémentaires pour la tâche d’extraction par lots, comme les formats de sortie.

<div id="how-it-works">
  ### Fonctionnement
</div>

Le fonctionnement est très proche de celui de l’endpoint `/crawl`. Il lance un traitement de scraping par lots et renvoie un identifiant de tâche (job ID) pour en suivre l’état.

Le SDK propose deux méthodes, synchrone et asynchrone. La méthode synchrone renvoie les résultats du scraping par lots, tandis que la méthode asynchrone renvoie un identifiant de tâche que vous pouvez utiliser pour en suivre l’état.

<div id="usage">
  ### Utilisation
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Réponse
</div>

Si vous utilisez les méthodes synchrones des SDK, elles renverront les résultats de la tâche de scraping par lot. Sinon, un ID de tâche sera renvoyé, que vous pourrez utiliser pour vérifier l’état de la tâche de scraping par lot.

<div id="synchronous">
  #### Synchrone
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### Asynchrone
</div>

Vous pouvez ensuite utiliser l’ID du job pour vérifier l’état du scraping par lot en appelant le point de terminaison `/batch/scrape/{id}`. Ce point de terminaison est conçu pour être utilisé pendant l’exécution du job ou juste après son achèvement, **car les jobs de scraping par lot expirent au bout de 24 heures**.

<BatchScrapeAsyncOutput />

<div id="batch-scrape-with-extraction">
  ## Extraction en lot avec scraping
</div>

Vous pouvez également utiliser le point de terminaison de scraping en lot pour extraire des données structurées des pages. C’est utile si vous souhaitez récupérer les mêmes données structurées à partir d’une liste d’URL.

<CodeGroup>
  <BatchScrapeExtractPython />

  <BatchScrapeExtractNode />

  <BatchScrapeExtractCURL />
</CodeGroup>

<div id="response">
  ### Réponse
</div>

<div id="synchronous">
  #### Synchrone
</div>

<BatchScrapeExtractOutput />

<div id="asynchronous">
  #### Asynchrone
</div>

<BatchScrapeExtractAsyncOutput />

<div id="batch-scrape-with-webhooks">
  ## Scraping par lots avec webhooks
</div>

Vous pouvez configurer des webhooks pour recevoir des notifications en temps réel à mesure que chaque URL de votre lot est scrapée. Cela vous permet de traiter les résultats immédiatement au lieu d’attendre la fin de tout le lot.

<BatchScrapeWebhookCURL />

Pour une documentation complète sur les webhooks, comprenant les types d’événements, la structure des payloads et des exemples d’implémentation, consultez la [documentation sur les webhooks](/fr/webhooks/overview).

<div id="quick-reference">
  ### Référence rapide
</div>

**Types d’événements :**

* `batch_scrape.started` - Quand le scraping par lot démarre
* `batch_scrape.page` - Pour chaque URL scrapée avec succès
* `batch_scrape.completed` - Quand toutes les URL sont traitées
* `batch_scrape.failed` - Si le scraping par lot rencontre une erreur

**Payload de base :**

```json
{
  "success": true,
  "type": "batch_scrape.page",
  "id": "batch-job-id",
  "data": [...], // Données de page pour les événements 'page'
  "metadata": {}, // Vos métadonnées personnalisées
  "error": null
}
```

<Note>
  Pour une configuration détaillée des webhooks, les meilleures pratiques de sécurité et
  des conseils de dépannage, consultez la [documentation sur les webhooks](/fr/webhooks/overview).
</Note>
