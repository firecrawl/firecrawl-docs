---
title: "Guide de scraping avancé"
description: "Découvrez comment améliorer votre scraping avec Firecrawl grâce à des options avancées."
og:title: "Guide de scraping avancé | Firecrawl"
og:description: "Découvrez comment améliorer votre scraping avec Firecrawl grâce à des options avancées."
---

Ce guide présente les différents points de terminaison de Firecrawl et explique comment les utiliser au mieux avec l’ensemble de leurs paramètres.

<div id="basic-scraping-with-firecrawl-scrape">
  ## Scraping de base avec Firecrawl (/scrape)
</div>

Pour extraire une seule page et obtenir du contenu Markdown propre, utilisez le point de terminaison /scrape.

<CodeGroup>

```python Python
# pip install firecrawl-py

from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR_API_KEY")

content = app.scrape_url("https://docs.firecrawl.dev")
```

```JavaScript JavaScript
// npm install @mendable/firecrawl-js

import { FirecrawlApp } from 'firecrawl-js';

const app = new FirecrawlApp({ apiKey: 'YOUR_API_KEY' });

const content = await app.scrapeUrl('https://docs.firecrawl.dev');
```

```go Go
// go get github.com/mendableai/firecrawl-go

import (
  "fmt"
  "log"

  "github.com/mendableai/firecrawl-go"
)

func main() {
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
    log.Fatalf("Failed to initialize FirecrawlApp: %v", err)
  }

  content, err := app.ScrapeURL("docs.firecrawl.dev", nil)
  if err != nil {
    log.Fatalf("Failed)
  }
}
```

```rust Rust
// Installez la crate firecrawl_rs avec Cargo

use firecrawl_rs::FirecrawlApp;
#[tokio::main]
async fn main() {
  // Initialisez FirecrawlApp avec la clé API
  let api_key = "YOUR_API_KEY";
  let api_url = "https://api.firecrawl.dev";
  let app = FirecrawlApp::new(api_key, api_url).expect("Failed to initialize FirecrawlApp");

  let scrape_result = app.scrape_url("https://docs.firecrawl.dev", None).await;
  match scrape_result {
    Ok(data) => println!("Résultat du scraping:\n{}", data["markdown"]),
    Err(e) => eprintln!("Échec du scraping: {}", e),
  }
}
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

</CodeGroup>

<div id="scraping-pdfs">
  ## Extraction de PDF
</div>

**Firecrawl prend en charge l’extraction des PDF par défaut.** Vous pouvez utiliser le point de terminaison `/scrape` pour extraire un lien vers un PDF et obtenir le contenu texte du PDF. Vous pouvez désactiver cette option en définissant `parsePDF` sur `false`.

<div id="scrape-options">
  ## Options de scraping
</div>

Lorsque vous utilisez le point de terminaison `/scrape`, vous pouvez personnaliser le comportement de scraping à l’aide de nombreux paramètres. Voici les options disponibles :

<div id="setting-the-content-formats-on-response-with-formats">
  ### Définir les formats de contenu dans la réponse avec `formats`
</div>

- Type : `array`
- Énumération : `["markdown", "links", "html", "rawHtml", "screenshot", "json"]`
- Description : Indiquez les formats à inclure dans la réponse. Options possibles :
  - `markdown` : Renvoie le contenu extrait au format Markdown.
  - `links` : Inclut tous les liens hypertexte trouvés sur la page.
  - `html` : Fournit le contenu au format HTML.
  - `rawHtml` : Renvoie le HTML brut, sans aucun traitement.
  - `screenshot` : Inclut une capture d’écran de la page telle qu’elle apparaît dans le navigateur.
  - `json` : Extrait des informations structurées de la page à l’aide du LLM.
- Valeur par défaut : `["markdown"]`

<div id="getting-the-full-page-content-as-markdown-with-onlymaincontent">
  ### Obtenir l’intégralité du contenu de la page en markdown avec `onlyMainContent`
</div>

- **Type**: `boolean`
- **Description**: Par défaut, l’outil d’extraction ne renvoie que le contenu principal de la page, en excluant les en-têtes, barres de navigation, pieds de page, etc. Définissez ce paramètre sur `false` pour renvoyer l’intégralité du contenu de la page.
- **Valeur par défaut**: `true`

<div id="setting-the-tags-to-include-with-includetags">
  ### Définir les balises à inclure avec `includeTags`
</div>

- **Type**: `array`
- **Description**: Indiquez les balises HTML, classes et identifiants à inclure dans la réponse.
- **Default**: undefined

<div id="setting-the-tags-to-exclude-with-excludetags">
  ### Définir les balises à exclure avec `excludeTags`
</div>

- **Type**: `array`
- **Description**: Indiquez les balises, classes et identifiants (id) HTML à exclure de la réponse.
- **Default**: undefined

<div id="waiting-for-the-page-to-load-with-waitfor">
  ### Attente du chargement de la page avec `waitFor`
</div>

- **Type**: `integer`
- **Description**: À n’utiliser qu’en dernier recours. Attend un nombre de millisecondes défini avant de charger la page et de récupérer le contenu.
- **Default**: `0`

<div id="setting-the-maximum-timeout">
  ### Définition du `timeout` maximum
</div>

- **Type**: `integer`
- **Description**: Définit la durée maximale, en millisecondes, pendant laquelle le scraper attend la réponse de la page avant d’interrompre l’opération.
- **Par défaut**: `30000` (30 secondes)

<div id="parsing-pdf-files-with-parsepdf">
  ### Analyse des fichiers PDF avec `parsePDF`
</div>

- **Type**: `boolean`
- **Description**: Détermine comment les fichiers PDF sont traités lors du scraping. Lorsque `true`, le contenu du PDF est extrait et converti en markdown, avec une facturation basée sur le nombre de pages (1 crédit par page). Lorsque `false`, le fichier PDF est renvoyé en base64 avec un tarif forfaitaire de 1 crédit au total.
- **Valeur par défaut**: `true`

<div id="example-usage">
  ### Exemple d’usage
</div>

```bash
curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer VOTRE_CLÉ_API' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "formats": ["markdown", "links", "html", "rawHtml", "screenshot"],
      "includeTags": ["h1", "p", "a", ".main-content"],
      "excludeTags": ["#ad", "#footer"],
      "onlyMainContent": false,
      "waitFor": 1000,
      "timeout": 15000,
      "parsePDF": false
    }'
```

Dans cet exemple, le scraper va :

* Renvoyer le contenu complet de la page en markdown.
* Inclure le markdown, le HTML brut, le HTML, les liens et la capture d’écran dans la réponse.
* La réponse n’inclura que les balises HTML `<h1>`, `<p>`, `<a>` et les éléments portant la classe `.main-content`, tout en excluant les éléments portant les ID `#ad` et `#footer`.
* Attendre 1000 millisecondes (1 seconde) que la page se charge avant de récupérer le contenu.
* Définir la durée maximale de la requête de scraping à 15000 millisecondes (15 secondes).
* Renvoyer les fichiers PDF au format base64 au lieu de les convertir en markdown.

Voici la documentation de l’API correspondante : [Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)


<div id="extractor-options">
  ## Options de l’extracteur
</div>

Avec le point de terminaison `/scrape`, vous pouvez définir des options pour **extraire des informations structurées** du contenu de la page via le paramètre `extract`. Voici les options disponibles :

<div id="using-the-llm-extraction">
  ### Utiliser l’extraction LLM
</div>

<div id="schema">
  ### schéma
</div>

- **Type**: `object`
- **Obligatoire**: Non si un prompt est fourni
- **Description**: Le schéma des données à extraire. Il définit la structure des données extraites.

<div id="system-prompt">
  ### consigne système
</div>

- **Type**: `string`
- **Required**: False
- **Description**: Consigne système pour le LLM.

<div id="prompt">
  ### prompt
</div>

- **Type**: `string`
- **Required**: Faux si un schéma est fourni
- **Description**: Une consigne destinée au LLM pour extraire les données selon la structure attendue.
- **Example**: `"Extract the features of the product"`

<div id="example-usage">
  ### Exemple d’utilisation
</div>

```bash
curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer VOTRE_CLÉ_API' \
    -d '{
      "url": "https://firecrawl.dev",
      "formats": ["markdown", "json"],
      "json": {
        "prompt": "Extrait les fonctionnalités du produit"
      }
    }'
```

```json
{
  "success": true,
  "data": {
    "content": "Contenu brut",
    "metadata": {
      "title": "Mendable",
      "description": "Mendable vous permet de créer facilement des applications de chat IA. Ingérez, personnalisez, puis déployez-les avec une seule ligne de code, où vous voulez. Proposé par SideGuide",
      "robots": "follow, index",
      "ogTitle": "Mendable",
      "ogDescription": "Mendable vous permet de créer facilement des applications de chat IA. Ingérez, personnalisez, puis déployez-les avec une seule ligne de code, où vous voulez. Proposé par SideGuide",
      "ogUrl": "https://docs.firecrawl.dev/",
      "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",
      "ogLocaleAlternate": [],
      "ogSiteName": "Mendable",
      "sourceURL": "https://docs.firecrawl.dev/",
      "statusCode": 200
    },
    "extract": {
      "product": "Firecrawl",
      "features": {
        "general": {
          "description": "Transformez les sites web en données prêtes pour les LLM.",
          "openSource": true,
          "freeCredits": 500,
          "useCases": [
            "Applications d’IA",
            "Science des données",
            "Études de marché",
            "Agrégation de contenu",
          ]
        },
        "crawlingAndScraping": {
          "crawlAllAccessiblePages": true,
          "noSitemapRequired": true,
          "dynamicContentHandling": true,
          "dataCleanliness": {
            "process": "Algorithmes avancés",
            "outputFormat": "Markdown"
          }
        },
        ...
      }
    }
  }
}
```


<div id="actions">
  ## Actions
</div>

Lorsque vous utilisez le point de terminaison `/scrape`, Firecrawl vous permet d’exécuter diverses actions sur une page web avant d’en extraire le contenu. C’est particulièrement utile pour interagir avec du contenu dynamique, naviguer entre les pages ou accéder à du contenu nécessitant une interaction de l’utilisateur.

<div id="available-actions">
  ### Actions disponibles
</div>

<div id="wait">
  #### wait
</div>

- **Type**: `object`
- **Description**: Attendre pendant un nombre déterminé de millisecondes.
- **Properties**:
  - `type`: `"wait"`
  - `milliseconds`: Nombre de millisecondes à attendre.
- **Example**:
  ```json
  {
    "type": "wait",
    "milliseconds": 2000
  }
  ```

<div id="screenshot">
  #### screenshot
</div>

- **Type**: `object`
- **Description**: Prend une capture d’écran.
- **Properties**:
  - `type`: `"screenshot"`
  - `fullPage`: La capture doit-elle couvrir toute la page ou seulement la zone d’affichage (viewport) ? (valeur par défaut : `false`)
- **Example**:
  ```json
  {
    "type": "screenshot",
    "fullPage": true
  }
  ```

<div id="click">
  #### click
</div>

- **Type**: `object`
- **Description**: Clique sur un élément.
- **Properties**:
  - `type`: `"click"`
  - `selector`: Sélecteur CSS pour identifier l’élément.
- **Example**:
  ```json
  {
    "type": "click",
    "selector": "#load-more-button"
  }
  ```

<div id="write">
  #### write
</div>

- **Type**: `object`
- **Description**: Saisir du texte dans un champ de saisie.
- **Properties**:
  - `type`: `"write"`
  - `text`: Texte à taper.
  - `selector`: Sélecteur CSS du champ de saisie.
- **Example**:
  ```json
  {
    "type": "write",
    "text": "Hello, world!",
    "selector": "#search-input"
  }
  ```

<div id="press">
  #### press
</div>

- **Type**: `object`
- **Description**: Appuyer sur une touche de la page.
- **Properties**:
  - `type`: `"press"`
  - `key`: Touche à appuyer.
- **Example**:
  ```json
  {
    "type": "press",
    "key": "Enter"
  }
  ```

<div id="scroll">
  #### scroll
</div>

- **Type**: `object`
- **Description**: Faire défiler la page.
- **Properties**:
  - `type`: `"scroll"`
  - `direction`: Sens du défilement (`"up"` ou `"down"`).
  - `amount`: Nombre de pixels à faire défiler.
- **Example**:
  ```json
  {
    "type": "scroll",
    "direction": "down",
    "amount": 500
  }
  ```

<div id="scrape">
  #### scrape
</div>

- **Type**: `object`
- **Description**: Récupère le contenu de la page actuelle et renvoie l’URL ainsi que le HTML. Le contenu extrait est renvoyé dans le tableau `actions.scrapes` de la réponse.
- **Properties**:
  - `type`: `"scrape"`
- **Example**:
  ```json
  {
    "type": "scrape"
  }
  ```

<div id="pdf">
  #### pdf
</div>

- **Type**: `object`
- **Description**: Génère un PDF de la page en cours. Le PDF sera renvoyé dans le tableau `actions.pdfs` de la réponse.
- **Propriétés**:
  - `type`: `"pdf"`
  - `format`: La taille de page du PDF généré (par défaut : `"Letter"`)
  - `landscape`: Indique s’il faut générer le PDF en orientation paysage (par défaut : `false`)
  - `scale`: Le facteur d’échelle du PDF généré (par défaut : `1`)
- **Exemple**:
  ```json
  {
    "type": "pdf",
    "format": "A4",
    "landscape": true,
    "scale": 0.8
  }
  ```

<div id="executejavascript">
  #### executeJavascript
</div>

- **Type**: `object`
- **Description**: Exécute du code JavaScript sur la page. Les valeurs retournées seront présentes dans le tableau `actions.javascriptReturns` de la réponse.
- **Properties**:
  - `type`: `"executeJavascript"`
  - `script`: Code JavaScript à exécuter.
- **Example**:
  ```json
  {
    "type": "executeJavascript",
    "script": "document.querySelector('.button').click();"
  }
  ```

Pour plus de détails sur les paramètres des actions, consultez la [référence de l’API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="crawling-multiple-pages">
  ## Explorer plusieurs pages
</div>

Pour explorer plusieurs pages, vous pouvez utiliser le point de terminaison `/crawl`. Ce point de terminaison vous permet de spécifier une URL de base à explorer, et toutes les sous-pages accessibles seront automatiquement explorées.

```bash
curl -X POST https://api.firecrawl.dev/v1/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

Renvoie un ID

```json
{ "id": "1234-5678-9101" }
```


<div id="check-crawl-job">
  ### Vérifier une tâche de crawl
</div>

Permet de vérifier l’état d’une tâche de crawl et d’en obtenir le résultat.

```bash
curl -X GET https://api.firecrawl.dev/v1/crawl/1234-5678-9101 \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer VOTRE_CLÉ_API'
```


<div id="paginationnext-url">
  #### Pagination/URL suivante
</div>

Si le contenu dépasse 10 Mo ou si la tâche de crawl est encore en cours, la réponse inclut un paramètre `next`. Ce paramètre est l’URL de la page suivante de résultats. Vous pouvez l’utiliser pour récupérer la page suivante de résultats.

<div id="crawler-options">
  ### Options du crawler
</div>

Lorsque vous utilisez le point de terminaison `/crawl`, vous pouvez personnaliser le comportement d’exploration via les paramètres du corps de la requête. Voici les options disponibles :

<div id="includepaths">
  #### `includePaths`
</div>

- **Type**: `array`
- **Description**: Expressions régulières à inclure dans l’exploration. Seules les URL correspondant à ces expressions seront explorées. Par exemple, `^/blog/.*` correspondra à toute URL qui commence par `/blog/`.
- **Example**: `["^/blog/.*$", "^/docs/.*$"]`

<div id="excludepaths">
  #### `excludePaths`
</div>

- **Type**: `array`
- **Description**: Expressions régulières à exclure de l’exploration. Les URL correspondant à ces modèles seront ignorées. Par exemple, `^/admin/.*` exclura toute URL commençant par `/admin/`.
- **Example**: `["^/admin/.*$", "^/private/.*$"]`

<div id="maxdepth">
  #### `maxDepth`
</div>

- **Type**: `integer`
- **Description**: Profondeur absolue maximale à explorer depuis la base de l’URL renseignée. Par exemple, si le chemin de l’URL est `/features/feature-1`, aucun résultat ne sera renvoyé sauf si `maxDepth` est au moins égal à 2.
- **Example**: `2`

<div id="limit">
  #### `limit`
</div>

- **Type**: `integer`
- **Description**: Nombre maximal de pages à crawler.
- **Default**: `10000`

<div id="allowbackwardlinks">
  #### `allowBackwardLinks`
</div>

- **Type**: `boolean`
- **Description**: Autorise le crawler à suivre des liens internes vers des URL sœurs ou parentes, pas seulement des chemins enfants.
  - **false**: N’explore que des URL plus profondes (enfants).
    - p. ex. /features/feature-1 → /features/feature-1/tips ✅
    - Ne suivra pas /pricing ni / ❌
  - **true**: Explore tous les liens internes, y compris les sœurs et les parentes.
    - p. ex. /features/feature-1 → /pricing, /, etc. ✅
  - Utilisez true pour une couverture interne plus large au-delà des chemins imbriqués.
- **Default**: `false`

<div id="allowexternallinks">
  ### `allowExternalLinks`
</div>

- **Type**: `boolean`
- **Description**: Cette option permet au crawler de suivre les liens pointant vers des domaines externes. Faites preuve de prudence avec cette option, car elle peut amener l’exploration à ne s’arrêter qu’en fonction des valeurs de `limit` et `maxDepth`.
- **Default**: `false`

<div id="allowsubdomains">
  ### `allowSubdomains`
</div>

- **Type**: `boolean`
- **Description**: Autorise le crawler à suivre les liens vers les sous-domaines du domaine principal. Par exemple, si vous explorez `example.com`, cela autorisera le suivi de liens vers `blog.example.com` ou `api.example.com`.
- **Default**: `false`

<div id="delay">
  ### `delay`
</div>

- **Type**: `number`
- **Description**: Délai, en secondes, entre les opérations de scraping. Cela permet de respecter les limites de débit du site et d’éviter de surcharger la cible. S’il n’est pas défini, le crawler peut utiliser la directive crawl-delay de robots.txt si elle est présente.
- **Default**: `undefined`

<div id="scrapeoptions">
  #### scrapeOptions
</div>

Dans le cadre des options du crawler, vous pouvez également spécifier le paramètre `scrapeOptions`. Ce paramètre vous permet d’ajuster le comportement de scraping pour chaque page.

- **Type** : `object`
- **Description** : Options du scraper.
- **Example** : `{"formats": ["markdown", "links", "html", "rawHtml", "screenshot"], "includeTags": ["h1", "p", "a", ".main-content"], "excludeTags": ["#ad", "#footer"], "onlyMainContent": false, "waitFor": 1000, "timeout": 15000}`
- **Default** : `{ "formats": ["markdown"] }`
- **See** : [Options de scraping](#setting-the-content-formats-on-response-with-formats)

<div id="example-usage">
  ### Exemple d’utilisation
</div>

```bash
curl -X POST https://api.firecrawl.dev/v1/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "includePaths": ["^/blog/.*$", "^/docs/.*$"],
      "excludePaths": ["^/admin/.*$", "^/private/.*$"],
      "maxDepth": 2,
      "limit": 1000
    }'
```

Dans cet exemple, le crawler va :

* Ne parcourir que les URL correspondant aux motifs `^/blog/.*$` et `^/docs/.*$`.
* Ignorer les URL correspondant aux motifs `^/admin/.*$` et `^/private/.*$`.
* Renvoyer les données complètes du document pour chaque page.
* Explorer jusqu’à une profondeur maximale de 2.
* Explorer au maximum 1000 pages.


<div id="mapping-website-links-with-map">
  ## Cartographier les liens d’un site avec `/map`
</div>

Le point de terminaison `/map` est particulièrement efficace pour identifier les URL contextuellement liées à un site donné. Cette fonctionnalité est essentielle pour comprendre l’écosystème de liens contextuels d’un site, ce qui aide grandement à l’analyse stratégique et à la planification de la navigation.

<div id="usage">
  ### Utilisation
</div>

Pour utiliser le point de terminaison `/map`, envoyez une requête GET avec l’URL de la page à mapper. Voici un exemple avec `curl` :

```bash
curl -X POST https://api.firecrawl.dev/v1/map \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer VOTRE_JETON_API' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

Cela renverra un objet JSON contenant des liens contextuellement liés à l’URL.


<div id="example-response">
  ### Exemple de réponse
</div>

```json
  {
    "success": true,
    "links": [
      "https://docs.firecrawl.dev",
      "https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete",
      "https://docs.firecrawl.dev/api-reference/endpoint/crawl-get",
      "https://docs.firecrawl.dev/api-reference/endpoint/crawl-post",
      "https://docs.firecrawl.dev/api-reference/endpoint/map",
      "https://docs.firecrawl.dev/api-reference/endpoint/scrape",
      "https://docs.firecrawl.dev/api-reference/introduction",
      "https://docs.firecrawl.dev/articles/search-announcement",
      ...
    ]
  }
```


<div id="map-options">
  ### Options de mappage
</div>

<div id="search">
  #### `search`
</div>

- **Type**: `string`
- **Description**: Recherche de liens contenant un texte spécifique.
- **Example**: `"blog"`

<div id="limit">
  #### `limit`
</div>

- **Type**: `integer`
- **Description**: Nombre maximal de liens à retourner.
- **Default**: `100`

<div id="ignoresitemap">
  #### `ignoreSitemap`
</div>

- **Type**: `boolean`
- **Description**: Ignorer le sitemap du site lors de l'exploration
- **Default**: `true`

<div id="includesubdomains">
  #### `includeSubdomains`
</div>

- **Type**: `boolean`
- **Description**: Inclure les sous-domaines du site
- **Default**: `true`

Voici la référence de l’API à ce sujet : [Documentation du point de terminaison /map](https://docs.firecrawl.dev/api-reference/endpoint/map)

Merci de votre lecture !