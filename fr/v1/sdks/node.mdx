---
title: "Node"
description: "Le SDK Node de Firecrawl est une surcouche à l’API Firecrawl pour vous aider à convertir facilement des sites web en Markdown."
icon: "node"
og:title: "SDK Node | Firecrawl"
og:description: "Le SDK Node de Firecrawl est une surcouche à l’API Firecrawl pour vous aider à convertir facilement des sites web en Markdown."
---

import InstallationNode from '/snippets/fr/v1/installation/js.mdx'
import ScrapeAndCrawlExampleNode from '/snippets/fr/v1/scrape-and-crawl/js.mdx'
import ScrapeNodeShort from '/snippets/fr/v1/scrape/short/js.mdx'
import CrawlNodeShort from '/snippets/fr/v1/crawl/short/js.mdx'
import CrawlAsyncNodeShort from '/snippets/fr/v1/crawl-async/short/js.mdx'
import CheckCrawlStatusNodeShort from '/snippets/fr/v1/crawl-status/short/js.mdx'
import CancelCrawlNodeShort from '/snippets/fr/v1/crawl-delete/short/js.mdx'
import MapNodeShort from '/snippets/fr/v1/map/short/js.mdx'
import ExtractNodeShort from '/snippets/fr/v1/extract/short/js.mdx'
import CrawlWebSocketNodeBase from '/snippets/fr/v1/crawl-websocket/base/js.mdx'

<div id="installation">
  ## Installation
</div>

Pour installer le SDK Node Firecrawl, vous pouvez utiliser npm :

<InstallationNode />

<div id="usage">
  ## Utilisation
</div>

1. Récupérez une clé API sur [firecrawl.dev](https://firecrawl.dev)
2. Définissez la clé API comme variable d’environnement nommée `FIRECRAWL_API_KEY` ou transmettez-la en paramètre à la classe `FirecrawlApp`.

Voici un exemple d’utilisation du SDK avec gestion des erreurs :

<ScrapeAndCrawlExampleNode />

<div id="scraping-a-url">
  ### Extraction d’une URL
</div>

Pour extraire une URL unique avec gestion des erreurs, utilisez la méthode `scrapeUrl`. Elle prend l’URL en paramètre et renvoie les données extraites sous forme de dictionnaire.

<ScrapeNodeShort />

<div id="crawling-a-website">
  ### Exploration d’un site web
</div>

Pour explorer un site web avec gestion des erreurs, utilisez la méthode `crawlUrl`. Elle prend l’URL de départ et des paramètres optionnels comme arguments. Le paramètre `params` vous permet de définir des options supplémentaires pour la tâche d’exploration, telles que le nombre maximal de pages à explorer, les domaines autorisés et le format de sortie.

<CrawlNodeShort />

<div id="asynchronous-crawling">
  ### Exploration asynchrone
</div>

Pour explorer un site web de façon asynchrone, utilisez la méthode `crawlUrlAsync`. Elle renvoie l’`ID` de l’exploration, que vous pouvez utiliser pour vérifier l’état de la tâche. Elle prend comme arguments l’URL de départ et des paramètres optionnels. L’argument `params` vous permet de spécifier des options supplémentaires pour l’exploration, comme le nombre maximal de pages à parcourir, les domaines autorisés et le format de sortie.

<CrawlAsyncNodeShort />

<div id="checking-crawl-status">
  ### Vérifier l’état d’un crawl
</div>

Pour vérifier l’état d’un travail de crawl avec gestion des erreurs, utilisez la méthode `checkCrawlStatus`. Elle prend l’`ID` en paramètre et renvoie l’état actuel du travail de crawl.

<CheckCrawlStatusNodeShort />

<div id="cancelling-a-crawl">
  ### Annuler un crawl
</div>

Pour annuler un job de crawl asynchrone, utilisez la méthode `cancelCrawl`. Elle prend l’ID du job en paramètre et renvoie l’état de l’annulation.

<CancelCrawlNodeShort />

<div id="mapping-a-website">
  ### Cartographier un site web
</div>

Pour cartographier un site web avec gestion des erreurs, utilisez la méthode `mapUrl`. Elle prend l’URL de départ en paramètre et renvoie les données cartographiées sous forme de dictionnaire.

<MapNodeShort />

{/* ### Extraire des données structurées depuis des sites web

Pour extraire des données structurées depuis des sites web avec gestion des erreurs, utilisez la méthode `extractUrl`. Elle prend l’URL de départ en paramètre et renvoie les données extraites sous forme de dictionnaire.

<ExtractNodeShort /> */}

<div id="crawling-a-website-with-websockets">
  ### Explorer un site web avec des WebSockets
</div>

Pour explorer un site web avec des WebSockets, utilisez la méthode `crawlUrlAndWatch`. Elle prend comme arguments l’URL de départ et des paramètres optionnels. L’argument `params` vous permet de définir des options supplémentaires pour la tâche d’exploration, comme le nombre maximal de pages à explorer, les domaines autorisés et le format de sortie.

<CrawlWebSocketNodeBase />

<div id="error-handling">
  ## Gestion des erreurs
</div>

Le SDK gère les erreurs renvoyées par l’API Firecrawl et déclenche des exceptions appropriées. Si une erreur survient lors d’une requête, une exception est levée avec un message d’erreur explicite. Les exemples ci-dessus montrent comment gérer ces erreurs à l’aide de blocs `try/catch`.