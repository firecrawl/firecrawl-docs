---
title: "Python"
description: "Le SDK Python de Firecrawl est une surcouche de l’API Firecrawl qui vous aide à convertir facilement des sites web en Markdown."
icon: "python"
og:title: "SDK Python | Firecrawl"
og:description: "Le SDK Python de Firecrawl est une surcouche de l’API Firecrawl qui vous aide à convertir facilement des sites web en Markdown."
---

import InstallationPython from '/snippets/fr/v1/installation/python.mdx'
import ScrapePythonShort from '/snippets/fr/v1/scrape/short/python.mdx'
import CrawlPythonShort from '/snippets/fr/v1/crawl/short/python.mdx'
import CheckCrawlStatusPythonShort from '/snippets/fr/v1/crawl-status/short/python.mdx'
import CrawlAsyncPythonShort from '/snippets/fr/v1/crawl-async/short/python.mdx'
import CancelCrawlPythonShort from '/snippets/fr/v1/crawl-delete/short/python.mdx'
import MapPythonShort from '/snippets/fr/v1/map/short/python.mdx'
import ExtractPythonShort from '/snippets/fr/v1/extract/short/python.mdx'
import ScrapeAndCrawlExamplePython from '/snippets/fr/v1/scrape-and-crawl/python.mdx'
import CrawlWebSocketPythonBase from '/snippets/fr/v1/crawl-websocket/base/python.mdx'
import AsyncPythonShort from '/snippets/fr/v1/async/short/python.mdx'


<div id="installation">
  ## Installation
</div>

Pour installer le SDK Python Firecrawl, vous pouvez utiliser pip :

<InstallationPython />

<div id="usage">
  ## Utilisation
</div>

1. Récupérez une clé API sur [firecrawl.dev](https://firecrawl.dev)
2. Définissez la clé API comme variable d’environnement nommée `FIRECRAWL_API_KEY` ou transmettez-la en paramètre à la classe `FirecrawlApp`.

Voici un exemple d’utilisation du SDK :

<ScrapeAndCrawlExamplePython />

<div id="scraping-a-url">
  ### Extraction d’une URL
</div>

Pour extraire une URL unique, utilisez la méthode `scrape_url`. Elle prend l’URL en paramètre et renvoie les données extraites sous forme de dictionnaire.

<ScrapePythonShort />

<div id="crawling-a-website">
  ### Exploration d’un site web
</div>

Pour explorer un site web, utilisez la méthode `crawl_url`. Elle prend l’URL de départ et des paramètres optionnels en arguments. L’argument `params` permet de définir des options supplémentaires pour la tâche d’exploration, comme le nombre maximal de pages à explorer, les domaines autorisés et le format de sortie.

<CrawlPythonShort />

<div id="asynchronous-crawling">
  ### Exploration asynchrone
</div>

<Tip>Vous cherchez des opérations asynchrones ? Voir la section [Async Class](#async-class) ci-dessous.</Tip>

Pour explorer un site web de façon asynchrone, utilisez la méthode `crawl_url_async`. Elle renvoie l’`ID` du crawl, que vous pouvez utiliser pour vérifier l’état du job. Elle prend en arguments l’URL de départ et des paramètres optionnels. L’argument `params` permet de définir des options supplémentaires pour le crawl, comme le nombre maximum de pages à explorer, les domaines autorisés et le format de sortie.

<CrawlAsyncPythonShort />

<div id="checking-crawl-status">
  ### Vérifier l’état du crawl
</div>

Pour consulter l’état d’un job de crawl, utilisez la méthode `check_crawl_status`. Elle prend l’ID du job en paramètre et renvoie l’état actuel du crawl.

<CheckCrawlStatusPythonShort />

<div id="cancelling-a-crawl">
  ### Annuler un crawl
</div>

Pour annuler un crawl asynchrone, utilisez la méthode `cancel_crawl`. Elle prend en paramètre l’ID de la tâche et renvoie le statut de l’annulation.

<CancelCrawlPythonShort />

<div id="map-a-website">
  ### Cartographier un site web
</div>

Utilisez `map_url` pour générer une liste d’URL à partir d’un site web. Le paramètre `params` vous permet de personnaliser le processus de cartographie, avec des options pour exclure les sous-domaines ou exploiter le sitemap.

<MapPythonShort />

{/* ### Extraction de données structurées à partir de sites web

Pour extraire des données structurées à partir de sites web, utilisez la méthode `extract`. Elle prend en argument les URL à partir desquelles extraire les données, un prompt et un schéma. Le schéma est un modèle Pydantic qui définit la structure des données extraites.

<ExtractPythonShort /> */}

<div id="crawling-a-website-with-websockets">
  ### Explorer un site web avec WebSockets
</div>

Pour explorer un site web avec WebSockets, utilisez la méthode `crawl_url_and_watch`. Elle prend en arguments l’URL de départ et des paramètres optionnels. Le paramètre `params` vous permet de définir des options supplémentaires pour le job d’exploration, comme le nombre maximal de pages à explorer, les domaines autorisés et le format de sortie.

<CrawlWebSocketPythonBase />

<div id="error-handling">
  ## Gestion des erreurs
</div>

Le SDK gère les erreurs renvoyées par l’API Firecrawl et lève des exceptions appropriées. En cas d’erreur lors d’une requête, une exception est levée avec un message d’erreur explicite.

<div id="async-class">
  ## Classe asynchrone
</div>

Pour les opérations asynchrones, vous pouvez utiliser la classe `AsyncFirecrawlApp`. Ses méthodes sont identiques à celles de la classe `FirecrawlApp`, mais elles ne bloquent pas le thread principal.

<AsyncPythonShort />