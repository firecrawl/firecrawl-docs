---
title: "Python"
description: "Le SDK Python de Firecrawl est une surcouche à l’API Firecrawl qui vous aide à convertir facilement des sites web en Markdown."
icon: "python"
og:title: "SDK Python | Firecrawl"
og:description: "Le SDK Python de Firecrawl est une surcouche à l’API Firecrawl qui vous aide à convertir facilement des sites web en Markdown."
---

import InstallationPython from '/snippets/fr/v2/installation/python.mdx'
import ScrapePythonShort from '/snippets/fr/v2/scrape/short/python.mdx'
import CrawlPythonShort from '/snippets/fr/v2/crawl/short/python.mdx'
import CrawlSitemapOnlyPython from '/snippets/fr/v2/crawl/sitemap-only/python.mdx'
import CheckCrawlStatusPythonShort from '/snippets/fr/v2/crawl-status/short/python.mdx'
import StartCrawlPythonShort from '/snippets/fr/v2/start-crawl/short/python.mdx'
import CancelCrawlPythonShort from '/snippets/fr/v2/crawl-delete/short/python.mdx'
import MapPythonShort from '/snippets/fr/v2/map/short/python.mdx'
import ExtractPythonShort from '/snippets/v2/extract/short/python.mdx'
import ScrapeAndCrawlExamplePython from '/snippets/fr/v2/scrape-and-crawl/python.mdx'
import CrawlWebSocketPythonBase from '/snippets/fr/v2/crawl-websocket/base/python.mdx'
import AIOPython from '/snippets/fr/v2/async/base/python.mdx'


<div id="installation">
  ## Installation
</div>

Pour installer le SDK Python Firecrawl, utilisez pip :

<InstallationPython />

<div id="usage">
  ## Utilisation
</div>

1. Récupérez une clé API sur [firecrawl.dev](https://firecrawl.dev)
2. Définissez la clé API comme variable d’environnement nommée `FIRECRAWL_API_KEY` ou passez-la en paramètre à la classe `Firecrawl`.

Voici un exemple d’utilisation du SDK :

<ScrapeAndCrawlExamplePython />

<div id="scraping-a-url">
  ### Extraction d’une URL
</div>

Pour extraire une URL unique, utilisez la méthode `scrape`. Elle prend l’URL en paramètre et renvoie le document extrait.

<ScrapePythonShort />

<div id="crawl-a-website">
  ### Explorer un site web
</div>

Pour explorer un site web, utilisez la méthode `crawl`. Elle prend en arguments l’URL de départ et des options facultatives. Ces options permettent de définir des paramètres supplémentaires pour la tâche d’exploration, comme le nombre maximal de pages à parcourir, les domaines autorisés et le format de sortie. Consultez [Pagination](#pagination) pour la pagination automatique/manuelle et les limites.

<CrawlPythonShort />

<div id="sitemap-only-crawl">
  ### Exploration du sitemap uniquement
</div>

Utilisez `sitemap="only"` pour explorer uniquement les URL du sitemap (l’URL de départ est toujours incluse et la découverte de liens HTML est ignorée).

<CrawlSitemapOnlyPython />

<div id="start-a-crawl">
  ### Démarrer un crawl
</div>

<Tip>Vous préférez ne pas bloquer l’exécution ? Consultez la section [Classe Async](#async-class) ci-dessous.</Tip>

Lancez une tâche sans attendre avec `start_crawl`. Elle renvoie un `ID` de tâche que vous pouvez utiliser pour vérifier l’état. Utilisez `crawl` lorsque vous voulez un attenteur qui bloque jusqu’à la fin. Voir [Pagination](#pagination) pour le comportement et les limites de pagination.

<StartCrawlPythonShort />

<div id="checking-crawl-status">
  ### Vérifier l’état d’un crawl
</div>

Pour connaître l’état d’un job de crawl, utilisez la méthode `get_crawl_status`. Elle prend l’ID du job en paramètre et renvoie l’état actuel du crawl.

<CheckCrawlStatusPythonShort />

<div id="cancelling-a-crawl">
  ### Annuler un crawl
</div>

Pour annuler une tâche de crawl, utilisez la méthode `cancel_crawl`. Elle prend l’ID du job renvoyé par `start_crawl` en paramètre et retourne l’état de l’annulation.

<CancelCrawlPythonShort />

<div id="map-a-website">
  ### Cartographier un site web
</div>

Utilisez `map` pour générer une liste d’URL à partir d’un site web. Les options permettent d’adapter le processus de cartographie, par exemple en excluant les sous-domaines ou en s’appuyant sur le sitemap.

<MapPythonShort />

{/* ### Extracting Structured Data from Websites

  To extract structured data from websites, use the `extract` method. It takes the URLs to extract data from, a prompt, and a schema as arguments. The schema is a Pydantic model that defines the structure of the extracted data.

  <ExtractPythonShort /> */}

<div id="crawling-a-website-with-websockets">
  ### Exploration d’un site web avec WebSockets
</div>

Pour explorer un site web avec WebSockets, lancez la tâche avec `start_crawl` et abonnez-vous à l’aide du helper `watcher`. Créez un watcher avec l’ID de la tâche et attachez des gestionnaires (par exemple pour page, completed, failed) avant d’appeler `start()`.

<CrawlWebSocketPythonBase />

<div id="pagination">
  ### Pagination
</div>

Les points de terminaison Firecrawl pour crawl et batch scrape renvoient une URL `next` lorsqu’il reste des données. Le SDK Python effectue par défaut une pagination automatique et agrège tous les documents ; dans ce cas, `next` vaut `None`. Vous pouvez désactiver l’auto‑pagination ou définir des limites pour contrôler le comportement de la pagination.

<div id="paginationconfig">
  #### PaginationConfig
</div>

Utilisez `PaginationConfig` pour contrôler le comportement de la pagination lorsque vous appelez `get_crawl_status` ou `get_batch_scrape_status` :

```python Python
from firecrawl.v2.types import PaginationConfig
```

| Option          | Type   | Par défaut | Description                                                                                                                                     |
| --------------- | ------ | ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| `auto_paginate` | `bool` | `True`     | Lorsque `True`, récupère automatiquement toutes les pages et agrège les résultats. Définissez sur `False` pour récupérer les pages une par une. |
| `max_pages`     | `int`  | `None`     | S&#39;arrête après avoir récupéré ce nombre de pages (s&#39;applique uniquement lorsque `auto_paginate=True`).                                  |
| `max_results`   | `int`  | `None`     | S&#39;arrête après avoir collecté ce nombre de documents (s&#39;applique uniquement lorsque `auto_paginate=True`).                              |
| `max_wait_time` | `int`  | `None`     | S&#39;arrête après ce nombre de secondes (s&#39;applique uniquement lorsque `auto_paginate=True`).                                              |


<div id="manual-pagination-helpers">
  #### Aides à la pagination manuelle
</div>

Lorsque `auto_paginate=False`, la réponse inclut une URL `next` si davantage de données sont disponibles. Utilisez ces méthodes utilitaires pour récupérer les pages suivantes :

- **`get_crawl_status_page(next_url)`** - Récupère la page suivante des résultats de crawl en utilisant l'URL opaque `next` provenant d'une réponse précédente.
- **`get_batch_scrape_status_page(next_url)`** - Récupère la page suivante des résultats de scraping par lot en utilisant l'URL opaque `next` provenant d'une réponse précédente.

Ces méthodes renvoient le même type de réponse que l'appel de statut initial, y compris une nouvelle URL `next` s'il reste d'autres pages.

<div id="crawl">
  #### Crawl
</div>

Utilisez la méthode « waiter » `crawl` pour l’approche la plus simple, ou démarrez un job et paginez manuellement.

<div id="simple-crawl-auto-pagination-default">
  ##### Crawl simple (pagination automatique, par défaut)
</div>

* Voir le flux par défaut dans [Explorer un site web](#crawl-a-website).

<div id="manual-crawl-with-pagination-control">
  ##### Crawl manuel avec contrôle de la pagination
</div>

Démarrez un job, puis récupérez une page à la fois avec `auto_paginate=False`. Utilisez `get_crawl_status_page` pour récupérer les pages suivantes :

```python Python
crawl_job = client.start_crawl("https://example.com", limit=100)

# Fetch first page
status = client.get_crawl_status(
    crawl_job.id,
    pagination_config=PaginationConfig(auto_paginate=False)
)
print("First page:", len(status.data), "docs")

# Récupérer les pages suivantes avec get_crawl_status_page
while status.next:
    status = client.get_crawl_status_page(status.next)
    print("Next page:", len(status.data), "docs")
```


<div id="manual-crawl-with-limits-auto-pagination-early-stop">
  ##### Exploration manuelle avec limites (pagination automatique + arrêt anticipé)
</div>

Laissez la pagination automatique activée, mais arrêtez plus tôt avec `max_pages`, `max_results` ou `max_wait_time` :

```python Python
status = client.get_crawl_status(
    crawl_job.id,
    pagination_config=PaginationConfig(max_pages=2, max_results=50, max_wait_time=15),
)
print("crawl limité :", status.status, "docs :", len(status.data), "suivant :", status.next)
```


<div id="batch-scrape">
  #### Scrape par lots
</div>

Utilisez la méthode de waiter `batch_scrape`, ou lancez un job et paginez manuellement.

<div id="simple-batch-scrape-auto-pagination-default">
  ##### Extraction par lot simple (pagination automatique, par défaut)
</div>

* Voir le parcours par défaut dans [Batch Scrape](/fr/features/batch-scrape).

<div id="manual-batch-scrape-with-pagination-control">
  ##### Scraping par lot manuel avec contrôle de la pagination
</div>

Lancez une tâche, puis récupérez les résultats page par page avec `auto_paginate=False`. Utilisez `get_batch_scrape_status_page` pour récupérer les pages suivantes :

```python Python
batch_job = client.start_batch_scrape(urls)

# Fetch first page
status = client.get_batch_scrape_status(
    batch_job.id,
    pagination_config=PaginationConfig(auto_paginate=False)
)
print("First page:", len(status.data), "docs")

# Récupérer les pages suivantes avec get_batch_scrape_status_page
while status.next:
    status = client.get_batch_scrape_status_page(status.next)
    print("Next page:", len(status.data), "docs")
```


<div id="manual-batch-scrape-with-limits-auto-pagination-early-stop">
  ##### Extraction par lots manuelle avec limites (pagination automatique + arrêt anticipé)
</div>

Laissez la pagination automatique activée, mais arrêtez plus tôt avec `max_pages`, `max_results` ou `max_wait_time` :

```python Python
status = client.get_batch_scrape_status(
    batch_job.id,
    pagination_config=PaginationConfig(max_pages=2, max_results=100, max_wait_time=20),
)
print("lot limité :", status.status, "docs :", len(status.data), "suivant :", status.next)
```


<div id="error-handling">
  ## Gestion des erreurs
</div>

Le SDK gère les erreurs renvoyées par l’API Firecrawl et déclenche des exceptions appropriées. En cas d’erreur lors d’une requête, une exception est levée avec un message d’erreur explicite.

<div id="async-class">
  ## Classe asynchrone
</div>

Pour les opérations asynchrones, utilisez la classe `AsyncFirecrawl`. Ses méthodes sont identiques à celles de `Firecrawl`, mais elles ne bloquent pas le thread principal.

<AIOPython />