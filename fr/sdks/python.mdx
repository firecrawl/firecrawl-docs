---
title: "Python"
description: "Le SDK Python de Firecrawl est une surcouche à l’API Firecrawl qui vous aide à convertir facilement des sites web en Markdown."
icon: "python"
og:title: "SDK Python | Firecrawl"
og:description: "Le SDK Python de Firecrawl est une surcouche à l’API Firecrawl qui vous aide à convertir facilement des sites web en Markdown."
---

import InstallationPython from '/snippets/fr/v2/installation/python.mdx'
import ScrapePythonShort from '/snippets/v2/scrape/short/python.mdx'
import CrawlPythonShort from '/snippets/fr/v2/crawl/short/python.mdx'
import CheckCrawlStatusPythonShort from '/snippets/fr/v2/crawl-status/short/python.mdx'
import StartCrawlPythonShort from '/snippets/v2/start-crawl/short/python.mdx'
import CancelCrawlPythonShort from '/snippets/fr/v2/crawl-delete/short/python.mdx'
import MapPythonShort from '/snippets/v2/map/short/python.mdx'
import ExtractPythonShort from '/snippets/v2/extract/short/python.mdx'
import ScrapeAndCrawlExamplePython from '/snippets/fr/v2/scrape-and-crawl/python.mdx'
import CrawlWebSocketPythonBase from '/snippets/fr/v2/crawl-websocket/base/python.mdx'
import AIOPython from '/snippets/fr/v2/async/base/python.mdx'

<div id="installation">
  ## Installation
</div>

Pour installer le SDK Python Firecrawl, utilisez pip :

<InstallationPython />

<div id="usage">
  ## Utilisation
</div>

1. Récupérez une clé API sur [firecrawl.dev](https://firecrawl.dev)
2. Définissez la clé API comme variable d’environnement nommée `FIRECRAWL_API_KEY` ou passez-la en paramètre à la classe `Firecrawl`.

Voici un exemple d’utilisation du SDK :

<ScrapeAndCrawlExamplePython />

<div id="scraping-a-url">
  ### Extraction d’une URL
</div>

Pour extraire une URL unique, utilisez la méthode `scrape`. Elle prend l’URL en paramètre et renvoie le document extrait.

<ScrapePythonShort />

<div id="crawl-a-website">
  ### Explorer un site web
</div>

Pour explorer un site web, utilisez la méthode `crawl`. Elle prend en arguments l’URL de départ et des options facultatives. Ces options permettent de définir des paramètres supplémentaires pour la tâche d’exploration, comme le nombre maximal de pages à parcourir, les domaines autorisés et le format de sortie. Consultez [Pagination](#pagination) pour la pagination automatique/manuelle et les limites.

<CrawlPythonShort />

<div id="start-a-crawl">
  ### Démarrer un crawl
</div>

<Tip>Vous préférez ne pas bloquer l’exécution ? Consultez la section [Classe Async](#async-class) ci-dessous.</Tip>

Lancez une tâche sans attendre avec `start_crawl`. Elle renvoie un `ID` de tâche que vous pouvez utiliser pour vérifier l’état. Utilisez `crawl` lorsque vous voulez un attenteur qui bloque jusqu’à la fin. Voir [Pagination](#pagination) pour le comportement et les limites de pagination.

<StartCrawlPythonShort />

<div id="checking-crawl-status">
  ### Vérifier l’état d’un crawl
</div>

Pour connaître l’état d’un job de crawl, utilisez la méthode `get_crawl_status`. Elle prend l’ID du job en paramètre et renvoie l’état actuel du crawl.

<CheckCrawlStatusPythonShort />

<div id="cancelling-a-crawl">
  ### Annuler un crawl
</div>

Pour annuler une tâche de crawl, utilisez la méthode `cancel_crawl`. Elle prend l’ID du job renvoyé par `start_crawl` en paramètre et retourne l’état de l’annulation.

<CancelCrawlPythonShort />

<div id="map-a-website">
  ### Cartographier un site web
</div>

Utilisez `map` pour générer une liste d’URL à partir d’un site web. Les options permettent d’adapter le processus de cartographie, par exemple en excluant les sous-domaines ou en s’appuyant sur le sitemap.

<MapPythonShort />

{/* ### Extracting Structured Data from Websites

  To extract structured data from websites, use the `extract` method. It takes the URLs to extract data from, a prompt, and a schema as arguments. The schema is a Pydantic model that defines the structure of the extracted data.

  <ExtractPythonShort /> */}

<div id="crawling-a-website-with-websockets">
  ### Exploration d’un site web avec WebSockets
</div>

Pour explorer un site web avec WebSockets, lancez la tâche avec `start_crawl` et abonnez-vous à l’aide du helper `watcher`. Créez un watcher avec l’ID de la tâche et attachez des gestionnaires (par exemple pour page, completed, failed) avant d’appeler `start()`.

<CrawlWebSocketPythonBase />

<div id="pagination">
  ### Pagination
</div>

Les points de terminaison Firecrawl pour crawl et batch renvoient une URL `next` lorsqu’il reste des données. Le SDK Python effectue par défaut une pagination automatique et agrège tous les documents ; dans ce cas, `next` vaut `None`. Vous pouvez désactiver l’auto‑pagination ou définir des limites.

<div id="crawl">
  #### Crawl
</div>

Utilisez la méthode « waiter » `crawl` pour l’approche la plus simple, ou démarrez un job et paginez manuellement.

<div id="simple-crawl-auto-pagination-default">
  ##### Crawl simple (pagination automatique, par défaut)
</div>

* Voir le flux par défaut dans [Explorer un site web](#crawl-a-website).

<div id="manual-crawl-with-pagination-control-single-page">
  ##### Exploration manuelle avec contrôle de la pagination (page unique)
</div>

* Lancez une tâche, puis récupérez les pages une par une avec `auto_paginate=False`.

```python Python
crawl_job = client.start_crawl("https://example.com", limit=100)

status = client.get_crawl_status(crawl_job.id, pagination_config=PaginationConfig(auto_paginate=False))
print("explorer une seule page :", status.status, "documents :", len(status.data), "suivant :", status.next)
```

<div id="manual-crawl-with-limits-auto-pagination-early-stop">
  ##### Exploration manuelle avec limites (pagination automatique + arrêt anticipé)
</div>

* Laissez la pagination automatique activée, mais arrêtez plus tôt avec `max_pages`, `max_results` ou `max_wait_time`.

```python Python
status = client.get_crawl_status(
    crawl_job.id,
    pagination_config=PaginationConfig(max_pages=2, max_results=50, max_wait_time=15),
)
print("crawl limité :", status.status, "docs :", len(status.data), "suivant :", status.next)
```

<div id="batch-scrape">
  #### Scrape par lots
</div>

Utilisez la méthode de waiter `batch_scrape`, ou lancez un job et paginez manuellement.

<div id="simple-batch-scrape-auto-pagination-default">
  ##### Extraction par lot simple (pagination automatique, par défaut)
</div>

* Voir le parcours par défaut dans [Batch Scrape](/fr/features/batch-scrape).

<div id="manual-batch-scrape-with-pagination-control-single-page">
  ##### Scraping par lots manuel avec contrôle de la pagination (page unique)
</div>

* Démarrez un job, puis récupérez une page à la fois avec `auto_paginate=False`.

```python Python
batch_job = client.start_batch_scrape(urls)
status = client.get_batch_scrape_status(batch_job.id, pagination_config=PaginationConfig(auto_paginate=False))
print("lot, une seule page :", status.status, "docs :", len(status.data), "suivant :", status.next)
```

<div id="manual-batch-scrape-with-limits-auto-pagination-early-stop">
  ##### Extraction par lots manuelle avec limites (pagination automatique + arrêt anticipé)
</div>

* Laissez la pagination automatique activée, mais arrêtez plus tôt avec `max_pages`, `max_results` ou `max_wait_time`.

```python Python
status = client.get_batch_scrape_status(
    batch_job.id,
    pagination_config=PaginationConfig(max_pages=2, max_results=100, max_wait_time=20),
)
print("lot limité :", status.status, "docs :", len(status.data), "suivant :", status.next)
```

<div id="error-handling">
  ## Gestion des erreurs
</div>

Le SDK gère les erreurs renvoyées par l’API Firecrawl et déclenche des exceptions appropriées. En cas d’erreur lors d’une requête, une exception est levée avec un message d’erreur explicite.

<div id="async-class">
  ## Classe asynchrone
</div>

Pour les opérations asynchrones, utilisez la classe `AsyncFirecrawl`. Ses méthodes sont identiques à celles de `Firecrawl`, mais elles ne bloquent pas le thread principal.

<AIOPython />