---
title: 'Skill + CLI'
description: "Firecrawl Skill est un moyen simple pour les agents IA tels que Claude Code, Antigravity et OpenCode d'utiliser Firecrawl via la CLI."
og:title: "CLI | Firecrawl"
og:description: "Firecrawl Skills est un moyen simple pour les agents IA d'utiliser Firecrawl via la CLI. Ils peuvent ainsi obtenir des donn√©es web via une interface plus efficace et mieux optimis√©e pour le contexte."
---

import InstallationCLI from '/snippets/fr/v2/cli/installation/bash.mdx'
import AuthLogin from '/snippets/fr/v2/cli/auth/login.mdx'
import AuthLogout from '/snippets/fr/v2/cli/auth/logout.mdx'
import AuthConfig from '/snippets/fr/v2/cli/auth/config.mdx'
import AuthSelfHosted from '/snippets/fr/v2/cli/auth/self-hosted.mdx'
import ScrapeBasic from '/snippets/fr/v2/cli/scrape/basic.mdx'
import ScrapeFormats from '/snippets/fr/v2/cli/scrape/formats.mdx'
import ScrapeOptions from '/snippets/fr/v2/cli/scrape/options.mdx'
import CrawlBasic from '/snippets/fr/v2/cli/crawl/basic.mdx'
import CrawlStatus from '/snippets/fr/v2/cli/crawl/status.mdx'
import CrawlOptions from '/snippets/fr/v2/cli/crawl/options.mdx'
import MapBasic from '/snippets/fr/v2/cli/map/basic.mdx'
import MapOptions from '/snippets/fr/v2/cli/map/options.mdx'
import SearchBasic from '/snippets/fr/v2/cli/search/basic.mdx'
import SearchOptions from '/snippets/fr/v2/cli/search/options.mdx'
import AgentBasic from '/snippets/fr/v2/cli/agent/basic.mdx'
import AgentOptions from '/snippets/fr/v2/cli/agent/options.mdx'
import BrowserBasic from '/snippets/fr/v2/browser/cli/basic.mdx'
import BrowserOptions from '/snippets/fr/v2/browser/cli/options.mdx'


<div id="installation">
  ## Installation
</div>

Installez la CLI Firecrawl globalement avec npm¬†:

<InstallationCLI />

Si vous utilisez un agent IA comme Claude Code, vous pouvez installer le skill Firecrawl ci-dessous et l‚Äôagent pourra le configurer pour vous.

```bash
npx -y firecrawl-cli@latest init --all --browser
```

* `--all` installe le skill Firecrawl pour tous les agents de codage IA d√©tect√©s
* `--browser` ouvre automatiquement le navigateur pour l‚Äôauthentification Firecrawl

<Note>
  Apr√®s avoir install√© le skill, red√©marrez Claude Code pour qu‚Äôil prenne en compte le nouveau skill.
</Note>


<div id="authentication">
  ## Authentification
</div>

Avant d'utiliser la CLI, vous devez vous authentifier avec votre cl√© API Firecrawl.

<div id="login">
  ### Connexion
</div>

<AuthLogin />

<div id="view-configuration">
  ### Afficher la configuration
</div>

<AuthConfig />

<div id="logout">
  ### D√©connexion
</div>

<AuthLogout />

<div id="self-hosted-local-development">
  ### Auto-h√©berg√© / D√©veloppement local
</div>

Pour les instances Firecrawl auto-h√©berg√©es ou le d√©veloppement local, utilisez l‚Äôoption `--api-url`¬†:

<AuthSelfHosted />

Lorsque vous utilisez une URL d‚ÄôAPI personnalis√©e (toute URL diff√©rente de `https://api.firecrawl.dev`), l‚Äôauthentification par cl√© d‚ÄôAPI est automatiquement contourn√©e, ce qui vous permet d‚Äôutiliser des instances locales sans cl√© d‚ÄôAPI.

<div id="check-status">
  ### V√©rifier l&#39;√©tat
</div>

V√©rifiez l&#39;installation, l&#39;authentification et affichez les limites de d√©bit¬†:

```bash CLI
firecrawl --status
```

Sortie une fois pr√™te¬†:

```
  üî• firecrawl cli v1.1.1

  ‚óè Authenticated via FIRECRAWL_API_KEY
  Concurrency: 0/100 jobs (parallel scrape limit)
  Credits: 500,000 remaining
```

* **Concurrence**¬†: Nombre maximal de t√¢ches en parall√®le. Ex√©cutez des op√©rations parall√®les au plus pr√®s de cette limite, sans la d√©passer.
* **Cr√©dits**¬†: Cr√©dits API restants. Chaque op√©ration de `scrape`/`crawl` consomme des cr√©dits.


<div id="commands">
  ## Commandes
</div>

<div id="scrape">
  ### Scrape
</div>

Analysez une seule URL et extrayez son contenu dans diff√©rents formats.

<Tip>
Utilisez `--only-main-content` pour obtenir un r√©sultat propre sans navigation, pied de page ni publicit√©s. C'est recommand√© pour la plupart des cas d'usage o√π vous souhaitez uniquement l'article ou le contenu principal de la page.
</Tip>

<ScrapeBasic />

<div id="output-formats">
  #### Formats de sortie
</div>

<ScrapeFormats />

<div id="scrape-options">
  #### Options de scraping
</div>

<ScrapeOptions />

**Options disponibles :**

| Option | Forme courte | Description |
|--------|--------------|-------------|
| `--url <url>` | `-u` | URL √† scraper (alternative √† l'argument positionnel) |
| `--format <formats>` | `-f` | formats de sortie (s√©par√©s par des virgules) : `markdown`, `html`, `rawHtml`, `links`, `screenshot`, `json`, `images`, `summary`, `suiviDesModifications`, `attributes`, `branding` |
| `--html` | `-H` | Raccourci pour `--format html` |
| `--only-main-content` | | Extraire uniquement le contenu principal |
| `--wait-for <ms>` | | Temps d'attente en millisecondes pour le rendu JS |
| `--screenshot` | | Prendre une capture d'√©cran |
| `--include-tags <tags>` | | Balises HTML √† inclure (s√©par√©es par des virgules) |
| `--exclude-tags <tags>` | | Balises HTML √† exclure (s√©par√©es par des virgules) |
| `--output <path>` | `-o` | Enregistrer la sortie dans un fichier |
| `--json` | | Forcer la sortie JSON m√™me avec un seul format |
| `--pretty` | | Afficher la sortie JSON de mani√®re lisible |
| `--timing` | | Afficher le temps de la requ√™te et d'autres informations utiles |

---

<div id="search">
  ### Recherche
</div>

Recherchez sur le Web et, si besoin, extrayez le contenu des r√©sultats.

<SearchBasic />

<div id="search-options">
  #### Options de recherche
</div>

<SearchOptions />

**Options disponibles¬†:**

| Option | Description |
|--------|-------------|
| `--limit <number>` | Nombre maximal de r√©sultats (par d√©faut¬†: 5, max¬†: 100) |
| `--sources <sources>` | Sources √† interroger¬†: `web`, `images`, `news` (s√©par√©es par des virgules) |
| `--categories <categories>` | Filtrer par cat√©gorie¬†: `github`, `research`, `pdf` (s√©par√©es par des virgules) |
| `--tbs <value>` | Filtre temporel¬†: `qdr:h` (heure), `qdr:d` (jour), `qdr:w` (semaine), `qdr:m` (mois), `qdr:y` (ann√©e) |
| `--location <location>` | Ciblage g√©ographique (p. ex. "Berlin,Germany") |
| `--country <code>` | Code de pays ISO (par d√©faut¬†: US) |
| `--timeout <ms>` | D√©lai d'expiration en millisecondes (par d√©faut¬†: 60000) |
| `--ignore-invalid-urls` | Exclure les URL invalides pour d'autres endpoints Firecrawl |
| `--scrape` | Scraper les r√©sultats de recherche |
| `--scrape-formats <formats>` | Formats pour le contenu extrait (par d√©faut¬†: markdown) |
| `--only-main-content` | Inclure uniquement le contenu principal lors du scraping (par d√©faut¬†: true) |
| `--json` | R√©sultat au format JSON |
| `--output <path>` | Enregistrer le r√©sultat dans un fichier |
| `--pretty` | Affichage JSON format√© |

---

<div id="map">
  ### Map
</div>

D√©couvrez rapidement toutes les URL d‚Äôun site.

<MapBasic />

<div id="map-options">
  #### Options de la commande Map
</div>

<MapOptions />

**Options disponibles :**

| Option | Description |
|--------|-------------|
| `--url <url>` | URL √† cartographier (alternative √† l‚Äôargument positionnel) |
| `--limit <number>` | Nombre maximal d‚ÄôURL √† d√©couvrir |
| `--search <query>` | Filtrer les URL selon une requ√™te de recherche |
| `--sitemap <mode>` | Gestion du sitemap : `include`, `skip`, `only` |
| `--include-subdomains` | Inclure les sous-domaines |
| `--ignore-query-parameters` | Consid√©rer les URL avec des param√®tres diff√©rents comme identiques |
| `--wait` | Attendre la fin de l‚Äôop√©ration de cartographie |
| `--timeout <seconds>` | D√©lai d‚Äôexpiration en secondes |
| `--json` | R√©sultat au format JSON |
| `--output <path>` | Enregistrer le r√©sultat dans un fichier |
| `--pretty` | Affichage JSON mis en forme |

---

<div id="crawl">
  ### Crawl
</div>

Lancer un crawl sur l'ensemble d'un site web √† partir d'une URL.

<CrawlBasic />

<div id="check-crawl-status">
  #### Consulter l'√©tat du crawl
</div>

<CrawlStatus />

<div id="crawl-options">
  #### Options de crawl
</div>

<CrawlOptions />

**Options disponibles :**

| Option | Description |
|--------|-------------|
| `--url <url>` | URL √† explorer (alternative √† l‚Äôargument positionnel) |
| `--wait` | Attendre la fin du crawl |
| `--progress` | Afficher un indicateur de progression pendant l‚Äôattente |
| `--poll-interval <seconds>` | Intervalle d‚Äôinterrogation (par d√©faut : 5) |
| `--timeout <seconds>` | D√©lai d‚Äôexpiration de l‚Äôattente |
| `--status` | V√©rifier l‚Äô√©tat d‚Äôune t√¢che de crawl existante |
| `--limit <number>` | Nombre maximal de pages √† explorer |
| `--max-depth <number>` | Profondeur maximale du crawl |
| `--include-paths <paths>` | Chemins √† inclure (s√©par√©s par des virgules) |
| `--exclude-paths <paths>` | Chemins √† exclure (s√©par√©s par des virgules) |
| `--sitemap <mode>` | Gestion du sitemap : `include`, `skip`, `only` |
| `--allow-subdomains` | Inclure les sous-domaines |
| `--allow-external-links` | Suivre les liens externes |
| `--crawl-entire-domain` | Explorer l‚Äôensemble du domaine |
| `--ignore-query-parameters` | Consid√©rer les URL avec des param√®tres diff√©rents comme identiques |
| `--delay <ms>` | D√©lai entre les requ√™tes |
| `--max-concurrency <n>` | Nombre maximal de requ√™tes simultan√©es |
| `--output <path>` | Enregistrer le r√©sultat dans un fichier |
| `--pretty` | Afficher la sortie JSON format√©e |

---

<div id="agent">
  ### Agent
</div>

Recherchez et collectez des donn√©es sur le web √† l'aide de prompts en langage naturel.

<AgentBasic />

<div id="agent-options">
  #### Options de l'agent
</div>

<AgentOptions />

**Options disponibles¬†:**

| Option | Description |
|--------|-------------|
| `--urls <urls>` | Liste facultative d‚ÄôURL sur lesquelles concentrer l‚Äôagent (s√©par√©es par des virgules) |
| `--model <model>` | Mod√®le √† utiliser¬†: `spark-1-mini` (par d√©faut, 60¬†% moins cher) ou `spark-1-pro` (meilleure pr√©cision) |
| `--schema <json>` | Sch√©ma JSON pour la sortie structur√©e (cha√Æne JSON int√©gr√©e) |
| `--schema-file <path>` | Chemin vers le fichier de sch√©ma JSON pour la sortie structur√©e |
| `--max-credits <number>` | Nombre maximal de cr√©dits √† utiliser (la t√¢che √©choue si la limite est atteinte) |
| `--status` | Consulter l‚Äô√©tat d‚Äôune t√¢che d‚Äôagent existante |
| `--wait` | Attendre que l‚Äôagent ait termin√© avant de renvoyer les r√©sultats |
| `--poll-interval <seconds>` | Intervalle d‚Äôinterrogation pendant l‚Äôattente (par d√©faut¬†: 5) |
| `--timeout <seconds>` | D√©lai d‚Äôattente maximal (par d√©faut¬†: aucun d√©lai) |
| `--output <path>` | Enregistrer la sortie dans un fichier |
| `--json` | Sortie au format JSON |

---

<div id="browser">
  ### Navigateur
</div>

Lancez des sessions de navigateur dans le cloud et ex√©cutez du code Python, JavaScript ou Bash √† distance. Chaque session ex√©cute une instance compl√®te de Chromium ‚Äî aucune installation de navigateur en local n'est requise. Le code s'ex√©cute c√¥t√© serveur avec un objet `page` [Playwright](https://playwright.dev/) pr√©configur√© et pr√™t √† l'emploi.

<BrowserBasic />

<div id="browser-options">
  #### Options du navigateur
</div>

<BrowserOptions />

**Sous-commandes¬†:**

| Sous-commande | Description |
|------------|-------------|
| `launch-session` | Lance une nouvelle session de navigateur cloud (renvoie l‚ÄôID de session, l‚ÄôURL CDP et l‚ÄôURL de vue en direct) |
| `execute <code>` | Ex√©cute du code Playwright Python/JS ou des commandes bash dans une session |
| `list [status]` | R√©pertorie les sessions de navigateur (filtrage par `active` ou `destroyed`) |
| `close` | Ferme une session de navigateur |

**Options d‚Äôex√©cution¬†:**

| Option | Description |
|--------|-------------|
| `--bash` | Ex√©cute des commandes bash √† distance dans le bac √† sable (par d√©faut). [agent-browser](https://github.com/vercel-labs/agent-browser) (40+ commandes) est pr√©install√© et automatiquement pr√©fix√©. `CDP_URL` est inject√© automatiquement pour qu‚Äôagent-browser se connecte √† votre session sans configuration suppl√©mentaire. Option recommand√©e pour les agents d‚ÄôIA. |
| `--python` | Ex√©cute du code Playwright Python. Un objet Playwright `page` est disponible ‚Äî utilisez `await page.goto()`, `await page.title()`, etc. |
| `--node` | Ex√©cute du code Playwright JavaScript. Le m√™me objet `page` est disponible. |
| `--session <id>` | Cible une session sp√©cifique (par d√©faut¬†: session active) |

**Options de lancement¬†:**

| Option | Description |
|--------|-------------|
| `--ttl <seconds>` | TTL total de la session (par d√©faut¬†: 300, plage¬†: 30‚Äì3600) |
| `--ttl-inactivity <seconds>` | Fermeture automatique apr√®s inactivit√© (plage¬†: 10‚Äì3600) |
| `--stream` | Active le streaming de la vue en direct |

**Options communes¬†:**

| Option | Description |
|--------|-------------|
| `--output <path>` | Enregistre la sortie dans un fichier |
| `--json` | Produit la sortie au format JSON |

---

<div id="credit-usage">
  ### Utilisation des cr√©dits
</div>

Consultez le solde et l&#39;utilisation des cr√©dits de votre √©quipe.

```bash CLI
# Voir l'utilisation des cr√©dits
firecrawl credit-usage

# Sortie en JSON
firecrawl credit-usage --json --pretty
```

***


<div id="version">
  ### Version
</div>

Afficher la version de la CLI.

```bash CLI
firecrawl version
# ou
firecrawl --version
```


<div id="global-options">
  ## Options globales
</div>

Ces options sont disponibles pour toutes les commandes :

| Option | Raccourci | Description |
|--------|-----------|-------------|
| `--status` | | Afficher la version, l‚Äô√©tat d‚Äôauthentification, le niveau de parall√©lisme et les cr√©dits |
| `--api-key <key>` | `-k` | Ignorer la cl√© d‚ÄôAPI enregistr√©e pour cette commande |
| `--api-url <url>` | | Utiliser une URL d‚ÄôAPI personnalis√©e (pour l‚Äôauto-h√©bergement ou le d√©veloppement local) |
| `--help` | `-h` | Afficher l‚Äôaide pour une commande |
| `--version` | `-V` | Afficher la version de la CLI |

<div id="output-handling">
  ## Gestion de la sortie
</div>

La CLI √©crit sur stdout par d√©faut, ce qui facilite l‚Äôutilisation de pipes ou la redirection¬†:

```bash CLI
# Pipe markdown to another command
firecrawl https://example.com | head -50

# Redirect to a file
firecrawl https://example.com > output.md

# Save JSON with pretty formatting
firecrawl https://example.com --format markdown,links --pretty -o data.json
```


<div id="format-behavior">
  ### Comportement des formats
</div>

* **Un seul format**¬†: renvoie le contenu brut (texte markdown, HTML, etc.)
* **Plusieurs formats**¬†: renvoie du JSON avec toutes les donn√©es demand√©es

```bash CLI
# Sortie markdown brute
firecrawl https://example.com --format markdown

# Sortie JSON avec plusieurs formats
firecrawl https://example.com --format markdown,links
```


<div id="examples">
  ## Exemples
</div>

<div id="quick-scrape">
  ### Scraping rapide
</div>

```bash CLI
# R√©cup√©rer le contenu markdown d'une URL (utiliser --only-main-content pour une sortie √©pur√©e)
firecrawl https://docs.firecrawl.dev --only-main-content

# Get HTML content
firecrawl https://example.com --html -o page.html
```


<div id="full-site-crawl">
  ### Exploration compl√®te du site
</div>

```bash CLI
# Crawle un site de docs avec des limites
firecrawl crawl https://docs.example.com --limit 50 --max-depth 2 --wait --progress -o docs.json
```


<div id="site-discovery">
  ### D√©couverte de sites web
</div>

```bash CLI
# Trouver tous les articles de blog
firecrawl map https://example.com --search "blog" -o blog-urls.txt
```


<div id="research-workflow">
  ### Flux de recherche
</div>

```bash CLI
# Rechercher et scraper les r√©sultats pour la recherche
firecrawl search "machine learning best practices 2024" --scrape --scrape-formats markdown --pretty
```


<div id="agent">
  ### Agent
</div>

```bash CLI
# Les URL sont facultatives
firecrawl agent "Find the top 5 AI startups and their funding amounts" --wait

# Se concentrer sur des URL sp√©cifiques
firecrawl agent "Compare pricing plans" --urls https://slack.com/pricing,https://teams.microsoft.com/pricing --wait
```


<div id="browser-automation">
  ### Automatisation du navigateur Web
</div>

```bash CLI
# Launch a session, scrape a page, and close
firecrawl browser launch-session
firecrawl browser execute "open https://news.ycombinator.com"
firecrawl browser execute "snapshot"
firecrawl browser execute "scrape"
firecrawl browser close

# Utiliser agent-browser via le mode bash (par d√©faut ‚Äî recommand√© pour les agents IA)
firecrawl browser launch-session
firecrawl browser execute "open https://example.com"
firecrawl browser execute "snapshot"
# snapshot returns @ref IDs ‚Äî use them to interact
firecrawl browser execute "click @e5"
firecrawl browser execute "fill @e3 'search query'"
firecrawl browser execute "scrape"
# Run --help to see all 40+ commands
firecrawl browser execute --bash "agent-browser --help"
firecrawl browser close
```


<div id="combine-with-other-tools">
  ### Combiner avec d&#39;autres outils
</div>

```bash CLI
# Extract URLs from search results
jq -r '.data.web[].url' search-results.json

# R√©cup√©rer les titres des r√©sultats de recherche
jq -r '.data.web[] | "\(.title): \(.url)"' search-results.json

# Extract links and process with jq
firecrawl https://example.com --format links | jq '.links[].url'

# Count URLs from map
firecrawl map https://example.com | wc -l
```


<div id="telemetry">
  ## T√©l√©m√©trie
</div>

La CLI collecte des donn√©es d‚Äôutilisation anonymes lors de l‚Äôauthentification afin d‚Äôam√©liorer le produit¬†:

* Version de la CLI, syst√®me d‚Äôexploitation et version de Node.js
* D√©tection de l‚Äôoutil de d√©veloppement (par exemple, Cursor, VS Code, Claude Code)

**Aucune donn√©e relative aux commandes, aux URL ou au contenu des fichiers n‚Äôest collect√©e via la CLI.**

Pour d√©sactiver la t√©l√©m√©trie, d√©finissez la variable d‚Äôenvironnement¬†:

```bash CLI
export FIRECRAWL_NO_TELEMETRY=1
```


<div id="open-source">
  ## Open Source
</div>

La CLI et la Skill Firecrawl sont open source et disponibles sur GitHub¬†: [firecrawl/cli](https://github.com/firecrawl/cli)