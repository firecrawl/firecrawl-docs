---
title: 'CLI'
description: "Firecrawl CLI est une interface en ligne de commande pour effectuer du scraping, du crawling, du mapping et des recherches sur le Web directement depuis votre terminal."
icon: 'terminal'
og:title: "CLI | Firecrawl"
og:description: "Firecrawl CLI est une interface en ligne de commande pour effectuer du scraping, du crawling, du mapping et des recherches sur le Web directement depuis votre terminal."
---

import InstallationCLI from '/snippets/fr/v2/cli/installation/bash.mdx'
import AuthLogin from '/snippets/fr/v2/cli/auth/login.mdx'
import AuthLogout from '/snippets/fr/v2/cli/auth/logout.mdx'
import AuthConfig from '/snippets/fr/v2/cli/auth/config.mdx'
import ScrapeBasic from '/snippets/fr/v2/cli/scrape/basic.mdx'
import ScrapeFormats from '/snippets/fr/v2/cli/scrape/formats.mdx'
import ScrapeOptions from '/snippets/fr/v2/cli/scrape/options.mdx'
import CrawlBasic from '/snippets/fr/v2/cli/crawl/basic.mdx'
import CrawlStatus from '/snippets/fr/v2/cli/crawl/status.mdx'
import CrawlOptions from '/snippets/fr/v2/cli/crawl/options.mdx'
import MapBasic from '/snippets/fr/v2/cli/map/basic.mdx'
import MapOptions from '/snippets/fr/v2/cli/map/options.mdx'
import SearchBasic from '/snippets/fr/v2/cli/search/basic.mdx'
import SearchOptions from '/snippets/fr/v2/cli/search/options.mdx'


<div id="installation">
  ## Installation
</div>

Installez la CLI Firecrawl globalement avec npm :

<InstallationCLI />

<div id="authentication">
  ## Authentification
</div>

Avant d'utiliser la CLI, vous devez vous authentifier avec votre clé API Firecrawl.

<div id="login">
  ### Connexion
</div>

<AuthLogin />

<div id="view-configuration">
  ### Afficher la configuration
</div>

<AuthConfig />

<div id="logout">
  ### Déconnexion
</div>

<AuthLogout />

<div id="commands">
  ## Commandes
</div>

<div id="scrape">
  ### Scrape
</div>

Analysez une seule URL et extrayez son contenu dans différents formats.

<ScrapeBasic />

<div id="output-formats">
  #### Formats de sortie
</div>

<ScrapeFormats />

<div id="scrape-options">
  #### Options de scraping
</div>

<ScrapeOptions />

**Options disponibles :**

| Option | Forme courte | Description |
|--------|--------------|-------------|
| `--url <url>` | `-u` | URL à scraper (alternative à l'argument positionnel) |
| `--format <formats>` | `-f` | formats de sortie (séparés par des virgules) : `markdown`, `html`, `rawHtml`, `links`, `images`, `screenshot`, `json` |
| `--html` | `-H` | Raccourci pour `--format html` |
| `--only-main-content` | | Extraire uniquement le contenu principal |
| `--wait-for <ms>` | | Temps d'attente en millisecondes pour le rendu JS |
| `--screenshot` | | Prendre une capture d'écran |
| `--include-tags <tags>` | | Balises HTML à inclure (séparées par des virgules) |
| `--exclude-tags <tags>` | | Balises HTML à exclure (séparées par des virgules) |
| `--output <path>` | `-o` | Enregistrer la sortie dans un fichier |
| `--pretty` | | Afficher la sortie JSON de manière lisible |

---

<div id="crawl">
  ### Crawl
</div>

Lancer un crawl sur l'ensemble d'un site web à partir d'une URL.

<CrawlBasic />

<div id="check-crawl-status">
  #### Consulter l'état du crawl
</div>

<CrawlStatus />

<div id="crawl-options">
  #### Options de crawl
</div>

<CrawlOptions />

**Options disponibles :**

| Option | Description |
|--------|-------------|
| `--url <url>` | URL à explorer (alternative à l’argument positionnel) |
| `--wait` | Attendre la fin du crawl |
| `--progress` | Afficher un indicateur de progression pendant l’attente |
| `--poll-interval <seconds>` | Intervalle d’interrogation (par défaut : 5) |
| `--timeout <seconds>` | Délai d’expiration de l’attente |
| `--status` | Vérifier l’état d’une tâche de crawl existante |
| `--limit <number>` | Nombre maximal de pages à explorer |
| `--max-depth <number>` | Profondeur maximale du crawl |
| `--include-paths <paths>` | Chemins à inclure (séparés par des virgules) |
| `--exclude-paths <paths>` | Chemins à exclure (séparés par des virgules) |
| `--allow-subdomains` | Inclure les sous-domaines |
| `--allow-external-links` | Suivre les liens externes |
| `--output <path>` | Enregistrer le résultat dans un fichier |
| `--pretty` | Afficher la sortie JSON formatée |

---

<div id="map">
  ### Map
</div>

Découvrez rapidement toutes les URL d’un site.

<MapBasic />

<div id="map-options">
  #### Options de la commande Map
</div>

<MapOptions />

**Options disponibles :**

| Option | Description |
|--------|-------------|
| `--url <url>` | URL à cartographier (alternative à l’argument positionnel) |
| `--limit <number>` | Nombre maximal d’URL à découvrir |
| `--search <query>` | Filtrer les URL selon une requête de recherche |
| `--sitemap <mode>` | Gestion du sitemap : `include`, `skip`, `only` |
| `--include-subdomains` | Inclure les sous-domaines |
| `--ignore-query-parameters` | Considérer les URL avec des paramètres différents comme identiques |
| `--json` | Résultat au format JSON |
| `--output <path>` | Enregistrer le résultat dans un fichier |
| `--pretty` | Affichage JSON mis en forme |

<div id="search">
  ### Recherche
</div>

Recherchez sur le Web et, si besoin, extrayez le contenu des résultats.

<SearchBasic />

<div id="search-options">
  #### Options de recherche
</div>

<SearchOptions />

**Options disponibles :**

| Option | Description |
|--------|-------------|
| `--limit <number>` | Nombre maximal de résultats (par défaut : 5, max : 100) |
| `--sources <sources>` | Sources à interroger : `web`, `images`, `news` (séparées par des virgules) |
| `--categories <categories>` | Filtrer par catégorie : `github`, `research`, `pdf` (séparées par des virgules) |
| `--tbs <value>` | Filtre temporel : `qdr:h` (heure), `qdr:d` (jour), `qdr:w` (semaine), `qdr:m` (mois), `qdr:y` (année) |
| `--location <location>` | Ciblage géographique (p. ex. "Berlin,Germany") |
| `--country <code>` | Code de pays ISO (par défaut : US) |
| `--scrape` | Scraper les résultats de recherche |
| `--scrape-formats <formats>` | Formats pour le contenu extrait (par défaut : markdown) |
| `--only-main-content` | Inclure uniquement le contenu principal lors du scraping |
| `--json` | Résultat au format JSON |
| `--output <path>` | Enregistrer le résultat dans un fichier |
| `--pretty` | Affichage JSON formaté |

---

<div id="credit-usage">
  ### Utilisation des crédits
</div>

Consultez le solde et l&#39;utilisation des crédits de votre équipe.

```bash CLI
# Voir l'utilisation des crédits
firecrawl credit-usage

# Sortie en JSON
firecrawl credit-usage --json --pretty
```

***


<div id="version">
  ### Version
</div>

Afficher la version de la CLI.

```bash CLI
firecrawl version
# ou
firecrawl --version
```


<div id="global-options">
  ## Options globales
</div>

Ces options sont disponibles pour toutes les commandes :

| Option | Raccourci | Description |
|--------|-----------|-------------|
| `--api-key <key>` | `-k` | Ignorer la clé d’API enregistrée pour cette commande |
| `--help` | `-h` | Afficher l’aide pour une commande |
| `--version` | `-V` | Afficher la version de la CLI |

<div id="output-handling">
  ## Gestion de la sortie
</div>

La CLI écrit sur stdout par défaut, ce qui facilite l’utilisation de pipes ou la redirection :

```bash CLI
# Pipe markdown to another command
firecrawl https://example.com | head -50

# Redirect to a file
firecrawl https://example.com > output.md

# Save JSON with pretty formatting
firecrawl https://example.com --format markdown,links --pretty -o data.json
```


<div id="examples">
  ## Exemples
</div>

<div id="quick-scrape">
  ### Scraping rapide
</div>

```bash CLI
# Récupérer le contenu markdown d'une URL
firecrawl https://docs.firecrawl.dev

# Get HTML content
firecrawl https://example.com --html -o page.html
```


<div id="full-site-crawl">
  ### Exploration complète du site
</div>

```bash CLI
# Crawle un site de docs avec des limites
firecrawl crawl https://docs.example.com --limit 50 --max-depth 2 --wait --progress -o docs.json
```


<div id="site-discovery">
  ### Découverte de sites web
</div>

```bash CLI
# Trouver tous les articles de blog
firecrawl map https://example.com --search "blog" -o blog-urls.txt
```


<div id="research-workflow">
  ### Flux de recherche
</div>

```bash CLI
# Rechercher et scraper les résultats pour la recherche
firecrawl search "machine learning best practices 2024" --scrape --scrape-formats markdown --pretty
```
