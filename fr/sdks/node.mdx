---
title: "Node"
description: "Le SDK Node de Firecrawl est un wrapper de l’API Firecrawl qui vous aide à convertir facilement des sites web en markdown."
icon: "node"
og:title: "SDK Node | Firecrawl"
og:description: "Le SDK Node de Firecrawl est un wrapper de l’API Firecrawl qui vous aide à convertir facilement des sites web en markdown."
---

import InstallationNode from '/snippets/fr/v2/installation/js.mdx'
import ScrapeAndCrawlExampleNode from '/snippets/fr/v2/scrape-and-crawl/js.mdx'
import ScrapeNodeShort from '/snippets/v2/scrape/short/js.mdx'
import CrawlNodeShort from '/snippets/fr/v2/crawl/short/js.mdx'
import StartCrawlNodeShort from '/snippets/v2/start-crawl/short/js.mdx'
import CheckCrawlStatusNodeShort from '/snippets/fr/v2/crawl-status/short/js.mdx'
import CancelCrawlNodeShort from '/snippets/fr/v2/crawl-delete/short/js.mdx'
import MapNodeShort from '/snippets/v2/map/short/js.mdx'
import ExtractNodeShort from '/snippets/v2/extract/short/js.mdx'
import CrawlWebSocketNodeBase from '/snippets/fr/v2/crawl-websocket/base/js.mdx'

<div id="installation">
  ## Installation
</div>

Pour installer le SDK Firecrawl pour Node, vous pouvez utiliser npm :

<InstallationNode />

<div id="usage">
  ## Utilisation
</div>

1. Récupérez une clé d’API sur [firecrawl.dev](https://firecrawl.dev)
2. Définissez la clé d’API comme variable d’environnement nommée `FIRECRAWL_API_KEY`, ou transmettez-la en paramètre à la classe `FirecrawlApp`.

Voici un exemple d’utilisation du SDK avec gestion des erreurs :

<ScrapeAndCrawlExampleNode />

<div id="scraping-a-url">
  ### Scraper une URL
</div>

Pour récupérer le contenu d’une URL avec gestion des erreurs, utilisez la méthode `scrapeUrl`. Elle prend l’URL en paramètre et renvoie les données récupérées sous forme de dictionnaire.

<ScrapeNodeShort />

<div id="crawling-a-website">
  ### Explorer un site web
</div>

Pour explorer un site web avec gestion des erreurs, utilisez la méthode `crawlUrl`. Elle prend en arguments l’URL de départ et des paramètres optionnels. L’argument `params` vous permet de définir des options supplémentaires pour la tâche d’exploration, comme le nombre maximal de pages à explorer, les domaines autorisés et le format de sortie. Voir [Pagination](#pagination) pour la pagination automatique/manuelle et la limitation.

<CrawlNodeShort />

<div id="start-a-crawl">
  ### Démarrer un crawl
</div>

Lancez une tâche sans attendre avec `startCrawl`. Elle renvoie un `ID` de tâche que vous pouvez utiliser pour vérifier l’état. Utilisez `crawl` si vous voulez un « waiter » qui bloque jusqu’à la fin. Voir [Pagination](#pagination) pour le comportement de pagination et les limites.

<StartCrawlNodeShort />

<div id="checking-crawl-status">
  ### Vérifier l’état du crawl
</div>

Pour consulter l’état d’un job de crawl avec gestion des erreurs, utilisez la méthode `checkCrawlStatus`. Elle prend l’ID en paramètre et renvoie l’état actuel du job de crawl.

<CheckCrawlStatusNodeShort />

<div id="cancelling-a-crawl">
  ### Annuler un crawl
</div>

Pour annuler une tâche de crawl, utilisez la méthode `cancelCrawl`. Elle prend l’ID de la tâche lancée par `startCrawl` en paramètre et renvoie l’état de l’annulation.

<CancelCrawlNodeShort />

<div id="mapping-a-website">
  ### Cartographier un site web
</div>

Pour cartographier un site web avec gestion des erreurs, utilisez la méthode `mapUrl`. Elle prend l’URL de départ en paramètre et renvoie les données cartographiées sous forme de dictionnaire.

<MapNodeShort />

{/* ### Extraire des données structurées à partir de sites web

  Pour extraire des données structurées à partir de sites web avec gestion des erreurs, utilisez la méthode `extractUrl`. Elle prend l’URL de départ en paramètre et renvoie les données extraites sous forme de dictionnaire.

  <ExtractNodeShort /> */}

<div id="crawling-a-website-with-websockets">
  ### Explorer un site web avec WebSockets
</div>

Pour explorer un site web avec WebSockets, utilisez la méthode `crawlUrlAndWatch`. Elle prend en arguments l’URL de départ et des paramètres optionnels. L’argument `params` permet de définir des options supplémentaires pour le job d’exploration, comme le nombre maximal de pages à explorer, les domaines autorisés et le format de sortie.

<CrawlWebSocketNodeBase />

<div id="pagination">
  ### Pagination
</div>

Les points de terminaison Firecrawl pour crawl et batch renvoient une URL `next` lorsqu’il reste des données. Le SDK Node effectue, par défaut, une pagination automatique et agrège tous les documents ; dans ce cas, `next` vaut `null`. Vous pouvez désactiver la pagination automatique ou définir des limites.

<div id="crawl">
  #### Crawl
</div>

Utilisez la méthode d’attente `crawl` pour la solution la plus simple, ou démarrez un job et paginez manuellement.

<div id="simple-crawl-auto-pagination-default">
  ##### Exploration simple (pagination automatique, par défaut)
</div>

* Voir le flux par défaut dans [Exploration d’un site web](#crawling-a-website).

<div id="manual-crawl-with-pagination-control-single-page">
  ##### Crawl manuel avec contrôle de la pagination (page unique)
</div>

* Lancez un job, puis récupérez les pages une par une avec `autoPaginate: false`.

```js Node
const crawlStart = await firecrawl.startCrawl('https://docs.firecrawl.dev', { limit: 5 });
const crawlJobId = crawlStart.id;

const crawlSingle = await firecrawl.getCrawlStatus(crawlJobId, { autoPaginate: false });
console.log('exploration d’une seule page :', crawlSingle.status, 'docs :', crawlSingle.data.length, 'suivant :', crawlSingle.next);
```

<div id="manual-crawl-with-limits-auto-pagination-early-stop">
  ##### Exploration manuelle avec limites (pagination automatique + arrêt anticipé)
</div>

* Conservez la pagination automatique activée, mais arrêtez plus tôt avec `maxPages`, `maxResults` ou `maxWaitTime`.

```js Node
const crawlLimited = await firecrawl.getCrawlStatus(crawlJobId, {
  autoPaginate: true,
  maxPages: 2,
  maxResults: 50,
  maxWaitTime: 15,
});
console.log('exploration limitée :', crawlLimited.status, 'docs :', crawlLimited.data.length, 'suivant :', crawlLimited.next);
```

<div id="batch-scrape">
  #### Scrape par lots
</div>

Utilisez la méthode du waiter `batchScrape`, ou lancez un job et paginez manuellement.

<div id="simple-batch-scrape-auto-pagination-default">
  ##### Collecte par lots simple (pagination automatique, par défaut)
</div>

* Voir le flux par défaut dans [Batch Scrape](/fr/features/batch-scrape).

<div id="manual-batch-scrape-with-pagination-control-single-page">
  ##### Scraping par lots manuel avec contrôle de la pagination (page unique)
</div>

* Lancez un job, puis récupérez les pages une par une avec `autoPaginate: false`.

```js Node
const batchStart = await firecrawl.startBatchScrape([
  'https://docs.firecrawl.dev',
  'https://firecrawl.dev',
], { options: { formats: ['markdown'] } });
const batchJobId = batchStart.id;

const batchSingle = await firecrawl.getBatchScrapeStatus(batchJobId, { autoPaginate: false });
console.log('lot page unique :', batchSingle.status, 'docs :', batchSingle.data.length, 'suivant :', batchSingle.next);
```

<div id="manual-batch-scrape-with-limits-auto-pagination-early-stop">
  ##### Scrape manuel par lots avec limites (pagination automatique + arrêt anticipé)
</div>

* Laissez la pagination automatique activée, mais arrêtez plus tôt avec `maxPages`, `maxResults` ou `maxWaitTime`.

```js Node
const batchLimited = await firecrawl.getBatchScrapeStatus(batchJobId, {
  autoPaginate: true,
  maxPages: 2,
  maxResults: 100,
  maxWaitTime: 20,
});
console.log('lot limité :', batchLimited.status, 'docs :', batchLimited.data.length, 'suivant :', batchLimited.next);
```

<div id="error-handling">
  ## Gestion des erreurs
</div>

Le SDK gère les erreurs renvoyées par l’API Firecrawl et déclenche les exceptions appropriées. Si une erreur survient lors d’une requête, une exception est levée avec un message d’erreur explicite. Les exemples ci-dessus illustrent la gestion de ces erreurs au moyen de blocs `try/catch`.