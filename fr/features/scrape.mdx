---
title: "Scrape"
description: "Transformez n'importe quelle URL en données propres"
og:title: "Scrape | Firecrawl"
og:description: "Transformez n'importe quelle URL en données propres"
---

import InstallationPython from "/snippets/fr/v2/installation/python.mdx";
import InstallationNode from "/snippets/fr/v2/installation/js.mdx";
import InstallationCLI from "/snippets/fr/v2/installation/cli.mdx";
import ScrapePython from "/snippets/fr/v2/scrape/base/python.mdx";
import ScrapeNode from "/snippets/fr/v2/scrape/base/js.mdx";
import ScrapeCURL from "/snippets/fr/v2/scrape/base/curl.mdx";
import ScrapeCLI from "/snippets/fr/v2/scrape/base/cli.mdx";
import ScrapeResponse from "/snippets/fr/v2/scrape/base/output.mdx";
import ExtractCURL from "/snippets/fr/v2/scrape/json/base/curl.mdx";
import ExtractPython from "/snippets/fr/v2/scrape/json/base/python.mdx";
import ExtractNode from "/snippets/fr/v2/scrape/json/base/js.mdx";
import ExtractOutput from "/snippets/fr/v2/scrape/json/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/fr/v2/scrape/json/no-schema/curl.mdx";
import ExtractNoSchemaPython from "/snippets/fr/v2/scrape/json/no-schema/python.mdx";
import ExtractNoSchemaNode from "/snippets/fr/v2/scrape/json/no-schema/js.mdx";
import ExtractNoSchemaOutput from "/snippets/fr/v2/scrape/json/no-schema/output.mdx";
import ScrapeActionsPython from "/snippets/fr/v2/scrape/actions/python.mdx";
import ScrapeActionsNode from "/snippets/fr/v2/scrape/actions/js.mdx";
import ScrapeActionsCURL from "/snippets/fr/v2/scrape/actions/curl.mdx";
import ScrapeActionsOutput from "/snippets/fr/v2/scrape/actions/output.mdx";
import BatchScrapePython from "/snippets/fr/v2/batch-scrape/short/python.mdx";
import BatchScrapeNode from "/snippets/fr/v2/batch-scrape/short/js.mdx";
import BatchScrapeCURL from "/snippets/fr/v2/batch-scrape/short/curl.mdx";
import BatchScrapeOutput from "/snippets/fr/v2/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/fr/v2/batch-scrape/base/async-output.mdx";
import ScrapeLocationPython from "/snippets/fr/v2/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/fr/v2/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/fr/v2/scrape/location/curl.mdx";
import ScrapeBrandingPython from "/snippets/fr/v2/scrape/branding/base/python.mdx";
import ScrapeBrandingNode from "/snippets/fr/v2/scrape/branding/base/js.mdx";
import ScrapeBrandingCURL from "/snippets/fr/v2/scrape/branding/base/curl.mdx";
import ScrapeBrandingOutput from "/snippets/fr/v2/scrape/branding/base/output.mdx";
import ScrapeBrandingCombinedPython from "/snippets/fr/v2/scrape/branding/combined/python.mdx";
import ScrapeBrandingCombinedNode from "/snippets/fr/v2/scrape/branding/combined/js.mdx";
import ScrapeBrandingCombinedCURL from "/snippets/fr/v2/scrape/branding/combined/curl.mdx";

Firecrawl convertit les pages web en Markdown, idéal pour les applications LLM.

* Il gère les complexités : proxys, mise en cache, limites de débit, contenu bloqué par JavaScript
* Prend en charge le contenu dynamique : sites dynamiques, sites rendus par JavaScript, PDF, images
* Produit un Markdown propre, des données structurées, des captures d’écran ou du HTML.

Pour plus de détails, consultez la [référence de l’API Scrape Endpoint](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="scraping-a-url-with-firecrawl">
  ## Extraire le contenu d’une URL avec Firecrawl
</div>

<div id="scrape-endpoint">
  ### point de terminaison /scrape
</div>

Utilisé pour scraper une URL et en récupérer le contenu.

<div id="installation">
  ### Installation
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationCLI />
</CodeGroup>

<div id="usage">
  ### Utilisation
</div>

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeCURL />

  <ScrapeCLI />
</CodeGroup>

Pour plus d’informations sur les paramètres, consultez la [référence de l’API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="response">
  ### Réponse
</div>

Les SDK renvoient directement l’objet de données. cURL renvoie la charge utile exactement comme ci-dessous.

<ScrapeResponse />

<div id="scrape-formats">
  ## Formats de scraping
</div>

Vous pouvez désormais choisir les formats de votre sortie. Vous pouvez spécifier plusieurs formats de sortie. Les formats pris en charge sont :

* Markdown (`markdown`)
* Résumé (`summary`)
* HTML (`html`)
* HTML brut (`rawHtml`) (sans modification)
* Capture d’écran (`screenshot`, avec des options comme `fullPage`, `quality`, `viewport`)
* Liens (`links`)
* JSON (`json`) - sortie structurée
* Images (`images`) - extrait toutes les URL d’images de la page
* Branding (`branding`) - extrait l’identité de marque et le design system

Les clés de sortie correspondront au format que vous choisissez.

<div id="extract-structured-data">
  ## Extraire des données structurées
</div>

<div id="scrape-with-json-endpoint">
  ### Point de terminaison /scrape (avec json)
</div>

Permet d’extraire des données structurées à partir de pages scrappées.

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

Résultat :

<ExtractOutput />

<div id="extracting-without-schema">
  ### Extraction sans schéma
</div>

Vous pouvez désormais extraire sans schéma en passant simplement un `prompt` au point de terminaison. Le LLM choisit la structure des données.

<CodeGroup>
  <ExtractNoSchemaPython />

  <ExtractNoSchemaNode />

  <ExtractNoSchemaCURL />
</CodeGroup>

Résultat :

<ExtractNoSchemaOutput />

<div id="json-format-options">
  ### Options du format JSON
</div>

Lorsque vous utilisez le format `json`, passez un objet dans `formats` avec les paramètres suivants :

* `schema` : schéma JSON pour la sortie structurée.
* `prompt` : invite facultative pour orienter l’extraction lorsqu’un schéma est présent ou lorsque vous souhaitez un guidage léger.

<div id="extract-brand-identity">
  ## Extraire l’identité de marque
</div>

<div id="scrape-with-branding-endpoint">
  ### endpoint /scrape (avec branding)
</div>

Le format « branding » extrait des informations complètes sur l’identité de marque à partir d’une page web, notamment les couleurs, les polices, la typographie, les espacements, les composants d’interface, et bien plus encore. C’est utile pour l’analyse de design systems, la veille de marque, ou la création d’outils qui doivent comprendre l’identité visuelle d’un site web.

<CodeGroup>
  <ScrapeBrandingPython />

  <ScrapeBrandingNode />

  <ScrapeBrandingCURL />
</CodeGroup>

### Réponse

Le format d’habillage de marque retourne un objet `BrandingProfile` complet avec la structure suivante :

<ScrapeBrandingOutput />

<div id="branding-profile-structure">
  ### Structure du profil de marque
</div>

L&#39;objet `branding` contient les propriétés suivantes :

* `colorScheme` : Schéma de couleurs détecté (« light » ou « dark »)
* `logo` : URL du logo principal
* `colors` : Objet contenant les couleurs de la marque :
  * `primary`, `secondary`, `accent` : Couleurs principales de la marque
  * `background`, `textPrimary`, `textSecondary` : Couleurs de l’interface
  * `link`, `success`, `warning`, `error` : Couleurs sémantiques
* `fonts` : Tableau des familles de polices utilisées sur la page
* `typography` : Informations détaillées sur la typographie :
  * `fontFamilies` : Familles de polices principales, titres et code
  * `fontSizes` : Définitions des tailles pour les titres et le corps du texte
  * `fontWeights` : Définitions des graisses (light, regular, medium, bold)
  * `lineHeights` : Valeurs d’interlignage pour différents types de texte
* `spacing` : Informations sur les espacements et la mise en page :
  * `baseUnit` : Unité d’espacement de base en pixels
  * `borderRadius` : Rayon de bordure par défaut
  * `padding`, `margins` : Valeurs d’espacement
* `components` : Styles des composants d’interface :
  * `buttonPrimary`, `buttonSecondary` : Styles des boutons
  * `input` : Styles des champs de saisie
* `icons` : Informations sur le style des icônes
* `images` : Images de la marque (logo, favicon, og:image)
* `animations` : Paramètres d’animation et de transition
* `layout` : Configuration de la mise en page (grille, hauteurs d’en-tête/pied de page)
* `personality` : Traits de personnalité de la marque (ton, énergie, public cible)

<div id="combining-with-other-formats">
  ### Combiner avec d’autres formats
</div>

Vous pouvez combiner le format « branding » avec d’autres formats pour obtenir des données de page complètes :

<CodeGroup>
  <ScrapeBrandingCombinedPython />

  <ScrapeBrandingCombinedNode />

  <ScrapeBrandingCombinedCURL />
</CodeGroup>

<div id="interacting-with-the-page-with-actions">
  ## Interagir avec la page à l’aide des actions
</div>

Firecrawl vous permet d’effectuer diverses actions sur une page web avant d’en extraire le contenu. C’est particulièrement utile pour interagir avec du contenu dynamique, naviguer entre les pages ou accéder à du contenu nécessitant une interaction de l’utilisateur.

Voici un exemple d’utilisation des actions pour accéder à google.com, rechercher Firecrawl, cliquer sur le premier résultat et prendre une capture d’écran.

Il est recommandé d’utiliser presque systématiquement l’action `wait` avant et après l’exécution d’autres actions afin de laisser suffisamment de temps au chargement de la page.

<div id="example">
  ### Exemple
</div>

<CodeGroup>
  <ScrapeActionsPython />

  <ScrapeActionsNode />

  <ScrapeActionsCURL />
</CodeGroup>

<div id="output">
  ### Résultat
</div>

<CodeGroup>
  <ScrapeActionsOutput />
</CodeGroup>

Pour en savoir plus sur les paramètres des actions, consultez la [référence de l’API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="location-and-language">
  ## Localisation et langue
</div>

Indiquez le pays et les langues souhaitées pour obtenir un contenu pertinent selon votre zone cible et vos préférences linguistiques.

<div id="how-it-works">
  ### Fonctionnement
</div>

Lorsque vous renseignez les paramètres de localisation, Firecrawl utilisera, si possible, un proxy adapté et adoptera la langue et le fuseau horaire correspondants. Par défaut, la localisation est définie sur « US » si aucun paramètre n’est fourni.

<div id="usage">
  ### Utilisation
</div>

Pour utiliser les paramètres de localisation et de langue, incluez l’objet `location` dans le corps de votre requête avec les propriétés suivantes :

* `country` : code pays ISO 3166-1 alpha-2 (p. ex. « US », « AU », « DE », « JP »). Par défaut : « US ».
* `languages` : un tableau des langues et paramètres régionaux préférés pour la requête, par ordre de priorité. Par défaut : la langue de la localisation spécifiée.

<CodeGroup>
  <ScrapeLocationPython />

  <ScrapeLocationNode />

  <ScrapeLocationCURL />
</CodeGroup>

Pour plus de détails sur les localisations prises en charge, consultez la [documentation sur les proxys](/fr/features/proxies).

<div id="caching-and-maxage">
  ## Mise en cache et maxAge
</div>

Pour accélérer les requêtes, Firecrawl renvoie par défaut les résultats depuis le cache lorsqu’une copie récente est disponible.

* **Fenêtre de fraîcheur par défaut** : `maxAge = 172800000` ms (2 jours). Si une page en cache est plus récente que ce délai, elle est renvoyée instantanément ; sinon, la page est explorée puis mise en cache.
* **Performances** : cela peut accélérer les scrapes jusqu’à 5x lorsque les données n’ont pas besoin d’être ultra fraîches.
* **Toujours récupérer du contenu frais** : définissez `maxAge` à `0`.
* **Éviter le stockage** : définissez `storeInCache` sur `false` si vous ne voulez pas que Firecrawl mette en cache/stocke les résultats pour cette requête.
* **Suivi des modifications** : les requêtes qui incluent `changeTracking` contournent le cache, donc `maxAge` est ignoré.

Exemple (forcer du contenu frais) :

<CodeGroup>
  ```python Python
  from firecrawl import Firecrawl
  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  doc = firecrawl.scrape(url='https://example.com', maxAge=0, formats=['markdown'])
  print(doc)
  ```

  ```js Node
  import Firecrawl from '@mendable/firecrawl-js';

  const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

  const doc = await firecrawl.scrape('https://example.com', { maxAge: 0, formats: ['markdown'] });
  console.log(doc);
  ```

  ```bash cURL
  curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
    -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "url": "https://example.com",
      "maxAge": 0,
      "formats": ["markdown"]
    }'
  ```
</CodeGroup>

Exemple (utiliser une fenêtre de cache de 10 minutes) :

<CodeGroup>
  ```python Python
  from firecrawl import Firecrawl
  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  doc = firecrawl.scrape(url='https://example.com', maxAge=600000, formats=['markdown', 'html'])
  print(doc)
  ```

  ```js Node

  const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

  const doc = await firecrawl.scrape('https://example.com', { maxAge: 600000, formats: ['markdown', 'html'] });
  console.log(doc);
  ```

  ```bash cURL
  curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
    -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "url": "https://example.com",
      "maxAge": 600000,
      "formats": ["markdown", "html"]
    }'
  ```
</CodeGroup>

<div id="batch-scraping-multiple-urls">
  ## Extraction par lots de plusieurs URL
</div>

Vous pouvez désormais lancer l’extraction par lots de plusieurs URL simultanément. La fonction prend en arguments les URL de départ ainsi que des paramètres optionnels. L’argument params vous permet de définir des options supplémentaires pour la tâche d’extraction par lots, comme les formats de sortie.

<div id="how-it-works">
  ### Fonctionnement
</div>

C’est très proche du fonctionnement du point de terminaison `/crawl`. Il lance un job de scraping par lot et renvoie un ID de job pour en vérifier l’état.

Le SDK propose deux méthodes, synchrone et asynchrone. La méthode synchrone renvoie les résultats du job de scraping par lot, tandis que la méthode asynchrone renvoie un ID de job que vous pouvez utiliser pour en suivre l’état.

<div id="usage">
  ### Utilisation
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Réponse
</div>

Si vous utilisez les méthodes synchrones des SDK, elles renverront les résultats du travail de scraping par lot. Sinon, elles renverront un identifiant de travail que vous pourrez utiliser pour vérifier l’état du scraping par lot.

<div id="synchronous">
  #### Synchrone
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### Asynchrone
</div>

Vous pouvez ensuite utiliser l’ID de la tâche pour vérifier l’état du batch scrape en appelant le point de terminaison `/batch/scrape/{id}`. Ce point de terminaison est destiné à être utilisé pendant l’exécution de la tâche ou juste après son achèvement, **car les tâches de batch scrape expirent après 24 heures**.

<BatchScrapeAsyncOutput />

<div id="enhanced-mode">
  ## Mode amélioré
</div>

Pour les sites web complexes, Firecrawl propose un mode amélioré qui offre de meilleurs taux de réussite tout en préservant la confidentialité.

En savoir plus sur le [Mode amélioré](/fr/features/enhanced-mode).