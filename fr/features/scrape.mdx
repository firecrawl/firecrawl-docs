---
title: "Scraper"
description: "Transformez n'importe quelle URL en données propres"
og:title: "Scraper | Firecrawl"
og:description: "Transformez n'importe quelle URL en données propres"
---

import InstallationPython from "/snippets/fr/v2/installation/python.mdx";
import InstallationNode from "/snippets/fr/v2/installation/js.mdx";
import ScrapePython from "/snippets/fr/v2/scrape/base/python.mdx";
import ScrapeNode from "/snippets/fr/v2/scrape/base/js.mdx";
import ScrapeCURL from "/snippets/fr/v2/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/fr/v2/scrape/base/output.mdx";
import ExtractCURL from "/snippets/fr/v2/scrape/json/base/curl.mdx";
import ExtractPython from "/snippets/fr/v2/scrape/json/base/python.mdx";
import ExtractNode from "/snippets/fr/v2/scrape/json/base/js.mdx";
import ExtractOutput from "/snippets/fr/v2/scrape/json/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/fr/v2/scrape/json/no-schema/curl.mdx";
import ExtractNoSchemaPython from "/snippets/fr/v2/scrape/json/no-schema/python.mdx";
import ExtractNoSchemaNode from "/snippets/fr/v2/scrape/json/no-schema/js.mdx";
import ExtractNoSchemaOutput from "/snippets/fr/v2/scrape/json/no-schema/output.mdx";
import ScrapeActionsPython from "/snippets/fr/v2/scrape/actions/python.mdx";
import ScrapeActionsNode from "/snippets/fr/v2/scrape/actions/js.mdx";
import ScrapeActionsCURL from "/snippets/fr/v2/scrape/actions/curl.mdx";
import ScrapeActionsOutput from "/snippets/fr/v2/scrape/actions/output.mdx";
import BatchScrapePython from "/snippets/fr/v2/batch-scrape/short/python.mdx";
import BatchScrapeNode from "/snippets/fr/v2/batch-scrape/short/js.mdx";
import BatchScrapeCURL from "/snippets/fr/v2/batch-scrape/short/curl.mdx";
import BatchScrapeOutput from "/snippets/fr/v2/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/fr/v2/batch-scrape/base/async-output.mdx";
import ScrapeLocationPython from "/snippets/fr/v2/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/fr/v2/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/fr/v2/scrape/location/curl.mdx";

Firecrawl convertit les pages web en markdown, idéal pour les applications LLM.

* Il gère les complexités : proxys, mise en cache, limites de débit, contenu bloqué par JS
* Prend en charge le contenu dynamique : sites dynamiques, sites rendus par JS, PDF, images
* Génère un markdown propre, des données structurées, des captures d’écran ou du HTML.

Pour plus de détails, consultez la [référence de l’API du point de terminaison /scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="scraping-a-url-with-firecrawl">
  ## Extraire le contenu d’une URL avec Firecrawl
</div>

<div id="scrape-endpoint">
  ### point de terminaison /scrape
</div>

Utilisé pour scraper une URL et en récupérer le contenu.

<div id="installation">
  ### Installation
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />
</CodeGroup>

<div id="usage">
  ### Utilisation
</div>

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeCURL />
</CodeGroup>

Pour plus d’informations sur les paramètres, consultez la [référence de l’API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="response">
  ### Réponse
</div>

Les SDK renvoient directement l’objet de données. cURL renvoie la charge utile exactement comme ci-dessous.

<ScrapeResponse />

<div id="scrape-formats">
  ## Formats de scrape
</div>

Vous pouvez désormais choisir les formats de sortie souhaités. Vous pouvez en spécifier plusieurs. Les formats pris en charge sont :

* Markdown (`markdown`)
* Résumé (`summary`)
* HTML (`html`)
* HTML brut (`rawHtml`) (sans modifications)
* Capture d’écran (`screenshot`, avec des options comme `fullPage`, `quality`, `viewport`)
* Liens (`links`)
* JSON (`json`) — sortie structurée
* Images (`images`) — extraction de toutes les URL d’images de la page

Les clés de sortie correspondront au format choisi.

<div id="extract-structured-data">
  ## Extraire des données structurées
</div>

<div id="scrape-with-json-endpoint">
  ### Point de terminaison /scrape (avec json)
</div>

Permet d’extraire des données structurées à partir de pages scrappées.

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

Résultat :

<ExtractOutput />

<div id="extracting-without-schema">
  ### Extraction sans schéma
</div>

Vous pouvez désormais extraire sans schéma en passant simplement un `prompt` au point de terminaison. Le LLM choisit la structure des données.

<CodeGroup>
  <ExtractNoSchemaPython />

  <ExtractNoSchemaNode />

  <ExtractNoSchemaCURL />
</CodeGroup>

Résultat :

<ExtractNoSchemaOutput />

<div id="json-format-options">
  ### Options du format JSON
</div>

Lorsque vous utilisez le format `json`, passez un objet dans `formats` avec les paramètres suivants :

* `schema` : schéma JSON pour la sortie structurée.
* `prompt` : invite facultative pour orienter l’extraction lorsqu’un schéma est présent ou lorsque vous souhaitez un guidage léger.

<div id="interacting-with-the-page-with-actions">
  ## Interagir avec la page à l’aide des actions
</div>

Firecrawl vous permet d’effectuer diverses actions sur une page web avant d’en extraire le contenu. C’est particulièrement utile pour interagir avec du contenu dynamique, naviguer entre les pages ou accéder à du contenu nécessitant une interaction de l’utilisateur.

Voici un exemple d’utilisation des actions pour accéder à google.com, rechercher Firecrawl, cliquer sur le premier résultat et prendre une capture d’écran.

Il est recommandé d’utiliser presque systématiquement l’action `wait` avant et après l’exécution d’autres actions afin de laisser suffisamment de temps au chargement de la page.

<div id="example">
  ### Exemple
</div>

<CodeGroup>
  <ScrapeActionsPython />

  <ScrapeActionsNode />

  <ScrapeActionsCURL />
</CodeGroup>

<div id="output">
  ### Résultat
</div>

<CodeGroup>
  <ScrapeActionsOutput />
</CodeGroup>

Pour en savoir plus sur les paramètres des actions, consultez la [référence de l’API](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<div id="location-and-language">
  ## Localisation et langue
</div>

Indiquez le pays et les langues souhaitées pour obtenir un contenu pertinent selon votre zone cible et vos préférences linguistiques.

<div id="how-it-works">
  ### Fonctionnement
</div>

Lorsque vous renseignez les paramètres de localisation, Firecrawl utilisera, si possible, un proxy adapté et adoptera la langue et le fuseau horaire correspondants. Par défaut, la localisation est définie sur « US » si aucun paramètre n’est fourni.

<div id="usage">
  ### Utilisation
</div>

Pour utiliser les paramètres de localisation et de langue, incluez l’objet `location` dans le corps de votre requête avec les propriétés suivantes :

* `country` : code pays ISO 3166-1 alpha-2 (p. ex. « US », « AU », « DE », « JP »). Par défaut : « US ».
* `languages` : un tableau des langues et paramètres régionaux préférés pour la requête, par ordre de priorité. Par défaut : la langue de la localisation spécifiée.

<CodeGroup>
  <ScrapeLocationPython />

  <ScrapeLocationNode />

  <ScrapeLocationCURL />
</CodeGroup>

Pour plus de détails sur les localisations prises en charge, consultez la [documentation sur les proxys](/fr/features/proxies).

<div id="caching-and-maxage">
  ## Mise en cache et maxAge
</div>

Pour accélérer les requêtes, Firecrawl renvoie par défaut les résultats depuis le cache lorsqu’une copie récente est disponible.

* **Fenêtre de fraîcheur par défaut** : `maxAge = 172800000` ms (2 jours). Si une page en cache est plus récente que ce délai, elle est renvoyée instantanément ; sinon, la page est explorée puis mise en cache.
* **Performances** : cela peut accélérer les scrapes jusqu’à 5x lorsque les données n’ont pas besoin d’être ultra fraîches.
* **Toujours récupérer du contenu frais** : définissez `maxAge` à `0`.
* **Éviter le stockage** : définissez `storeInCache` sur `false` si vous ne voulez pas que Firecrawl mette en cache/stocke les résultats pour cette requête.

Exemple (forcer du contenu frais) :

<CodeGroup>
  ```python Python
  from firecrawl import Firecrawl
  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  doc = firecrawl.scrape(url='https://example.com', maxAge=0, formats=['markdown'])
  print(doc)
  ```

  ```js Node
  import Firecrawl from '@mendable/firecrawl-js';

  const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

  const doc = await firecrawl.scrape('https://example.com', { maxAge: 0, formats: ['markdown'] });
  console.log(doc);
  ```

  ```bash cURL
  curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
    -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "url": "https://example.com",
      "maxAge": 0,
      "formats": ["markdown"]
    }'
  ```
</CodeGroup>

Exemple (utiliser une fenêtre de cache de 10 minutes) :

<CodeGroup>
  ```python Python
  from firecrawl import Firecrawl
  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  doc = firecrawl.scrape(url='https://example.com', maxAge=600000, formats=['markdown', 'html'])
  print(doc)
  ```

  ```js Node

  const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

  const doc = await firecrawl.scrape('https://example.com', { maxAge: 600000, formats: ['markdown', 'html'] });
  console.log(doc);
  ```

  ```bash cURL
  curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
    -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "url": "https://example.com",
      "maxAge": 600000,
      "formats": ["markdown", "html"]
    }'
  ```
</CodeGroup>

<div id="batch-scraping-multiple-urls">
  ## Extraction par lots de plusieurs URL
</div>

Vous pouvez désormais lancer l’extraction par lots de plusieurs URL simultanément. La fonction prend en arguments les URL de départ ainsi que des paramètres optionnels. L’argument params vous permet de définir des options supplémentaires pour la tâche d’extraction par lots, comme les formats de sortie.

<div id="how-it-works">
  ### Fonctionnement
</div>

C’est très proche du fonctionnement du point de terminaison `/crawl`. Il lance un job de scraping par lot et renvoie un ID de job pour en vérifier l’état.

Le SDK propose deux méthodes, synchrone et asynchrone. La méthode synchrone renvoie les résultats du job de scraping par lot, tandis que la méthode asynchrone renvoie un ID de job que vous pouvez utiliser pour en suivre l’état.

<div id="usage">
  ### Utilisation
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Réponse
</div>

Si vous utilisez les méthodes synchrones des SDK, elles renverront les résultats du travail de scraping par lot. Sinon, elles renverront un identifiant de travail que vous pourrez utiliser pour vérifier l’état du scraping par lot.

<div id="synchronous">
  #### Synchrone
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### Asynchrone
</div>

Vous pouvez ensuite utiliser l’ID de la tâche pour vérifier l’état du batch scrape en appelant le point de terminaison `/batch/scrape/{id}`. Ce point de terminaison est destiné à être utilisé pendant l’exécution de la tâche ou juste après son achèvement, **car les tâches de batch scrape expirent après 24 heures**.

<BatchScrapeAsyncOutput />

<div id="stealth-mode">
  ## Mode furtif
</div>

Pour les sites web dotés d’une protection anti‑bot avancée, Firecrawl propose un mode proxy furtif qui améliore les taux de réussite lors du scraping de sites exigeants.

En savoir plus sur le [mode furtif](/fr/features/stealth-mode).