---
title: 'Exploration'
description: "Firecrawl peut parcourir de manière récursive les sous-domaines d'une URL et en extraire le contenu"
og:title: 'Exploration | Firecrawl'
og:description: "Firecrawl peut parcourir de manière récursive les sous-domaines d'une URL et en extraire le contenu"
icon: 'spider'
---

import InstallationPython from '/snippets/fr/v2/installation/python.mdx';
import InstallationNode from '/snippets/fr/v2/installation/js.mdx';
import CrawlPython from '/snippets/fr/v2/crawl/base/python.mdx';
import CrawlNode from '/snippets/fr/v2/crawl/base/js.mdx';
import CrawlCURL from '/snippets/fr/v2/crawl/base/curl.mdx';
import CheckCrawlJobPython from '/snippets/fr/v2/crawl-status/short/python.mdx';
import CheckCrawlJobNode from '/snippets/fr/v2/crawl-status/short/js.mdx';
import CheckCrawlJobCURL from '/snippets/fr/v2/crawl-status/short/curl.mdx';
import CheckCrawlJobOutputScraping from '/snippets/fr/v2/crawl-status/base/output-scraping.mdx';
import CheckCrawlJobOutputCompleted from '/snippets/fr/v2/crawl-status/base/output-completed.mdx';
import CrawlWebSocketPython from '/snippets/fr/v2/crawl-websocket/base/python.mdx';
import CrawlWebSocketNode from '/snippets/fr/v2/crawl-websocket/base/js.mdx';
import CrawlWebhookCURL from '/snippets/fr/v2/crawl-webhook/base/curl.mdx';
import PythonCrawlExample from '/snippets/fr/v2/crawl/sdk-example/python.mdx';
import NodeCrawlExample from '/snippets/fr/v2/crawl/sdk-example/js.mdx';
import PythonCrawlExampleResponse from '/snippets/fr/v2/crawl/sdk-example/python-response.mdx';
import NodeCrawlExampleResponse from '/snippets/fr/v2/crawl/sdk-example/js-response.mdx';
import StartCrawlPython from '/snippets/fr/v2/start-crawl/base/python.mdx';
import StartCrawlNode from '/snippets/fr/v2/start-crawl/base/js.mdx';
import StartCrawlCURL from '/snippets/fr/v2/start-crawl/base/curl.mdx';
import StartCrawlOutput from '/snippets/fr/v2/start-crawl/base/output.mdx';

Firecrawl explore efficacement les sites web pour extraire des données complètes tout en contournant les blocages. Le processus :

1. **Analyse d’URL :** Parcourt le sitemap et le site pour identifier les liens
2. **Parcours :** Suit les liens de manière récursive afin de trouver toutes les sous-pages
3. **Scraping :** Extrait le contenu de chaque page, en gérant le JavaScript et les limites de taux
4. **Résultats :** Convertit les données en Markdown propre ou en format structuré

Cela garantit une collecte de données exhaustive à partir de n’importe quelle URL de départ.

<div id="crawling">
  ## Crawl
</div>

<div id="crawl-endpoint">
  ### point de terminaison /crawl
</div>

Permet d’explorer une URL et toutes les sous-pages accessibles. Soumet un travail d’exploration et renvoie un ID de tâche pour suivre l’état de l’exploration.

<Warning>
  Par défaut, Crawl ignore les sous-liens d’une page s’ils ne sont pas des « enfants »
  de l’URL fournie. Ainsi, website.com/other-parent/blog-1 ne sera pas
  renvoyé si vous explorez website.com/blogs/. Si vous souhaitez
  website.com/other-parent/blog-1, utilisez le paramètre `crawlEntireDomain`. Pour
  explorer des sous-domaines comme blog.website.com lors de l’exploration de website.com, utilisez le
  paramètre `allowSubdomains`.
</Warning>

<div id="installation">
  ### Installation
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />
</CodeGroup>

<div id="usage">
  ### Utilisation
</div>

<CodeGroup>
  <CrawlPython />

  <CrawlNode />

  <CrawlCURL />
</CodeGroup>

<div id="scrape-options-in-crawl">
  ### Options de scrape dans crawl
</div>

Toutes les options de l’endpoint Scrape sont disponibles dans Crawl via `scrapeOptions` (JS) / `scrape_options` (Python). Elles s’appliquent à chaque page que le crawler extrait : formats, proxy, mise en cache, actions, localisation, tags, etc. Consultez la liste complète dans la [référence de l’API Scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<CodeGroup>
  ```js Node
  import Firecrawl from '@mendable/firecrawl-js';

  const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR_API_KEY' });

  // Crawl avec des options de scrape
  const crawlResponse = await firecrawl.crawl('https://example.com', {
    limit: 100,
    scrapeOptions: {
      formats: [
        'markdown',
        {
          type: 'json',
          schema: { type: 'object', properties: { title: { type: 'string' } } },
        },
      ],
      proxy: 'auto',
      maxAge: 600000,
      onlyMainContent: true,
    },
  });
  ```

  ```python Python
  from firecrawl import Firecrawl

  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  # Crawl avec des options de scrape
  response = firecrawl.crawl('https://example.com',
      limit=100,
      scrape_options={
          'formats': [
              'markdown',
              { 'type': 'json', 'schema': { 'type': 'object', 'properties': { 'title': { 'type': 'string' } } } }
          ],
          'proxy': 'auto',
          'maxAge': 600000,
          'onlyMainContent': True
      }
  )
  ```
</CodeGroup>

<div id="api-response">
  ### Réponse de l’API
</div>

Si vous utilisez cURL ou la méthode starter, cela renverra un `ID` pour vérifier l’état du crawl.

<Note>
  Si vous utilisez le SDK, consultez les méthodes ci-dessous pour comprendre le comportement « waiter » vs « starter ».
</Note>

<StartCrawlOutput />

<div id="check-crawl-job">
  ### Vérifier un job de crawl
</div>

Permet de consulter l’état d’un job de crawl et d’en récupérer le résultat.

<Note>
  Cet endpoint fonctionne uniquement pour les crawls en cours ou ceux qui viennent
  de se terminer.{' '}
</Note>

<CodeGroup>
  <CheckCrawlJobPython />

  <CheckCrawlJobNode />

  <CheckCrawlJobCURL />
</CodeGroup>

<div id="response-handling">
  #### Gestion des réponses
</div>

La réponse varie selon l’état du crawl.

Pour les crawls non terminés ou les réponses volumineuses dépassant 10 Mo, un paramètre d’URL `next` est fourni. Vous devez appeler cette URL pour récupérer les 10 Mo de données suivants. Si le paramètre `next` est absent, cela indique la fin des données du crawl.

Le paramètre skip définit le nombre maximal de résultats renvoyés pour chaque segment de résultats.

<Info>
  Les paramètres skip et next ne sont pertinents que lors d’appels directs à l’API.
  Si vous utilisez le SDK, nous gérons cela pour vous et renverrons
  tous les résultats en une seule fois.
</Info>

<CodeGroup>
  <CheckCrawlJobOutputScraping />

  <CheckCrawlJobOutputCompleted />
</CodeGroup>

<div id="sdk-methods">
  ### Méthodes du SDK
</div>

Deux approches sont possibles pour utiliser le SDK :

1. **Crawler puis attendre** (`crawl`) :
   * Attend la fin du crawl et renvoie la réponse complète
   * Gère automatiquement la pagination
   * Recommandé pour la plupart des cas d’usage

<CodeGroup>
  <PythonCrawlExample />

  <NodeCrawlExample />
</CodeGroup>

La réponse inclut l’état du crawl et toutes les données extraites :

<CodeGroup>
  <PythonCrawlExampleResponse />

  <NodeCrawlExampleResponse />
</CodeGroup>

2. **Démarrer puis vérifier l’état** (`startCrawl`/`start_crawl`) :
   * Renvoie immédiatement un ID de crawl
   * Permet une vérification manuelle de l’état
   * Utile pour les crawls de longue durée ou une logique de polling personnalisée

<CodeGroup>
  <StartCrawlPython />

  <StartCrawlNode />

  <StartCrawlCURL />
</CodeGroup>

<div id="crawl-websocket">
  ## WebSocket de crawl
</div>

La méthode WebSocket de Firecrawl, « Crawl URL and Watch », permet l’extraction et la supervision des données en temps réel. Lancez un crawl à partir d’une URL et personnalisez-le avec des options comme des limites de pages, des domaines autorisés et des formats de sortie — idéal pour le traitement immédiat des données.

<CodeGroup>
  <CrawlWebSocketPython />

  <CrawlWebSocketNode />
</CodeGroup>

<div id="crawl-webhook">
  ## Webhook de crawl
</div>

Vous pouvez configurer des webhooks pour recevoir des notifications en temps réel à mesure que votre crawl progresse. Cela vous permet de traiter les pages dès leur extraction, au lieu d’attendre la fin du crawl.

<CrawlWebhookCURL />

Pour une documentation complète sur les webhooks, incluant les types d’événements, la structure des payloads et des exemples d’implémentation, consultez la [documentation Webhooks](/fr/webhooks/overview).

<div id="quick-reference">
  ### Référence rapide
</div>

**Types d’événements :**

* `crawl.started` - Au début de l’exploration
* `crawl.page` - Pour chaque page extraite avec succès
* `crawl.completed` - À la fin de l’exploration
* `crawl.failed` - Si l’exploration échoue

**Charge utile de base :**

```json
{
  "success": true,
  "type": "crawl.page",
  "id": "crawl-job-id",
  "data": [...], // Données de page pour les événements 'page'
  "metadata": {}, // Vos métadonnées personnalisées
  "error": null
}
```

<Note>
  Pour une configuration détaillée des webhooks, les meilleures pratiques de sécurité et
  la résolution des problèmes, consultez la [documentation sur les webhooks](/fr/webhooks/overview).
</Note>
