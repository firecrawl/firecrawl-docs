---
title: 'Exploration'
description: "Firecrawl peut parcourir de manière récursive les sous-domaines d'une URL et en extraire le contenu"
og:title: 'Exploration | Firecrawl'
og:description: "Firecrawl peut parcourir de manière récursive les sous-domaines d'une URL et en extraire le contenu"
icon: 'spider'
---

import InstallationPython from '/snippets/fr/v2/installation/python.mdx';
import InstallationNode from '/snippets/fr/v2/installation/js.mdx';
import InstallationCLI from '/snippets/fr/v2/installation/cli.mdx';
import CrawlPython from '/snippets/fr/v2/crawl/base/python.mdx';
import CrawlNode from '/snippets/fr/v2/crawl/base/js.mdx';
import CrawlCURL from '/snippets/fr/v2/crawl/base/curl.mdx';
import CrawlCLI from '/snippets/fr/v2/crawl/base/cli.mdx';
import CheckCrawlJobPython from '/snippets/fr/v2/crawl-status/short/python.mdx';
import CheckCrawlJobNode from '/snippets/fr/v2/crawl-status/short/js.mdx';
import CheckCrawlJobCURL from '/snippets/fr/v2/crawl-status/short/curl.mdx';
import CheckCrawlJobCLI from '/snippets/fr/v2/crawl-status/short/cli.mdx';
import CheckCrawlJobOutputScraping from '/snippets/fr/v2/crawl-status/base/output-scraping.mdx';
import CheckCrawlJobOutputCompleted from '/snippets/fr/v2/crawl-status/base/output-completed.mdx';
import CrawlWebSocketPython from '/snippets/fr/v2/crawl-websocket/base/python.mdx';
import CrawlWebSocketNode from '/snippets/fr/v2/crawl-websocket/base/js.mdx';
import CrawlWebhookCURL from '/snippets/fr/v2/crawl-webhook/base/curl.mdx';
import PythonCrawlExample from '/snippets/fr/v2/crawl/sdk-example/python.mdx';
import NodeCrawlExample from '/snippets/fr/v2/crawl/sdk-example/js.mdx';
import PythonCrawlExampleResponse from '/snippets/fr/v2/crawl/sdk-example/python-response.mdx';
import NodeCrawlExampleResponse from '/snippets/fr/v2/crawl/sdk-example/js-response.mdx';
import StartCrawlPython from '/snippets/fr/v2/start-crawl/base/python.mdx';
import StartCrawlNode from '/snippets/fr/v2/start-crawl/base/js.mdx';
import StartCrawlCURL from '/snippets/fr/v2/start-crawl/base/curl.mdx';
import StartCrawlCLI from '/snippets/fr/v2/start-crawl/base/cli.mdx';
import StartCrawlOutput from '/snippets/fr/v2/start-crawl/base/output.mdx';

Firecrawl explore efficacement les sites web pour extraire des données complètes tout en gérant une infrastructure web complexe. Le processus :

1. **Analyse d’URL :** Parcourt le sitemap et le site pour identifier les liens
2. **Parcours :** Suit les liens de manière récursive afin de trouver toutes les sous-pages
3. **Scraping :** Extrait le contenu de chaque page, en gérant le JavaScript et les limites de taux
4. **Résultats :** Convertit les données en Markdown propre ou en format structuré

Cela garantit une collecte de données exhaustive à partir de n’importe quelle URL de départ.

<div id="crawling">
  ## Crawl
</div>

<div id="crawl-endpoint">
  ### /crawl endpoint
</div>

Permet d’explorer une URL et toutes les sous-pages accessibles. Soumet un travail d’exploration et renvoie un ID de tâche pour suivre l’état de l’exploration.

<Warning>
  Par défaut, Crawl ignore les sous-liens d’une page s’ils ne sont pas des « enfants »
  de l’URL fournie. Ainsi, website.com/other-parent/blog-1 ne sera pas
  renvoyé si vous explorez website.com/blogs/. Si vous souhaitez
  website.com/other-parent/blog-1, utilisez le paramètre `crawlEntireDomain`. Pour
  explorer des sous-domaines comme blog.website.com lors de l’exploration de website.com, utilisez le
  paramètre `allowSubdomains`.
</Warning>

<Info>
  Chaque page explorée consomme 1 crédit (ainsi que des crédits supplémentaires pour des options comme le mode JSON, le proxy amélioré ou l’analyse de PDF). La valeur par défaut de `limit` est de 10 000 pages. Définissez une valeur de `limit` plus basse pour contrôler l’utilisation des crédits – par exemple, `limit: 100` pour plafonner l’exploration à 100 pages.
</Info>

<div id="installation">
  ### Installation
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationCLI />
</CodeGroup>

<div id="usage">
  ### Utilisation
</div>

<CodeGroup>
  <CrawlPython />

  <CrawlNode />

  <CrawlCURL />

  <CrawlCLI />
</CodeGroup>

<div id="scrape-options-in-crawl">
  ### Options de scrape dans crawl
</div>

Toutes les options de l’endpoint Scrape sont disponibles dans Crawl via `scrapeOptions` (JS) / `scrape_options` (Python). Elles s’appliquent à chaque page que le crawler extrait : formats, proxy, mise en cache, actions, localisation, tags, etc. Consultez la liste complète dans la [référence de l’API Scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<CodeGroup>
  ```js Node
  import Firecrawl from '@mendable/firecrawl-js';

  const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR_API_KEY' });

  // Crawl avec des options de scrape
  const crawlResponse = await firecrawl.crawl('https://example.com', {
    limit: 100,
    scrapeOptions: {
      formats: [
        'markdown',
        {
          type: 'json',
          schema: { type: 'object', properties: { title: { type: 'string' } } },
        },
      ],
      proxy: 'auto',
      maxAge: 600000,
      onlyMainContent: true,
    },
  });
  ```

  ```python Python
  from firecrawl import Firecrawl

  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  # Crawl avec des options de scrape
  response = firecrawl.crawl('https://example.com',
      limit=100,
      scrape_options={
          'formats': [
              'markdown',
              { 'type': 'json', 'schema': { 'type': 'object', 'properties': { 'title': { 'type': 'string' } } } }
          ],
          'proxy': 'auto',
          'maxAge': 600000,
          'onlyMainContent': True
      }
  )
  ```
</CodeGroup>

<div id="api-response">
  ### Réponse de l’API
</div>

Si vous utilisez cURL ou la méthode starter, cela renverra un `ID` pour vérifier l’état du crawl.

<Note>
  Si vous utilisez le SDK, consultez les méthodes ci-dessous pour comprendre le comportement « waiter » vs « starter ».
</Note>

<StartCrawlOutput />

<div id="check-crawl-job">
  ### Vérifier un job de crawl
</div>

Permet de consulter l’état d’un job de crawl et d’en récupérer le résultat.

<Note>
  Les résultats des jobs de crawl sont disponibles via l’API pendant 24 heures après leur achèvement. Après cette période, vous pouvez toujours consulter l’historique de vos crawls et leurs résultats dans les [journaux d’activité](https://www.firecrawl.dev/app/logs).
</Note>

<Note>
  Les pages dans le tableau `data` des résultats de crawl sont des pages que Firecrawl a extraites avec succès — même si le site cible a renvoyé une erreur HTTP comme 404. Le champ `metadata.statusCode` indique le code de statut HTTP renvoyé par le site cible. Pour récupérer les pages que Firecrawl lui‑même n’a pas réussi à extraire (par exemple en cas d’erreurs réseau, d’expirations de délai ou de blocages liés à robots.txt), utilisez l’endpoint dédié [Get Crawl Errors](/fr/api-reference/endpoint/crawl-get-errors) (`GET /crawl/{id}/errors`).
</Note>

<CodeGroup>
  <CheckCrawlJobPython />

  <CheckCrawlJobNode />

  <CheckCrawlJobCURL />

  <CheckCrawlJobCLI />
</CodeGroup>

<div id="response-handling">
  #### Gestion des réponses
</div>

La réponse varie selon l’état du crawl.

Pour les crawls non terminés ou les réponses volumineuses dépassant 10 Mo, un paramètre d’URL `next` est fourni. Vous devez appeler cette URL pour récupérer les 10 Mo de données suivants. Si le paramètre `next` est absent, cela indique la fin des données du crawl.

Le paramètre skip définit le nombre maximal de résultats renvoyés pour chaque segment de résultats.

<Info>
  Les paramètres skip et next ne sont pertinents que lors d’appels directs à l’API.
  Si vous utilisez le SDK, nous gérons cela pour vous et renverrons
  tous les résultats en une seule fois.
</Info>

<CodeGroup>
  <CheckCrawlJobOutputScraping />

  <CheckCrawlJobOutputCompleted />
</CodeGroup>

<div id="sdk-methods">
  ### Méthodes du SDK
</div>

Deux approches sont possibles pour utiliser le SDK :

1. **Crawler puis attendre** (`crawl`) :
   * Attend la fin du crawl et renvoie la réponse complète
   * Gère automatiquement la pagination
   * Recommandé pour la plupart des cas d’usage

<CodeGroup>
  <PythonCrawlExample />

  <NodeCrawlExample />
</CodeGroup>

La réponse inclut l’état du crawl et toutes les données extraites :

<CodeGroup>
  <PythonCrawlExampleResponse />

  <NodeCrawlExampleResponse />
</CodeGroup>

2. **Démarrer puis vérifier l’état** (`startCrawl`/`start_crawl`) :
   * Renvoie immédiatement un ID de crawl
   * Permet une vérification manuelle de l’état
   * Utile pour les crawls de longue durée ou une logique de polling personnalisée

<CodeGroup>
  <StartCrawlPython />

  <StartCrawlNode />

  <StartCrawlCURL />

  <StartCrawlCLI />
</CodeGroup>

<div id="crawl-websocket">
  ## WebSocket de crawl
</div>

La méthode WebSocket de Firecrawl, « Crawl URL and Watch », permet l’extraction et la supervision des données en temps réel. Lancez un crawl à partir d’une URL et personnalisez-le avec des options comme des limites de pages, des domaines autorisés et des formats de sortie — idéal pour le traitement immédiat des données.

<CodeGroup>
  <CrawlWebSocketPython />

  <CrawlWebSocketNode />
</CodeGroup>

<div id="crawl-webhook">
  ## Webhook de crawl
</div>

Vous pouvez configurer des webhooks pour recevoir des notifications en temps réel à mesure que votre crawl progresse. Cela vous permet de traiter les pages dès leur extraction, au lieu d’attendre la fin du crawl.

<CrawlWebhookCURL />

<div id="quick-reference">
  ### Référence rapide
</div>

**Types d’événements :**

* `crawl.started` - Au début de l’exploration
* `crawl.page` - Pour chaque page extraite avec succès
* `crawl.completed` - À la fin de l’exploration
* `crawl.failed` - Si l’exploration échoue

**Charge utile de base :**

```json
{
  "success": true,
  "type": "crawl.page",
  "id": "crawl-job-id",
  "data": [...], // Données de page pour les événements 'page'
  "metadata": {}, // Your custom metadata
  "error": null
}
```

<div id="security-verifying-webhook-signatures">
  ### Sécurité : vérification des signatures de webhook
</div>

Chaque requête de webhook provenant de Firecrawl inclut un en-tête `X-Firecrawl-Signature` contenant une signature HMAC-SHA256. **Vérifiez toujours cette signature** pour vous assurer que le webhook est authentique et n&#39;a pas été altéré.

**Fonctionnement :**

1. Récupérez votre secret de webhook dans l&#39;[onglet Advanced](https://www.firecrawl.dev/app/settings?tab=advanced) des paramètres de votre compte
2. Extrayez la signature de l&#39;en-tête `X-Firecrawl-Signature`
3. Calculez le HMAC-SHA256 du corps brut de la requête à l&#39;aide de votre secret
4. Comparez-le avec l&#39;en-tête de signature en utilisant une fonction sécurisée contre les attaques par temporisation

<Warning>
  Ne traitez jamais un webhook sans vérifier d&#39;abord sa signature. L&#39;en-tête `X-Firecrawl-Signature` contient la signature au format : `sha256=abc123def456...`
</Warning>

Pour des exemples d&#39;implémentation complets en JavaScript et Python, consultez la [documentation sur la sécurité des webhooks](/fr/webhooks/security).

<div id="full-documentation">
  ### Documentation complète
</div>

Pour une documentation complète des webhooks, incluant le détail des payloads d’événements, la structure des payloads, la configuration avancée et le dépannage, consultez la [documentation Webhooks](/fr/webhooks/overview).