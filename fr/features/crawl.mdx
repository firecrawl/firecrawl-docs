---
title: 'Crawl'
description: 'Firecrawl peut explorer récursivement les sous-domaines d’une URL et en extraire le contenu'
og:title: 'Crawl | Firecrawl'
og:description: 'Firecrawl peut explorer récursivement les sous-domaines d’une URL et en extraire le contenu'
icon: 'spider'
---

import InstallationPython from '/snippets/fr/v2/installation/python.mdx';
import InstallationNode from '/snippets/fr/v2/installation/js.mdx';
import CrawlPython from '/snippets/fr/v2/crawl/base/python.mdx';
import CrawlNode from '/snippets/fr/v2/crawl/base/js.mdx';
import CrawlCURL from '/snippets/fr/v2/crawl/base/curl.mdx';
import CheckCrawlJobPython from '/snippets/fr/v2/crawl-status/short/python.mdx';
import CheckCrawlJobNode from '/snippets/fr/v2/crawl-status/short/js.mdx';
import CheckCrawlJobCURL from '/snippets/fr/v2/crawl-status/short/curl.mdx';
import CheckCrawlJobOutputScraping from '/snippets/fr/v2/crawl-status/base/output-scraping.mdx';
import CheckCrawlJobOutputCompleted from '/snippets/fr/v2/crawl-status/base/output-completed.mdx';
import CrawlWebSocketPython from '/snippets/fr/v2/crawl-websocket/base/python.mdx';
import CrawlWebSocketNode from '/snippets/fr/v2/crawl-websocket/base/js.mdx';
import CrawlWebhookCURL from '/snippets/fr/v2/crawl-webhook/base/curl.mdx';
import PythonCrawlExample from '/snippets/fr/v2/crawl/sdk-example/python.mdx';
import NodeCrawlExample from '/snippets/fr/v2/crawl/sdk-example/js.mdx';
import PythonCrawlExampleResponse from '/snippets/fr/v2/crawl/sdk-example/python-response.mdx';
import NodeCrawlExampleResponse from '/snippets/fr/v2/crawl/sdk-example/js-response.mdx';
import StartCrawlPython from '/snippets/fr/v2/start-crawl/base/python.mdx';
import StartCrawlNode from '/snippets/fr/v2/start-crawl/base/js.mdx';
import StartCrawlCURL from '/snippets/fr/v2/start-crawl/base/curl.mdx';
import StartCrawlOutput from '/snippets/fr/v2/start-crawl/base/output.mdx';

Firecrawl explore efficacement les sites web pour extraire des données complètes tout en contournant les blocages. Le processus :

1. **Analyse de l’URL :** Analyse le sitemap et parcourt le site pour identifier les liens
2. **Exploration :** Suit les liens de façon récursive pour trouver toutes les sous-pages
3. **Scraping :** Extrait le contenu de chaque page, en gérant le JavaScript et les limites de débit
4. **Résultats :** Convertit les données en Markdown propre ou en format structuré

Cela garantit une collecte de données exhaustive à partir de n’importe quelle URL de départ.

<div id='crawling'>## Crawl</div>

<div id='crawl-endpoint'>### point de terminaison /crawl</div>

Permet d’explorer une URL et toutes ses sous-pages accessibles. Cette opération crée une tâche de crawl et renvoie un ID de tâche pour suivre l’état du crawl.

<Warning>
  Par défaut, le crawl ignore les sous-liens d’une page s’ils ne sont pas des
  enfants de l’URL fournie. Ainsi, website.com/other-parent/blog-1 ne sera pas
  renvoyé si vous avez crawlé website.com/blogs/. Si vous souhaitez inclure
  website.com/other-parent/blog-1, utilisez le paramètre `crawlEntireDomain`.
  Pour explorer des sous-domaines comme blog.website.com lors du crawl de
  website.com, utilisez le paramètre `allowSubdomains`.
</Warning>

<div id='installation'>### Installation</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />
</CodeGroup>

<div id='usage'>### Utilisation</div>

<CodeGroup>
  <CrawlPython />

{' '}
<CrawlNode />

  <CrawlCURL />
</CodeGroup>

<div id='scrape-options-in-crawl'>### Options de scraping dans crawl</div>

Toutes les options du point de terminaison /scrape sont disponibles dans crawl via `scrapeOptions` (JS) / `scrape_options` (Python). Elles s’appliquent à chaque page que le crawler récupère : formats, proxy, mise en cache, actions, localisation, tags, etc. Consultez la liste complète dans la [référence de l’API Scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<CodeGroup>
  ```js Node
  import Firecrawl from '@mendable/firecrawl-js';

const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR_API_KEY' });

// Crawl with scrape options
const crawlResponse = await firecrawl.crawl('https://example.com', {
limit: 100,
scrapeOptions: {
formats: [
'markdown',
{ type: 'json', schema: { type: 'object', properties: { title: { type: 'string' } } } }
],
proxy: 'auto',
maxAge: 600000,
onlyMainContent: true
}
});

````

```python Python
from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

# Crawl with scrape options
response = firecrawl.crawl('https://example.com',
    limit=100,
    scrape_options={
        'formats': [
            'markdown',
            { 'type': 'json', 'schema': { 'type': 'object', 'properties': { 'title': { 'type': 'string' } } } }
        ],
        'proxy': 'auto',
        'maxAge': 600000,
        'onlyMainContent': True
    }
)
````

</CodeGroup>

<div id='api-response'>### Réponse de l’API</div>

Si vous utilisez cURL ou la méthode starter, un `ID` sera renvoyé pour vérifier l’état du crawl.

<Note>
  Si vous utilisez le SDK, consultez les méthodes ci-dessous pour comprendre le
  comportement waiter vs starter.
</Note>

<StartCrawlOutput />

<div id='check-crawl-job'>### Vérifier une tâche de crawl</div>

Permet de vérifier l’état d’une tâche de crawl et d’obtenir son résultat.

<Note>
  Ce point de terminaison fonctionne uniquement pour les crawls en cours ou ceux
  qui viennent de se terminer.{' '}
</Note>

<CodeGroup>
  <CheckCrawlJobPython />

{' '}
<CheckCrawlJobNode />

  <CheckCrawlJobCURL />
</CodeGroup>

<div id='response-handling'>#### Gestion de la réponse</div>

La réponse varie selon l’état du crawl.

Pour les réponses non terminées ou volumineuses dépassant 10 Mo, un paramètre d’URL `next` est fourni. Vous devez appeler cette URL pour récupérer les 10 Mo de données suivants. Si le paramètre `next` est absent, cela signifie que vous avez atteint la fin des données du crawl.

Le paramètre skip définit le nombre maximal de résultats renvoyés pour chaque lot de résultats.

<Info>
  Les paramètres skip et next ne sont pertinents que lorsque vous appelez
  directement l’API. Si vous utilisez le SDK, nous gérons cela pour vous et
  renvoyons tous les résultats en une seule fois.
</Info>

<CodeGroup>
  <CheckCrawlJobOutputScraping />

  <CheckCrawlJobOutputCompleted />
</CodeGroup>

<div id='sdk-methods'>### Méthodes du SDK</div>

Deux approches sont possibles pour utiliser le SDK :

1. **Crawler puis attendre** (`crawl`) :
   - Attend la fin du crawl et renvoie la réponse complète
   - Gère automatiquement la pagination
   - Recommandé pour la plupart des cas d’usage

<CodeGroup>
  <PythonCrawlExample />

  <NodeCrawlExample />
</CodeGroup>

La réponse inclut l’état du crawl et toutes les données extraites :

<CodeGroup>
  <PythonCrawlExampleResponse />

  <NodeCrawlExampleResponse />
</CodeGroup>

2. **Démarrer puis vérifier l’état** (`startCrawl`/`start_crawl`) :
   - Renvoie immédiatement un ID de crawl
   - Permet une vérification manuelle de l’état
   - Utile pour les crawls de longue durée ou une logique de polling personnalisée

<CodeGroup>
  <StartCrawlPython />

{' '}
<StartCrawlNode />

  <StartCrawlCURL />
</CodeGroup>

<div id='crawl-websocket'>## WebSocket de crawl</div>

La méthode WebSocket de Firecrawl, « Crawl URL and Watch », permet l’extraction et la supervision des données en temps réel. Lancez un crawl à partir d’une URL et personnalisez-le avec des options comme des limites de pages, des domaines autorisés et des formats de sortie — idéal pour le traitement immédiat des données.

<CodeGroup>
  <CrawlWebSocketPython />

  <CrawlWebSocketNode />
</CodeGroup>

<div id='crawl-webhook'>## Webhook de crawl</div>

Vous pouvez configurer des webhooks pour recevoir des notifications en temps réel à mesure que votre crawl avance. Cela vous permet de traiter les pages dès qu’elles sont extraites, au lieu d’attendre la fin complète du crawl.

<CrawlWebhookCURL />

Pour une documentation complète sur les webhooks, incluant les types d’événements, la structure des charges utiles et des exemples d’implémentation, consultez la [documentation sur les webhooks](/fr/webhooks/overview).

<div id='quick-reference'>### Référence rapide</div>

**Types d’événements :**

- `crawl.started` - Au début de l’exploration
- `crawl.page` - Pour chaque page extraite avec succès
- `crawl.completed` - À la fin de l’exploration
- `crawl.failed` - Si l’exploration rencontre une erreur

**Payload de base :**

```json
{
  "success": true,
  "type": "crawl.page",
  "id": "crawl-job-id",
  "data": [...], // Données de page pour les événements « page »
  "metadata": {}, // Vos métadonnées personnalisées
  "error": null
}
```

<Note>
  Pour des informations détaillées sur la configuration des webhooks, les bonnes
  pratiques de sécurité et le dépannage, consultez la [documentation sur les
  webhooks](/fr/webhooks/overview).
</Note>
