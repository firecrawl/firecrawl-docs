---
title: "Extraction par lots"
description: "Extraire plusieurs URL par lots"
og:title: "Extraction par lots | Firecrawl"
og:description: "Extraire plusieurs URL par lots"
---

import BatchScrapePython from "/snippets/fr/v2/batch-scrape/base/python.mdx";
import BatchScrapeNode from "/snippets/fr/v2/batch-scrape/base/js.mdx";
import BatchScrapeCURL from "/snippets/fr/v2/batch-scrape/base/curl.mdx";
import BatchScrapeOutput from "/snippets/fr/v2/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/fr/v2/batch-scrape/base/async-output.mdx";
import BatchScrapeExtractPython from "/snippets/fr/v2/batch-scrape/json/python.mdx";
import BatchScrapeExtractNode from "/snippets/fr/v2/batch-scrape/json/js.mdx";
import BatchScrapeExtractCURL from "/snippets/fr/v2/batch-scrape/json/curl.mdx";
import BatchScrapeExtractOutput from "/snippets/fr/v2/batch-scrape/json/output.mdx";
import BatchScrapeExtractAsyncOutput from "/snippets/fr/v2/batch-scrape/json/async-output.mdx";
import BatchScrapeWebhookCURL from "/snippets/fr/v1/batch-scrape-webhook/base/curl.mdx";

<div id="batch-scraping-multiple-urls">
  ## Extraction en lot de plusieurs URL
</div>

Vous pouvez désormais extraire en lot plusieurs URL simultanément. La fonction prend les URL de départ et des paramètres optionnels comme arguments. L’argument params vous permet de définir des options supplémentaires pour le traitement en lot, telles que les formats de sortie.

<div id="how-it-works">
  ### Fonctionnement
</div>

C’est très similaire au fonctionnement du point de terminaison `/crawl`. Vous pouvez soit lancer le lot et attendre qu’il se termine, soit le lancer et gérer vous‑même sa finalisation.

* `batchScrape` (JS) / `batch_scrape` (Python) : lance un lot et attend sa fin, puis renvoie les résultats.
* `startBatchScrape` (JS) / `start_batch_scrape` (Python) : lance un lot et renvoie l’ID du job pour que vous puissiez effectuer du polling ou utiliser des webhooks.

<div id="usage">
  ### Utilisation
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Réponse
</div>

* Appeler `batchScrape`/`batch_scrape` renvoie les résultats complets une fois le lot terminé.

<BatchScrapeOutput />

`

* Appeler `startBatchScrape`/`start_batch_scrape` renvoie un ID de tâche que vous pouvez suivre via `getBatchScrapeStatus`/`get_batch_scrape_status`, à l’aide du point de terminaison d’API `/batch/scrape/{id}` ou de webhooks. Ce point de terminaison est destiné aux contrôles en cours ou juste après l’achèvement, **car les tâches de lot expirent au bout de 24 heures**.

<BatchScrapeAsyncOutput />

<div id="batch-scrape-with-structured-extraction">
  ## Grattage en lot avec extraction structurée
</div>

Vous pouvez aussi utiliser le point de terminaison de grattage en lot pour extraire des données structurées depuis les pages. C’est utile si vous voulez obtenir les mêmes données structurées à partir d’une liste d’URL.

<CodeGroup>
  <BatchScrapeExtractPython />

  <BatchScrapeExtractNode />

  <BatchScrapeExtractCURL />
</CodeGroup>

<div id="response">
  ### Réponse
</div>

* `batchScrape`/`batch_scrape` renvoie les résultats complets :

<BatchScrapeExtractOutput />

* `startBatchScrape`/`start_batch_scrape` renvoie un ID de tâche :

<BatchScrapeExtractAsyncOutput />

<div id="batch-scrape-with-webhooks">
  ## Scraping par lot avec webhooks
</div>

Vous pouvez configurer des webhooks pour recevoir des notifications en temps réel au fur et à mesure que chaque URL de votre lot est traitée. Cela vous permet de traiter les résultats immédiatement plutôt que d’attendre la fin de l’ensemble du lot.

<BatchScrapeWebhookCURL />

Pour une documentation complète sur les webhooks — y compris les types d’événements, la structure des payloads et des exemples d’implémentation — consultez la [documentation des webhooks](/fr/features/webhooks).

<div id="quick-reference">
  ### Référence rapide
</div>

**Types d’événements :**

* `batch_scrape.started` - Au démarrage du batch scrape
* `batch_scrape.page` - Pour chaque URL extraite avec succès
* `batch_scrape.completed` - Lorsque toutes les URL ont été traitées
* `batch_scrape.failed` - Si le batch scrape rencontre une erreur

**Payload de base :**

```json
{
  "success": true,
  "type": "batch_scrape.page",
  "id": "batch-job-id",
  "data": [...], // Données de page pour les événements "page"
  "metadata": {}, // Vos métadonnées personnalisées
  "error": null
}
```

<Note>
  Pour des informations détaillées sur la configuration des webhooks, les bonnes pratiques de sécurité et le dépannage, consultez la [documentation Webhooks](/fr/features/webhooks).
</Note>
