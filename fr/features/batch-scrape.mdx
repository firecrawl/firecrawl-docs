---
title: "Scrape par lots"
description: "Scraper plusieurs URL par lots"
og:title: "Scrape par lots | Firecrawl"
og:description: "Scraper plusieurs URL par lots"
---

import BatchScrapePython from '/snippets/fr/v2/batch-scrape/base/python.mdx';
import BatchScrapeNode from '/snippets/fr/v2/batch-scrape/base/js.mdx';
import BatchScrapeCURL from '/snippets/fr/v2/batch-scrape/base/curl.mdx';
import BatchScrapeOutput from '/snippets/fr/v2/batch-scrape/base/output.mdx';
import BatchScrapeAsyncOutput from '/snippets/fr/v2/batch-scrape/base/async-output.mdx';
import BatchScrapeExtractPython from '/snippets/fr/v2/batch-scrape/json/python.mdx';
import BatchScrapeExtractNode from '/snippets/fr/v2/batch-scrape/json/js.mdx';
import BatchScrapeExtractCURL from '/snippets/fr/v2/batch-scrape/json/curl.mdx';
import BatchScrapeExtractOutput from '/snippets/fr/v2/batch-scrape/json/output.mdx';
import BatchScrapeExtractAsyncOutput from '/snippets/fr/v2/batch-scrape/json/async-output.mdx';
import BatchScrapeWebhookCURL from '/snippets/fr/v1/batch-scrape-webhook/base/curl.mdx';

<div id="batch-scraping-multiple-urls">
  ## Extraction en lot de plusieurs URL
</div>

Vous pouvez désormais extraire en lot plusieurs URL simultanément. La fonction prend les URL de départ et des paramètres optionnels comme arguments. L’argument params vous permet de définir des options supplémentaires pour le traitement en lot, telles que les formats de sortie.

<div id="how-it-works">
  ### Fonctionnement
</div>

C’est très similaire au fonctionnement du point de terminaison `/crawl`. Vous pouvez soit lancer le lot et attendre qu’il se termine, soit le lancer et gérer vous‑même sa finalisation.

* `batchScrape` (JS) / `batch_scrape` (Python) : lance un lot et attend sa fin, puis renvoie les résultats.
* `startBatchScrape` (JS) / `start_batch_scrape` (Python) : lance un lot et renvoie l’ID du job pour que vous puissiez effectuer du polling ou utiliser des webhooks.

<div id="usage">
  ### Utilisation
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Réponse
</div>

* L’appel à `batchScrape`/`batch_scrape` renvoie les résultats complets une fois le lot terminé.

<BatchScrapeOutput />`- L’appel à`startBatchScrape`/`start&#95;batch&#95;scrape`renvoie
un ID de tâche que vous pouvez suivre via`getBatchScrapeStatus`/`get&#95;batch&#95;scrape&#95;status`, avec
l’endpoint d’API `/batch/scrape/{id}` ou des webhooks. Cet endpoint est destiné aux
contrôles en cours ou juste après la fin, **car les tâches de lot expirent après
24 heures**.

<BatchScrapeAsyncOutput />

<div id="batch-scrape-with-structured-extraction">
  ## Grattage en lot avec extraction structurée
</div>

Vous pouvez aussi utiliser le point de terminaison de grattage en lot pour extraire des données structurées depuis les pages. C’est utile si vous voulez obtenir les mêmes données structurées à partir d’une liste d’URL.

<CodeGroup>
  <BatchScrapeExtractPython />

  <BatchScrapeExtractNode />

  <BatchScrapeExtractCURL />
</CodeGroup>

<div id="response">
  ### Réponse
</div>

* `batchScrape`/`batch_scrape` renvoie les résultats complets :

<BatchScrapeExtractOutput />

* `startBatchScrape`/`start_batch_scrape` renvoie un ID de tâche :

<BatchScrapeExtractAsyncOutput />

<div id="batch-scrape-with-webhooks">
  ## Récupération en lot avec webhooks
</div>

Vous pouvez configurer des webhooks pour recevoir des notifications en temps réel à mesure que chaque URL de votre lot est récupérée. Cela vous permet de traiter les résultats immédiatement, sans attendre la fin de l’ensemble du lot.

<BatchScrapeWebhookCURL />

Pour une documentation complète sur les webhooks, incluant les types d’événements, la structure des payloads et des exemples d’implémentation, consultez la [documentation sur les webhooks](/fr/webhooks/overview).

<div id="quick-reference">
  ### Référence rapide
</div>

**Types d’événements :**

* `batch_scrape.started` - Lorsque le scraping par lot démarre
* `batch_scrape.page` - Pour chaque URL extraite avec succès
* `batch_scrape.completed` - Lorsque toutes les URL sont traitées
* `batch_scrape.failed` - Si le scraping par lot rencontre une erreur

**Charge utile de base :**

```json
{
  "success": true,
  "type": "batch_scrape.page",
  "id": "batch-job-id",
  "data": [...], // Données de page pour les événements 'page'
  "metadata": {}, // Vos métadonnées personnalisées
  "error": null
}
```

<Note>
  Pour une configuration détaillée des webhooks, les bonnes pratiques de sécurité et
  le dépannage, consultez la [documentation sur les webhooks](/fr/webhooks/overview).
</Note>
