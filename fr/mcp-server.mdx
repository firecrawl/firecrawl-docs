---
title: Serveur MCP Firecrawl
description: "Utilisez lâ€™API Firecrawl via le Model Context Protocol"
og:title: 'Serveur MCP Firecrawl'
og:description: "Les serveurs MCP Firecrawl vous permettent dâ€™utiliser lâ€™API Firecrawl via le Model Context Protocol"
sidebarTitle: 'Serveur MCP'
---

Une implÃ©mentation de serveur Model Context Protocol (MCP) intÃ©grant [Firecrawl](https://github.com/mendableai/firecrawl) pour le web scraping. Notre serveur MCP est open source et disponible sur [GitHub](https://github.com/mendableai/firecrawl-mcp-server).

## FonctionnalitÃ©s

- Scraping, crawling et dÃ©couverte du web
- Recherche et extraction de contenu
- Recherche approfondie et scraping en lots
- Prise en charge du cloud et de lâ€™autoâ€‘hÃ©bergement
- Prise en charge du streaming HTTP

## Installation

Vous pouvez utiliser notre URL hÃ©bergÃ©e ou exÃ©cuter le serveur en local. RÃ©cupÃ©rez votre clÃ© API sur [https://firecrawl.dev/app/api-keys](https://www.firecrawl.dev/app/api-keys)

### URL hÃ©bergÃ©e Ã  distance

```bash
https://mcp.firecrawl.dev/{FIRECRAWL_API_KEY}/v2/mcp
```


### ExÃ©cuter avec npx

```bash
env FIRECRAWL_API_KEY=fc-VOTRE_CLÃ‰_API npx -y firecrawl-mcp
```


### Installation manuelle

```bash
npm install -g firecrawl-mcp
```


### ExÃ©cution dans Cursor

<a href='cursor://anysphere.cursor-deeplink/mcp/install?name=firecrawl&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsImZpcmVjcmF3bC1tY3AiXSwiZW52Ijp7IkZJUkVDUkFXTF9BUElfS0VZIjoiWU9VUi1BUEktS0VZIn19'>
  <img
    src='https://cursor.com/deeplink/mcp-install-dark.png'
    alt='Ajouter le serveur MCP Firecrawl Ã  Cursor'
    style={{ maxHeight: 32 }}
  />
</a>

#### Installation manuelle

Configuration de Cursor ðŸ–¥ï¸
Remarque : nÃ©cessite Cursor version 0.45.6+
Pour des instructions de configuration Ã  jour, consultez la documentation officielle de Cursor sur la configuration des serveurs MCP :
[Guide de configuration du serveur MCP de Cursor](https://docs.cursor.com/context/model-context-protocol#configuring-mcp-servers)

Pour configurer Firecrawl MCP dans Cursor **v0.48.6**

1. Ouvrez les paramÃ¨tres de Cursor
2. Allez dans Features > MCP Servers
3. Cliquez sur "+ Add new global MCP server"
4. Saisissez le code suivant :
   ```json
   {
     "mcpServers": {
       "firecrawl-mcp": {
         "command": "npx",
         "args": ["-y", "firecrawl-mcp"],
         "env": {
           "FIRECRAWL_API_KEY": "YOUR-API-KEY"
         }
       }
     }
   }
   ```

Pour configurer Firecrawl MCP dans Cursor **v0.45.6**

1. Ouvrez les paramÃ¨tres de Cursor
2. Allez dans Features > MCP Servers
3. Cliquez sur "+ Add New MCP Server"
4. Renseignez les Ã©lÃ©ments suivants :
   - Name: "firecrawl-mcp" (ou le nom de votre choix)
   - Type: "command"
   - Command: `env FIRECRAWL_API_KEY=your-api-key npx -y firecrawl-mcp`

> Si vous utilisez Windows et rencontrez des problÃ¨mes, essayez `cmd /c "set FIRECRAWL_API_KEY=your-api-key && npx -y firecrawl-mcp"`

Remplacez `your-api-key` par votre clÃ© API Firecrawl. Si vous n'en avez pas encore, crÃ©ez un compte et rÃ©cupÃ©rez-la via https://www.firecrawl.dev/app/api-keys

AprÃ¨s lâ€™ajout, actualisez la liste des serveurs MCP pour voir les nouveaux outils. Le Composer Agent utilisera automatiquement Firecrawl MCP lorsque câ€™est pertinent, mais vous pouvez aussi le demander explicitement en dÃ©crivant vos besoins en web scraping. AccÃ©dez au Composer via Command+L (Mac), sÃ©lectionnez Â« Agent Â» Ã  cÃ´tÃ© du bouton dâ€™envoi, puis saisissez votre requÃªte.

### ExÃ©cuter sur Windsurf

Ajoutez ceci Ã  votre `./codeium/windsurf/model_config.json` :

```json
{
  "mcpServers": {
    "mcp-server-firecrawl": {
      "command": "npx",
      "args": ["-y", "firecrawl-mcp"],
      "env": {
        "FIRECRAWL_API_KEY": "VOTRE_API_KEY"
      }
    }
  }
}
```


### ExÃ©cution en mode HTTP diffusÃ©

Pour exÃ©cuter le serveur en local avec le transport HTTP diffusÃ© plutÃ´t que le transport stdio par dÃ©fautÂ :

```bash
env HTTP_STREAMABLE_SERVER=true FIRECRAWL_API_KEY=fc-VOTRE_CLE_API npx -y firecrawl-mcp
```

Utilisez lâ€™URLÂ : http://localhost:3000/v2/mcp ou https://mcp.firecrawl.dev/{FIRECRAWL_API_KEY}/v2/mcp


### Installation via Smithery (ancien)

Pour installer automatiquement Firecrawl pour Claude Desktop via [Smithery](https://smithery.ai/server/@mendableai/mcp-server-firecrawl)Â :

```bash
npx -y @smithery/cli install @mendableai/mcp-server-firecrawl --client claude
```


### ExÃ©cution dans VS Code

Pour une installation en un clic, cliquez sur lâ€™un des boutons ci-dessousâ€¦

[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square\&logo=visualstudiocode\&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=firecrawl\&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Firecrawl%20API%20Key%22%2C%22password%22%3Atrue%7D%5D\&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22firecrawl-mcp%22%5D%2C%22env%22%3A%7B%22FIRECRAWL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square\&logo=visualstudiocode\&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=firecrawl\&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Firecrawl%20API%20Key%22%2C%22password%22%3Atrue%7D%5D\&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22firecrawl-mcp%22%5D%2C%22env%22%3A%7B%22FIRECRAWL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D\&quality=insiders)

Pour une installation manuelle, ajoutez le bloc JSON suivant Ã  votre fichier User Settings (JSON) dans VS Code. Pour cela, appuyez sur `Ctrl + Shift + P` et tapez `Preferences: Open User Settings (JSON)`.

```json
{
  "mcp": {
    "inputs": [
      {
        "type": "promptString",
        "id": "apiKey",
        "description": "ClÃ© API Firecrawl",
        "password": true
      }
    ],
    "servers": {
      "firecrawl": {
        "command": "npx",
        "args": ["-y", "firecrawl-mcp"],
        "env": {
          "FIRECRAWL_API_KEY": "${input:apiKey}"
        }
      }
    }
  }
}
```

Vous pouvez aussi lâ€™ajouter Ã  un fichier nommÃ© `.vscode/mcp.json` dans votre espace de travail. Cela vous permettra de partager la configuration avec dâ€™autresÂ :

```json
{
  "inputs": [
    {
      "type": "promptString",
      "id": "apiKey",
      "description": "ClÃ© API Firecrawl",
      "password": true
    }
  ],
  "servers": {
    "firecrawl": {
      "command": "npx",
      "args": ["-y", "firecrawl-mcp"],
      "env": {
        "FIRECRAWL_API_KEY": "${input:apiKey}"
      }
    }
  }
}
```

**Remarque :** Certains utilisateurs ont signalÃ© des problÃ¨mes lors de lâ€™ajout du serveur MCP Ã  VS Code, dus Ã  la validation du JSON avec un format de schÃ©ma obsolÃ¨te ([microsoft/vscode#155379](https://github.com/microsoft/vscode/issues/155379)).
Cela affecte plusieurs outils MCP, dont Firecrawl.

**Solution de contournement :** DÃ©sactivez la validation JSON dans VS Code pour permettre le bon chargement du serveur MCP.\
Voir rÃ©fÃ©rence : [directus/directus#25906 (commentaire)](https://github.com/directus/directus/issues/25906#issuecomment-3369169513).

Le serveur MCP fonctionne toujours correctement lorsquâ€™il est appelÃ© via dâ€™autres extensions, mais le problÃ¨me survient spÃ©cifiquement lors de son enregistrement direct dans la liste des serveurs MCP. Nous publierons des consignes dÃ¨s que VS Code aura mis Ã  jour sa validation de schÃ©ma.


### ExÃ©cution sur Claude Desktop

Ajoutez ceci au fichier de configuration de ClaudeÂ :

```json
{
  "mcpServers": {
    "firecrawl": {
      "url": "https://mcp.firecrawl.dev/v2/mcp",
      "headers": {
        "Authorization": "Bearer VOTRE_CLE_API"
      }
    }
  }
}
```


### ExÃ©cution avec Claude Code

Ajoutez le serveur MCP de Firecrawl Ã  lâ€™aide de la CLI de Claude Code :

```bash
claude mcp add firecrawl -e FIRECRAWL_API_KEY=your-api-key -- npx -y firecrawl-mcp
```


### ExÃ©cution sur n8n

Pour connecter le serveur MCP de Firecrawl dans n8nÂ :

1. RÃ©cupÃ©rez votre clÃ© API Firecrawl depuis [https://firecrawl.dev/app/api-keys](https://www.firecrawl.dev/app/api-keys)
2. Dans votre workflow n8n, ajoutez un nÅ“ud **AI Agent**
3. Dans la configuration dâ€™AI Agent, ajoutez un nouveau **Tool**
4. SÃ©lectionnez **MCP Client Tool** comme type dâ€™outil
5. Saisissez lâ€™endpoint du serveur MCP (remplacez `{YOUR_FIRECRAWL_API_KEY}` par votre clÃ© API)Â :

```
  https://mcp.firecrawl.dev/{VOTRE_CLE_API_FIRECRAWL}/v2/mcp
```

6. DÃ©finissez **Server Transport** sur **HTTP Streamable**
7. DÃ©finissez **Authentication** sur **None**
8. Pour **Tools to include**, vous pouvez sÃ©lectionner **All**, **Selected** ou **All Except** â€” cela exposera les outils Firecrawl (scrape, crawl, map, search, extract, etc.)

Pour les dÃ©ploiements auto-hÃ©bergÃ©s, exÃ©cutez le serveur MCP avec npx et activez le mode de transport HTTPÂ :

```bash
env HTTP_STREAMABLE_SERVER=true \
    FIRECRAWL_API_KEY=fc-VOTRE_CLE_API \
    FIRECRAWL_API_URL=VOTRE_INSTANCE_FIRECRAWL \
    npx -y firecrawl-mcp
```

Cela dÃ©marrera le serveur sur `http://localhost:3000/v2/mcp`, que vous pourrez utiliser comme point de terminaison dans votre workflow n8n. La variable dâ€™environnement `HTTP_STREAMABLE_SERVER=true` est nÃ©cessaire, car n8n requiert le transport HTTP.


## Configuration

### Variables dâ€™environnement

#### Requis pour lâ€™API Cloud

- `FIRECRAWL_API_KEY` : votre clÃ© API Firecrawl
  - Requise lors de lâ€™utilisation de lâ€™API cloud (par dÃ©faut)
  - Facultative lors de lâ€™utilisation dâ€™une instance auto-hÃ©bergÃ©e avec `FIRECRAWL_API_URL`
- `FIRECRAWL_API_URL` (facultatif) : point de terminaison API personnalisÃ© pour les instances auto-hÃ©bergÃ©es
  - Exemple : `https://firecrawl.your-domain.com`
  - Si elle nâ€™est pas renseignÃ©e, lâ€™API cloud sera utilisÃ©e (nÃ©cessite une clÃ© API)

#### Configuration facultative

##### Configuration des relances

- `FIRECRAWL_RETRY_MAX_ATTEMPTS`: Nombre maximal de tentatives de relance (par dÃ©faut : 3)
- `FIRECRAWL_RETRY_INITIAL_DELAY`: DÃ©lai initial en millisecondes avant la premiÃ¨re relance (par dÃ©faut : 1000)
- `FIRECRAWL_RETRY_MAX_DELAY`: DÃ©lai maximal en millisecondes entre les relances (par dÃ©faut : 10000)
- `FIRECRAWL_RETRY_BACKOFF_FACTOR`: Facteur de backoff exponentiel (par dÃ©faut : 2)

##### Surveillance de lâ€™utilisation des crÃ©dits

- `FIRECRAWL_CREDIT_WARNING_THRESHOLD`: Seuil dâ€™avertissement pour lâ€™utilisation des crÃ©dits (par dÃ©fautÂ : 1000)
- `FIRECRAWL_CREDIT_CRITICAL_THRESHOLD`: Seuil critique pour lâ€™utilisation des crÃ©dits (par dÃ©fautÂ : 100)

### Exemples de configuration

Pour lâ€™utilisation de lâ€™API cloud avec des tentatives de reprise personnalisÃ©es et le suivi des crÃ©ditsÂ :

```bash
# Requis pour lâ€™API cloud
export FIRECRAWL_API_KEY=your-api-key

# ParamÃ¨tres de nouvelle tentative (facultatif)
export FIRECRAWL_RETRY_MAX_ATTEMPTS=5        # Augmenter le nombre maximal de tentatives
export FIRECRAWL_RETRY_INITIAL_DELAY=2000    # Commencer avec un dÃ©lai de 2 s
export FIRECRAWL_RETRY_MAX_DELAY=30000       # DÃ©lai maximal de 30 s
export FIRECRAWL_RETRY_BACKOFF_FACTOR=3      # Backoff plus agressif

# Surveillance des crÃ©dits (facultatif)
export FIRECRAWL_CREDIT_WARNING_THRESHOLD=2000    # Avertissement Ã  2000 crÃ©dits
export FIRECRAWL_CREDIT_CRITICAL_THRESHOLD=500    # Seuil critique Ã  500 crÃ©dits
```

Pour une instance autoâ€‘hÃ©bergÃ©eÂ :

```bash
# Requis pour lâ€™autoâ€‘hÃ©bergement
export FIRECRAWL_API_URL=https://firecrawl.your-domain.com

# Authentification facultative pour lâ€™autoâ€‘hÃ©bergement
export FIRECRAWL_API_KEY=your-api-key  # Si votre instance requiert une authentification

# Configuration personnalisÃ©e des tentatives
export FIRECRAWL_RETRY_MAX_ATTEMPTS=10
export FIRECRAWL_RETRY_INITIAL_DELAY=500     # DÃ©marrer avec des tentatives plus rapides
```


### Configuration personnalisÃ©e avec Claude Desktop

Ajoutez ceci Ã  votre fichier `claude_desktop_config.json` :

```json
{
  "mcpServers": {
    "mcp-server-firecrawl": {
      "command": "npx",
      "args": ["-y", "firecrawl-mcp"],
      "env": {
        "FIRECRAWL_API_KEY": "VOTRE_CLEF_API_ICI",

        "FIRECRAWL_RETRY_MAX_ATTEMPTS": "5",
        "FIRECRAWL_RETRY_INITIAL_DELAY": "2000",
        "FIRECRAWL_RETRY_MAX_DELAY": "30000",
        "FIRECRAWL_RETRY_BACKOFF_FACTOR": "3",

        "FIRECRAWL_CREDIT_WARNING_THRESHOLD": "2000",
        "FIRECRAWL_CREDIT_CRITICAL_THRESHOLD": "500"
      }
    }
  }
}
```


### Configuration du systÃ¨me

Le serveur propose plusieurs paramÃ¨tres configurables pouvant Ãªtre dÃ©finis via des variables dâ€™environnement. Voici les valeurs par dÃ©faut si rien nâ€™est spÃ©cifiÃ©Â :

```typescript
const CONFIG = {
  retry: {
    maxAttempts: 3, // Nombre de tentatives de rÃ©essai pour les requÃªtes limitÃ©es par le dÃ©bit
    initialDelay: 1000, // DÃ©lai initial avant le premier rÃ©essai (en millisecondes)
    maxDelay: 10000, // DÃ©lai maximal entre les rÃ©essais (en millisecondes)
    backoffFactor: 2, // Multiplicateur pour lâ€™exponentiel de backoff
  },
  credit: {
    warningThreshold: 1000, // Avertir lorsque lâ€™utilisation des crÃ©dits atteint ce seuil
    criticalThreshold: 100, // Alerte critique lorsque lâ€™utilisation des crÃ©dits atteint ce seuil
  },
};
```

Ces paramÃ¨tres contrÃ´lent :

1. **Comportement de relance**

   * Relance automatiquement les requÃªtes Ã©chouÃ©es en raison des limites de dÃ©bit
   * Utilise un retour arriÃ¨re exponentiel (exponential backoff) pour Ã©viter de surcharger lâ€™API
   * Exemple : avec les paramÃ¨tres par dÃ©faut, les relances seront tentÃ©es aux moments suivants :
     * 1re relance : dÃ©lai de 1 seconde
     * 2e relance : dÃ©lai de 2 secondes
     * 3e relance : dÃ©lai de 4 secondes (plafonnÃ© par maxDelay)

2. **Suivi de lâ€™utilisation des crÃ©dits**
   * Suit la consommation de crÃ©dits pour lâ€™utilisation de lâ€™API cloud
   * Ã‰met des avertissements Ã  des seuils dÃ©finis
   * Aide Ã  prÃ©venir les interruptions de service imprÃ©vues
   * Exemple : avec les paramÃ¨tres par dÃ©faut :
     * Avertissement Ã  1â€¯000 crÃ©dits restants
     * Alerte critique Ã  100 crÃ©dits restants


### Limitation de dÃ©bit et traitement par lots

Le serveur utilise les fonctionnalitÃ©s intÃ©grÃ©es de Firecrawl pour la limitation de dÃ©bit et le traitement par lotsÂ :

- Gestion automatique des limites de dÃ©bit avec backoff exponentiel
- Traitement parallÃ¨le efficace pour les opÃ©rations par lots
- Mise en file dâ€™attente et limitation intelligente des requÃªtes
- RÃ©essais automatiques en cas dâ€™erreurs transitoires

## Outils disponibles

### 1. Outil dâ€™extraction (`firecrawl_scrape`)

Extraire le contenu dâ€™une URL unique avec des options avancÃ©es.

```json
{
  "name": "firecrawl_scrape",
  "arguments": {
    "url": "https://example.com",
    "formats": ["markdown"],
    "onlyMainContent": true,
    "waitFor": 1000,
    "timeout": 30000,
    "mobile": false,
    "includeTags": ["article", "main"],
    "excludeTags": ["nav", "footer"],
    "skipTlsVerification": false
  }
}
```


### 2. Outil dâ€™extraction par lots (`firecrawl_batch_scrape`)

Exploitez plusieurs URL efficacement grÃ¢ce Ã  une limitation de dÃ©bit intÃ©grÃ©e et un traitement parallÃ¨le.

```json
{
  "name": "firecrawl_batch_scrape",
  "arguments": {
    "urls": ["https://example1.com", "https://example2.com"],
    "options": {
      "formats": ["markdown"],
      "onlyMainContent": true
    }
  }
}
```

La rÃ©ponse inclut lâ€™ID dâ€™opÃ©ration pour vÃ©rifier lâ€™Ã©tatÂ :

```json
{
  "content": [
    {
      "type": "text",
      "text": "OpÃ©ration par lots mise en file dâ€™attente avec lâ€™IDÂ : batch_1. Utilisez firecrawl_check_batch_status pour suivre la progression."
    }
  ],
  "isError": false
}
```


### 3. VÃ©rifier lâ€™Ã©tat du lot (`firecrawl_check_batch_status`)

VÃ©rifiez lâ€™Ã©tat dâ€™une opÃ©ration par lots.

```json
{
  "name": "firecrawl_check_batch_status",
  "arguments": {
    "id": "batch_1"
  }
}
```


### 4. Outil de cartographie (`firecrawl_map`)

Cartographiez un site web pour dÃ©couvrir toutes les URL indexÃ©es du site.

```json
{
  "name": "firecrawl_map",
  "arguments": {
    "url": "https://example.com",
    "search": "blog",
    "sitemap": "include",
    "includeSubdomains": false,
    "limit": 100,
    "ignoreQueryParameters": true
  }
}
```


#### Options de lâ€™outil MapÂ :

- `url`Â : URL de base du site Ã  cartographier
- `search`Â : Terme de recherche facultatif pour filtrer les URL
- `sitemap`Â : ContrÃ´le lâ€™utilisation du sitemap â€” Â«Â includeÂ Â», Â«Â skipÂ Â» ou Â«Â onlyÂ Â»
- `includeSubdomains`Â : Indique sâ€™il faut inclure les sous-domaines dans la cartographie
- `limit`Â : Nombre maximal dâ€™URL Ã  retourner
- `ignoreQueryParameters`Â : Indique sâ€™il faut ignorer les paramÃ¨tres de requÃªte lors de la cartographie

**IdÃ©al pourÂ :** DÃ©couvrir des URL sur un site avant de dÃ©cider quoi extraireÂ ; trouver des sections spÃ©cifiques dâ€™un site.
**RenvoieÂ :** Tableau dâ€™URL trouvÃ©es sur le site.

### 5. Outil de recherche (`firecrawl_search`)

Effectuez une recherche sur le web et, si besoin, extrayez le contenu des rÃ©sultats.

```json
{
  "name": "firecrawl_search",
  "arguments": {
    "query": "votre requÃªte de recherche",
    "limit": 5,
    "lang": "en",
    "country": "us",
    "scrapeOptions": {
      "formats": ["markdown"],
      "onlyMainContent": true
    }
  }
}
```


### 6. Outil dâ€™exploration (`firecrawl_crawl`)

Lancez une exploration asynchrone avec des options avancÃ©es.

```json
{
  "name": "firecrawl_crawl",
  "arguments": {
    "url": "https://example.com",
    "maxDepth": 2,
    "limit": 100,
    "allowExternalLinks": false,
    "deduplicateSimilarURLs": true
  }
}
```


### 7. VÃ©rifier lâ€™Ã©tat du crawl (`firecrawl_check_crawl_status`)

VÃ©rifiez lâ€™Ã©tat dâ€™un job de crawl.

```json
{
  "name": "firecrawl_check_crawl_status",
  "arguments": {
    "id": "550e8400-e29b-41d4-a716-446655440000"
  }
}
```

**RenvoieÂ :** Ã‰tat et avancement de la tÃ¢che de crawl, y compris les rÃ©sultats sâ€™ils sont disponibles.


### 8. Outil dâ€™extraction (`firecrawl_extract`)

Extrayez des informations structurÃ©es Ã  partir de pages web grÃ¢ce aux capacitÃ©s des LLM. Prend en charge lâ€™extraction via des IA cloud ou des LLM autoâ€‘hÃ©bergÃ©s.

```json
{
  "name": "firecrawl_extract",
  "arguments": {
    "urls": ["https://example.com/page1", "https://example.com/page2"],
    "prompt": "Extraire les informations produit incluant le nom, le prix et la description",
    "systemPrompt": "Vous Ãªtes un assistant utile qui extrait les informations produit",
    "schema": {
      "type": "object",
      "properties": {
        "name": { "type": "string" },
        "price": { "type": "number" },
        "description": { "type": "string" }
      },
      "required": ["name", "price"]
    },
    "allowExternalLinks": false,
    "enableWebSearch": false,
    "includeSubdomains": false
  }
}
```

Exemple de rÃ©ponseÂ :

```json
{
  "content": [
    {
      "type": "text",
      "text": {
        "name": "Produit d'exemple",
        "price": 99.99,
        "description": "Ceci est un exemple de description de produit"
      }
    }
  ],
  "isError": false
}
```


#### Options de lâ€™outil dâ€™extractionÂ :

- `urls`Â : Tableau dâ€™URL Ã  partir desquelles extraire des informations
- `prompt`Â : Invite personnalisÃ©e pour lâ€™extraction par le LLM
- `systemPrompt`Â : Invite systÃ¨me pour guider le LLM
- `schema`Â : SchÃ©ma JSON pour lâ€™extraction de donnÃ©es structurÃ©es
- `allowExternalLinks`Â : Autoriser lâ€™extraction Ã  partir de liens externes
- `enableWebSearch`Â : Activer la recherche web pour un contexte supplÃ©mentaire
- `includeSubdomains`Â : Inclure les sous-domaines dans lâ€™extraction

Avec une instance auto-hÃ©bergÃ©e, lâ€™extraction utilise votre LLM configurÃ©. Avec lâ€™API cloud, elle utilise le service LLM managÃ© de Firecrawl.

## SystÃ¨me de journalisation

Le serveur inclut une journalisation complÃ¨teÂ :

* Ã‰tat et progression des opÃ©rations
* Indicateurs de performance
* Suivi de lâ€™utilisation des crÃ©dits
* Suivi des limites de dÃ©bit
* Conditions dâ€™erreur

Exemples de messages de journalÂ :

```
[INFO] Serveur MCP Firecrawl initialisÃ© avec succÃ¨s
[INFO] Lancement du scraping pour lâ€™URLÂ : https://example.com
[INFO] OpÃ©ration par lots mise en file dâ€™attente avec lâ€™IDÂ : batch_1
[WARNING] Lâ€™utilisation des crÃ©dits a atteint le seuil dâ€™alerte
[ERROR] Limite de dÃ©bit dÃ©passÃ©e, nouvelle tentative dans 2Â sâ€¦
```


## Gestion des erreurs

Le serveur offre une gestion des erreurs robuste :

* RÃ©essais automatiques pour les erreurs transitoires
* Gestion des limites de dÃ©bit avec backoff
* Messages dâ€™erreur dÃ©taillÃ©s
* Avertissements sur lâ€™utilisation des crÃ©dits
* RÃ©silience rÃ©seau

Exemple de rÃ©ponse dâ€™erreur :

```json
{
  "content": [
    {
      "type": "text",
      "text": "ErreurÂ : limite de dÃ©bit dÃ©passÃ©e. Nouvelle tentative dans 2Â secondesâ€¦"
    }
  ],
  "isError": true
}
```


## DÃ©veloppement

```bash
# Installer les dÃ©pendances
npm install

# GÃ©nÃ©rer le build
npm run build

# Lancer les tests
npm test
```


### Contribuer

1. Forkez le dÃ©pÃ´t
2. CrÃ©ez une branche pour votre fonctionnalitÃ©
3. ExÃ©cutez les tests : `npm test`
4. Ouvrez une pull request

### Merci aux contributeurs

Merci Ã  [@vrknetha](https://github.com/vrknetha) et [@cawstudios](https://caw.tech) pour la mise en Å“uvre initialeâ€¯!

Merci Ã  MCP.so et Klavis AI pour lâ€™hÃ©bergement, ainsi quâ€™Ã  [@gstarwd](https://github.com/gstarwd), [@xiangkaiz](https://github.com/xiangkaiz) et [@zihaolin96](https://github.com/zihaolin96) dâ€™avoir intÃ©grÃ© notre serveur.

## Licence

Licence MIT â€” voir le fichier LICENSE pour plus de dÃ©tails