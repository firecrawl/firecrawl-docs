---
title: "Guide de scraping avancé"
description: "Apprenez à optimiser votre scraping Firecrawl grâce aux options avancées."
og:title: "Guide de scraping avancé | Firecrawl"
og:description: "Apprenez à optimiser votre scraping Firecrawl grâce aux options avancées."
---

Ce guide présente les différents points de terminaison de Firecrawl et explique comment les exploiter pleinement avec l’ensemble de leurs paramètres.

<div id="basic-scraping-with-firecrawl-scrape">
  ## Scraping simple avec Firecrawl (/scrape)
</div>

Pour extraire le contenu propre d’une page unique au format Markdown, utilisez le point de terminaison `/scrape`.

<CodeGroup>

```python Python
# pip install firecrawl-py

from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR_API_KEY")

content = app.scrape_url("https://docs.firecrawl.dev")
```

```JavaScript JavaScript
// npm install @mendable/firecrawl-js

import { FirecrawlApp } from 'firecrawl-js';

const app = new FirecrawlApp({ apiKey: 'YOUR_API_KEY' });

const content = await app.scrapeUrl('https://docs.firecrawl.dev');
```

```go Go
// go get github.com/firecrawl/firecrawl-go

import (
  "fmt"
  "log"

  "github.com/firecrawl/firecrawl-go"
)

func main() {
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
    log.Fatalf("Failed to initialize FirecrawlApp: %v", err)
  }

  content, err := app.ScrapeURL("docs.firecrawl.dev", nil)
  if err != nil {
    log.Fatalf("Failed")
  }
}
```

```rust Rust
// Installez la crate firecrawl_rs avec Cargo

use firecrawl_rs::FirecrawlApp;
#[tokio::main]
async fn main() {
  // Initialisez FirecrawlApp avec la clé API
  let api_key = "YOUR_API_KEY";
  let api_url = "https://api.firecrawl.dev";
  let app = FirecrawlApp::new(api_key, api_url).expect("Failed to initialize FirecrawlApp");

  let scrape_result = app.scrape_url("https://docs.firecrawl.dev", None).await;
  match scrape_result {
    Ok(data) => println!("Résultat du scraping:\n{}", data["markdown"]),
    Err(e) => eprintln!("Échec du scraping: {}", e),
  }
}
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

</CodeGroup>

<div id="scraping-pdfs">
  ## Extraction des PDF
</div>

**Firecrawl prend en charge l’extraction des PDF par défaut.** Vous pouvez utiliser le point de terminaison `/scrape` pour extraire un lien PDF et obtenir le contenu texte du PDF. Vous pouvez désactiver cette option en définissant `pageOptions.parsePDF` sur `false`.

<div id="page-options">
  ## Options de page
</div>

Lors de l’utilisation du point de terminaison `/scrape`, vous pouvez personnaliser le comportement d’extraction avec le paramètre `pageOptions`. Voici les options disponibles :

<div id="getting-cleaner-content-with-onlymaincontent">
  ### Obtenir un contenu plus propre avec `onlyMainContent`
</div>

- **Type**: `boolean`
- **Description**: Renvoie uniquement le contenu principal de la page, en excluant les en-têtes, les barres de navigation, les pieds de page, etc.
- **Valeur par défaut**: `false`

<div id="getting-the-html-with-includehtml">
  ### Obtenir le HTML avec `includeHtml`
</div>

- **Type**: `boolean`
- **Description**: Inclut la version HTML du contenu de la page. Cela ajoutera une clé `html` dans la réponse.
- **Valeur par défaut**: `false`

<div id="getting-the-raw-html-with-includerawhtml">
  ### Récupérer le HTML brut avec `includeRawHtml`
</div>

- **Type**: `boolean`
- **Description**: Inclut le contenu HTML brut de la page. Cela ajoutera une clé `rawHtml` dans la réponse.
- **Valeur par défaut**: `false`

<div id="getting-a-screenshot-of-the-page-with-screenshot">
  ### Obtenir une capture d’écran de la page avec `screenshot`
</div>

- **Type**: `boolean`
- **Description**: Inclut une capture de la partie supérieure de la page que vous êtes en train de scraper.
- **Par défaut**: `false`

<div id="waiting-for-the-page-to-load-with-waitfor">
  ### Attendre le chargement de la page avec `waitFor`
</div>

- **Type**: `integer`
- **Description**: À n’utiliser qu’en dernier recours. Attend un nombre de millisecondes donné avant de récupérer le contenu, le temps que la page se charge.
- **Default**: `0`

<div id="example-usage">
  ### Exemple d’utilisation
</div>

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "pageOptions": {
        "onlyMainContent": true,
        "includeHtml": true,
        "includeRawHtml":true,
        "screenshot": true,
        "waitFor": 5000
      }
    }'
```

Dans cet exemple, le scraper va :

* Ne renvoyer que le contenu principal de la page.
* Inclure le HTML brut dans la réponse sous la clé `html`.
* Attendre 5000 millisecondes (5 secondes) que la page se charge avant de récupérer le contenu.

Voici la référence API correspondante : [Documentation du point de terminaison /scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape)

<div id="extractor-options">
  ## Options de l’extracteur
</div>

Lorsque vous utilisez le point de terminaison `/scrape`, vous pouvez définir des options pour **extraire des informations structurées** à partir du contenu de la page via le paramètre `extractorOptions`. Voici les options disponibles :

### mode

- **Type**: `string`
- **Enum**: `["llm-extraction", "llm-extraction-from-raw-html"]`
- **Description**: Le mode d’extraction à utiliser.

  - `llm-extraction` : Extrait des informations à partir du contenu nettoyé et parsé.
  - `llm-extraction-from-raw-html` : Extrait des informations directement à partir du HTML brut.

- **Type**: `string`
- **Description**: Un prompt décrivant les informations à extraire de la page.

<div id="extractionschema">
  ### extractionSchema
</div>

- **Type**: `object`
- **Description**: Schéma des données à extraire, définissant la structure des données extraites.

### Exemple d’usage

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev/",
      "extractorOptions": {
        "mode": "llm-extraction",
        "extractionPrompt": "D’après les informations de la page, extrayez les données selon le schéma.",
        "extractionSchema": {
          "type": "object",
          "properties": {
            "company_mission": {
                      "type": "string"
            },
            "supports_sso": {
                      "type": "boolean"
            },
            "is_open_source": {
                      "type": "boolean"
            },
            "is_in_yc": {
                      "type": "boolean"
            }
          },
          "required": [
            "company_mission",
            "supports_sso",
            "is_open_source",
            "is_in_yc"
          ]
        }
      }
    }'
```

```json
{
  "success": true,
  "data": {
    "content": "Contenu brut",
    "metadata": {
      "title": "Mendable",
      "description": "Mendable vous permet de créer facilement des applications de chat IA. Ingérez, personnalisez, puis déployez n’importe où avec une seule ligne de code. Proposé par SideGuide",
      "robots": "suivre, indexer",
      "ogTitle": "Mendable",
      "ogDescription": "Mendable vous permet de créer facilement des applications de chat IA. Ingérez, personnalisez, puis déployez n’importe où avec une seule ligne de code. Proposé par SideGuide",
      "ogUrl": "https://docs.firecrawl.dev/",
      "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",
      "ogLocaleAlternate": [],
      "ogSiteName": "Mendable",
      "sourceURL": "https://docs.firecrawl.dev/"
    },
    "llm_extraction": {
      "company_mission": "Entraînez une IA sécurisée sur vos ressources techniques afin qu’elle réponde aux questions des clients et des employés, pour que votre équipe n’ait pas à le faire"
      "supports_sso": true,
      "is_open_source": false,
      "is_in_yc": true
    }
  }
}
```

<div id="adjusting-timeout">
  ## Ajuster le délai d’attente
</div>

Vous pouvez régler le délai d’attente du processus de scraping à l’aide du paramètre `timeout`, en millisecondes.

<div id="example-usage">
  ### Exemple d’utilisation
</div>

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer VOTRE_CLE_API' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "timeout": 50000
    }'
```

<div id="crawling-multiple-pages">
  ## Explorer plusieurs pages
</div>

Pour explorer plusieurs pages, vous pouvez utiliser le point de terminaison `/crawl`. Ce point de terminaison vous permet de spécifier une URL de base à explorer, et toutes les sous-pages accessibles seront parcourues.

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer VOTRE_CLÉ_API' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

Renvoie un jobId

```json
{ "jobId": "1234-5678-9101" }
```

<div id="check-crawl-job">
  ### Vérifier une tâche de crawl
</div>

Permet de vérifier l’état d’une tâche de crawl et d’en récupérer le résultat.

```bash
curl -X GET https://api.firecrawl.dev/v0/crawl/status/1234-5678-9101 \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer VOTRE_CLÉ_API'
```

<div id="crawler-options">
  ### Options du crawler
</div>

Lors de l’utilisation du point de terminaison `/crawl`, vous pouvez personnaliser le comportement d’exploration à l’aide du paramètre `crawlerOptions`. Voici les options disponibles :

<div id="includes">
  #### `includes`
</div>

- **Type**: `array`
- **Description**: Modèles d’URL à inclure dans l’exploration. Seules les URL correspondant à ces modèles seront explorées.
- **Example**: `["/blog/*", "/products/*"]`

<div id="excludes">
  #### `excludes`
</div>

- **Type**: `array`
- **Description**: Modèles d’URL à exclure du crawl. Les URL correspondant à ces modèles seront ignorées.
- **Example**: `["/admin/*", "/login/*"]`

<div id="returnonlyurls">
  #### `returnOnlyUrls`
</div>

- **Type**: `boolean`
- **Description**: Si la valeur est `true`, la réponse contiendra uniquement une liste d’URL, au lieu des données complètes du document.
- **Default**: `false`

<div id="maxdepth">
  #### `maxDepth`
</div>

- **Type**: `integer`
- **Description**: Profondeur maximale d’exploration par rapport à l’URL fournie. Un maxDepth de 0 ne traite que l’URL fournie. Un maxDepth de 1 traite l’URL fournie et toutes les pages à un niveau de profondeur. Un maxDepth de 2 traite l’URL fournie et toutes les pages jusqu’à deux niveaux de profondeur. Des valeurs plus élevées suivent le même principe.
- **Example**: `2`

<div id="mode">
  #### `mode`
</div>

- **Type**: `string`
- **Enum**: `["default", "fast"]`
- **Description**: Le mode de crawl à utiliser. Le mode `fast` explore les sites sans sitemap jusqu’à 4× plus vite, mais peut être moins précis et n’est pas recommandé pour les sites fortement rendus côté JavaScript.
- **Default**: `default`

<div id="limit">
  #### `limit`
</div>

- **Type**: `integer`
- **Description**: Nombre maximal de pages à explorer.
- **Default**: `10000`

### Exemple d’usage

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer VOTRE_CLÉ_API' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "crawlerOptions": {
        "includes": ["/blog/*", "/products/*"],
        "excludes": ["/admin/*", "/login/*"],
        "returnOnlyUrls": false,
        "maxDepth": 2,
        "mode": "fast",
        "limit": 1000
      }
    }'
```

Dans cet exemple, le crawler va :

* Ne parcourir que les URL correspondant aux modèles `/blog/*` et `/products/*`.
* Ignorer les URL correspondant aux modèles `/admin/*` et `/login/*`.
* Renvoyer les données complètes du document pour chaque page.
* Explorer jusqu’à une profondeur maximale de 2.
* Utiliser le mode d’exploration rapide.
* Parcourir au maximum 1000 pages.

<div id="page-options-crawler-options">
  ## Options de page + Options du crawler
</div>

Vous pouvez combiner les paramètres `pageOptions` et `crawlerOptions` pour personnaliser l’ensemble du comportement d’exploration.

<div id="example-usage">
  ### Exemple d’utilisation
</div>

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "pageOptions": {
        "onlyMainContent": true,
        "includeHtml": true,
        "includeRawHtml": true,
        "screenshot": true,
        "waitFor": 5000
      },
      "crawlerOptions": {
        "includes": ["/blog/*", "/products/*"],
        "maxDepth": 2,
        "mode": "fast",
      }
    }'
```

Dans cet exemple, le crawler va :

* Ne renvoyer que le contenu principal de chaque page.
* Inclure le HTML brut de chaque page.
* Attendre 5 000 millisecondes que chaque page se charge avant d’en récupérer le contenu.
* N’explorer que les URL correspondant aux motifs `/blog/*` et `/products/*`.
* Explorer jusqu’à une profondeur maximale de 2.
* Utiliser le mode d’exploration rapide.

<div id="extractor-options-crawler-options">
  ## Options de l’extracteur + options du crawler
</div>

À venir...