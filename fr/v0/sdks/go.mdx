---
title: 'Go'
description: 'Le SDK Go de Firecrawl est un wrapper autour de l’API Firecrawl pour vous aider à convertir facilement des sites web en markdown.'
icon: 'golang'
og:title: "SDK Go | Firecrawl"
og:description: "Le SDK Go de Firecrawl est un wrapper autour de l’API Firecrawl pour vous aider à convertir facilement des sites web en markdown."
---

> Remarque : ceci utilise la [version v0 de l’API Firecrawl](/fr/v0/introduction), en cours d’abandon. Nous recommandons de passer à la [v1](/fr/sdks/go).

<div id="installation">
  ## Installation
</div>

Pour installer le SDK Go de Firecrawl, utilisez go get :

```bash
go get github.com/firecrawl/firecrawl-go
```

<div id="usage">
  ## Utilisation
</div>

1. Récupérez une clé d’API sur [firecrawl.dev](https://firecrawl.dev)
2. Définissez la clé d’API comme variable d’environnement nommée `FIRECRAWL_API_KEY` ou transmettez-la comme paramètre à la structure `FirecrawlApp`.

Voici un exemple d’utilisation du SDK avec gestion des erreurs :

```go
import (
  "fmt"
  "log"

  "github.com/firecrawl/firecrawl-go"
)

func main() {
  // Initialisez FirecrawlApp avec votre clé API
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
    log.Fatalf("Échec de l’initialisation de FirecrawlApp : %v", err)
  }

  // Récupérer une seule URL
  scrapedData, err := app.ScrapeURL("docs.firecrawl.dev", nil)
  if err != nil {
    log.Fatalf("Erreur lors de la récupération : %v", err)
  }
  fmt.Println(scrapedData)

  // Explorer un site web
  params := map[string]any{
    "pageOptions": map[string]any{
      "onlyMainContent": true,
    },
  }

  crawlResult, err := app.CrawlURL("docs.firecrawl.dev", params)
  if err != nil {
    log.Fatalf("Erreur lors de l’exploration : %v", err)
  }
  fmt.Println(crawlResult)
}
```

<div id="scraping-a-url">
  ### Extraction d’une URL
</div>

Pour extraire une URL unique avec gestion des erreurs, utilisez la méthode `ScrapeURL`. Elle prend l’URL en paramètre et renvoie les données extraites sous forme de dictionnaire.

```go
scrapedData, err := app.ScrapeURL("docs.firecrawl.dev", nil)
if err != nil {
  log.Fatalf("Échec de l’extraction de l’URL : %v", err)
}
fmt.Println(scrapedData)
```

<div id="crawling-a-website">
  ### Explorer un site web
</div>

Pour explorer un site web, utilisez la méthode `CrawlUrl`. Elle prend l’URL de départ et des paramètres optionnels en arguments. L’argument `params` permet de définir des options supplémentaires pour la tâche d’exploration, comme le nombre maximal de pages à parcourir, les domaines autorisés et le format de sortie.

```go
crawlParams := map[string]any{
  "crawlerOptions": map[string]any{
    "excludes": []string{"blog/*"},
    "includes": []string{}, // laisser vide pour inclure toutes les pages
    "limit": 1000,
  },
  "pageOptions": map[string]any{
    "onlyMainContent": true,
  },
}
crawlResult, err := app.CrawlURL("docs.firecrawl.dev", crawlParams, true, 2, idempotencyKey)
if err != nil {
  log.Fatalf("Échec de l'exploration de l’URL : %v", err)
}
fmt.Println(crawlResult)
```

<div id="checking-crawl-status">
  ### Vérifier l’état d’un crawl
</div>

Pour vérifier l’état d’une tâche de crawl, utilisez la méthode `CheckCrawlStatus`. Elle prend l’ID de la tâche en paramètre et renvoie l’état actuel du crawl.

```go
status, err := app.CheckCrawlStatus(jobId)
if err != nil {
  log.Fatalf("Échec de la vérification de l’état du crawl : %v", err)
}
fmt.Println(status)
```

<div id="canceling-a-crawl-job">
  ### Annulation d’un job de crawl
</div>

Pour annuler un job de crawl, utilisez la méthode `CancelCrawlJob`. Elle prend l’ID du job en paramètre et renvoie l’état d’annulation du job de crawl.

```go
canceled, err := app.CancelCrawlJob(jobId)
if err != nil {
  log.Fatalf("Échec de l’annulation de la tâche de crawl : %v", err)
}
fmt.Println(canceled)
```

<div id="extracting-structured-data-from-a-url">
  ### Extraction de données structurées à partir d’une URL
</div>

Avec l’extraction via LLM, vous pouvez facilement extraire des données structurées depuis n’importe quelle URL. Voici comment l’utiliser :

```go
jsonSchema := map[string]any{
  "type": "object",
  "properties": map[string]any{
    "top": map[string]any{
      "type": "array",
      "items": map[string]any{
        "type": "object",
        "properties": map[string]any{
          "title":       map[string]string{"type": "string"},
          "points":      map[string]string{"type": "number"},
          "by":          map[string]string{"type": "string"},
          "commentsURL": map[string]string{"type": "string"},
        },
        "required": []string{"title", "points", "by", "commentsURL"},
      },
      "minItems":    5,
      "maxItems":    5,
      "description": "Les 5 meilleures actus sur Hacker News",
    },
  },
  "required": []string{"top"},
}

llmExtractionParams := map[string]any{
  "extractorOptions": firecrawl.ExtractorOptions{
    ExtractionSchema: jsonSchema,
  },
}

scrapeResult, err := app.ScrapeURL("https://news.ycombinator.com", llmExtractionParams)
if err != nil {
  log.Fatalf("Échec de l’extraction par LLM : %v", err)
}
fmt.Println(scrapeResult)
```

<div id="search-for-a-query">
  ### Rechercher une requête
</div>

Pour lancer une recherche web, obtenir les résultats les plus pertinents, scraper chaque page et renvoyer le markdown, utilisez la méthode `Search`. Cette méthode prend la requête en paramètre et renvoie les résultats de recherche.

```go
query := "Qu’est-ce que Firecrawl ?"
searchResult, err := app.Search(query)
if err != nil {
  log.Fatalf("Échec de la recherche : %v", err)
}
fmt.Println(searchResult)
```

<div id="error-handling">
  ## Gestion des erreurs
</div>

Le SDK gère les erreurs renvoyées par l’API Firecrawl et déclenche des exceptions adaptées. En cas d’erreur lors d’une requête, une exception est levée avec un message explicite.