---
title: 'Python'
description: 'Le SDK Python de Firecrawl est un SDK d’interface pour l’API Firecrawl, conçu pour transformer facilement des sites web en Markdown.'
icon: 'python'
og:title: "Python SDK | Firecrawl"
og:description: "Le SDK Python de Firecrawl est un SDK d’interface pour l’API Firecrawl, conçu pour transformer facilement des sites web en Markdown."
---

> Remarque : ceci utilise la [version v0 de l’API Firecrawl](/fr/v0/introduction), en cours de dépréciation. Nous recommandons de passer à [v1](/fr/sdks/python).

<div id="installation">
  ## Installation
</div>

Pour installer le SDK Python Firecrawl, utilisez pip :

```bash
pip install firecrawl-py==0.0.16
```

<div id="usage">
  ## Utilisation
</div>

1. Récupérez une clé API sur [firecrawl.dev](https://firecrawl.dev)
2. Définissez la clé API comme variable d’environnement nommée `FIRECRAWL_API_KEY` ou transmettez-la en paramètre à la classe `FirecrawlApp`.

Voici un exemple d’utilisation du SDK :

```python
from firecrawl import FirecrawlApp

# Initialisez FirecrawlApp avec votre clé d’API
app = FirecrawlApp(api_key='your_api_key')

# Extraire une seule URL
url = 'https://docs.firecrawl.dev'
scraped_data = app.scrape_url(url)

# Explorer un site web
crawl_url = 'https://docs.firecrawl.dev'
params = {
    'pageOptions': {
        'onlyMainContent': True
    }
}
crawl_result = app.crawl_url(crawl_url, params=params)
```

<div id="scraping-a-url">
  ### Extraction d’une URL
</div>

Pour extraire une URL unique, utilisez la méthode `scrape_url`. Elle prend l’URL en paramètre et renvoie les données extraites sous forme de dictionnaire.

```python
url = 'https://example.com'
donnees_scrapees = app.scrape_url(url)
```

### Extraire des données structurées à partir d&#39;une URL

Avec l’extraction via LLM, vous pouvez facilement extraire des données structurées à partir de n’importe quelle URL. Nous prenons en charge les schémas Pydantic pour vous simplifier la vie. Voici comment l’utiliser :

```python
class ArticleSchema(BaseModel):
    title: str
    points: int 
    by: str
    commentsURL: str

class TopArticlesSchema(BaseModel):
    top: List[ArticleSchema] = Field(..., max_items=5, description="Top 5 articles")

data = app.scrape_url('https://news.ycombinator.com', {
    'extractorOptions': {
        'extractionSchema': TopArticlesSchema.model_json_schema(),
        'mode': 'llm-extraction'
    },
    'pageOptions':{
        'onlyMainContent': True
    }
})
print(data["llm_extraction"])
```

<div id="crawling-a-website">
  ### Exploration d’un site web
</div>

Pour explorer un site web, utilisez la méthode `crawl_url`. Elle prend en arguments l’URL de départ et des paramètres optionnels. L’argument `params` vous permet de spécifier des options supplémentaires pour la tâche d’exploration, comme le nombre maximal de pages à explorer, les domaines autorisés et le format de sortie.

Le paramètre `wait_until_done` détermine si la méthode doit attendre la fin de la tâche d’exploration avant de renvoyer le résultat. S’il est défini sur `True`, la méthode vérifiera périodiquement l’état de la tâche d’exploration jusqu’à son achèvement ou jusqu’à ce que le `timeout` spécifié (en secondes) soit atteint. S’il est défini sur `False`, la méthode renverra immédiatement l’ID de la tâche, et vous pourrez vérifier manuellement l’état de la tâche d’exploration à l’aide de la méthode `check_crawl_status`.

```python
crawl_url = 'https://example.com'
params = {
    'crawlerOptions': {
        'excludes': ['blog/*'],
        'includes': [], # laisser vide pour inclure toutes les pages
        'limit': 1000,
    },
    'pageOptions': {
        'onlyMainContent': True
    }
}
crawl_result = app.crawl_url(crawl_url, params=params, wait_until_done=True, timeout=5)
```

Si `wait_until_done` est défini sur `True`, la méthode `crawl_url` renverra le résultat de l’exploration une fois le travail terminé. Si le travail échoue ou est interrompu, une exception sera levée.

<div id="checking-crawl-status">
  ### Vérifier l’état d’un crawl
</div>

Pour consulter l’état d’une tâche de crawl, utilisez la méthode `check_crawl_status`. Elle prend l’ID de la tâche en paramètre et renvoie l’état actuel du crawl.

```python
job_id = crawl_result['jobId']
status = app.check_crawl_status(job_id)
```

<div id="search-for-a-query">
  ### Rechercher une requête
</div>

Permet de rechercher sur le web, d’obtenir les résultats les plus pertinents, d’extraire chaque page et de renvoyer le markdown.

```python
query = 'qu’est-ce que Mendable ?'
search_result = app.search(query)
```

<div id="error-handling">
  ## Gestion des erreurs
</div>

Le SDK gère les erreurs renvoyées par l’API Firecrawl et déclenche des exceptions adaptées. En cas d’erreur lors d’une requête, une exception est levée avec un message explicite.