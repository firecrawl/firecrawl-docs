---
title: Bienvenue dans la V1
description: "Firecrawl vous permet de transformer des sites web entiers en Markdown prêt pour les LLM"
og:title: "Bienvenue dans la V1 | Firecrawl"
og:description: "Firecrawl vous permet de transformer des sites web entiers en Markdown prêt pour les LLM"
---

import InstallationPython from "/snippets/fr/v1/installation/python.mdx";
import InstallationNode from "/snippets/fr/v1/installation/js.mdx";
import InstallationGo from "/snippets/fr/v1/installation/go.mdx";
import InstallationRust from "/snippets/fr/v1/installation/rust.mdx";
import ScrapePython from "/snippets/fr/v1/scrape/base/python.mdx";
import ScrapeNode from "/snippets/fr/v1/scrape/base/js.mdx";
import ScrapeGo from "/snippets/fr/v1/scrape/base/go.mdx";
import ScrapeRust from "/snippets/fr/v1/scrape/base/rust.mdx";
import ScrapeCURL from "/snippets/fr/v1/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/fr/v1/scrape/base/output.mdx";
import MapPython from "/snippets/fr/v1/map/base/python.mdx";
import MapJavaScript from "/snippets/fr/v1/map/base/js.mdx";
import MapGo from "/snippets/fr/v1/map/base/go.mdx";
import MapRust from "/snippets/fr/v1/map/base/rust.mdx";
import MapCURL from "/snippets/fr/v1/map/base/curl.mdx";
import MapResponse from "/snippets/fr/v1/map/base/output.mdx";
import CrawlWebSocketPythonBase from "/snippets/fr/v1/crawl-websocket/base/python.mdx";
import CrawlWebSocketNodeBase from "/snippets/fr/v1/crawl-websocket/base/js.mdx";
import ExtractCURL from "/snippets/fr/v1/llm-extract/base/curl.mdx";
import ExtractPython from "/snippets/fr/v1/llm-extract/base/python.mdx";
import ExtractNode from "/snippets/fr/v1/llm-extract/base/js.mdx";
import ExtractOutput from "/snippets/fr/v1/llm-extract/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/fr/v1/llm-extract/no-schema/curl.mdx";
import ExtractNoSchemaOutput from "/snippets/fr/v1/llm-extract/no-schema/output.mdx";
import CrawlWebhookCURL from "/snippets/fr/v1/crawl-webhook/base/curl.mdx";

Firecrawl V1 est là ! Nous présentons une API plus fiable et plus conviviale pour les développeurs.

Voici les nouveautés :

* Formats de sortie pour `/scrape`. Choisissez les formats dans lesquels vous souhaitez votre résultat.
* Nouveau [point de terminaison `/map`](/fr/features/map) pour récupérer la plupart des URL d’une page web.
* API conviviale pour les développeurs pour l’état de `/crawl/{id}`.
* Limites de débit doublées pour toutes les offres.
* [SDK Go](/fr/sdks/go) et [SDK Rust](/fr/sdks/rust)
* Prise en charge des équipes
* Gestion des clés API dans le tableau de bord.
* `onlyMainContent` est désormais `true` par défaut.
* Prise en charge des webhooks et des websockets pour `/crawl`.

<div id="scrape-formats">
  ## Formats de scrape
</div>

Vous pouvez désormais choisir les formats de sortie souhaités. Il est possible d’en spécifier plusieurs. Les formats pris en charge sont :

* Markdown (markdown)
* HTML (html)
* HTML brut (rawHtml) (sans modification)
* Capture d’écran (screenshot ou screenshot@fullPage)
* Liens (links)
* JSON (json) — sortie structurée

Les clés de sortie correspondront au format choisi.

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeGo />

  <ScrapeRust />

  <ScrapeCURL />
</CodeGroup>

<div id="response">
  ### Réponse
</div>

Les SDK renverront directement l’objet de données. cURL renverra le payload exactement comme ci-dessous.

<ScrapeResponse />

<div id="introducing-map-alpha">
  ## Présentation de /map (Alpha)
</div>

La manière la plus simple de transformer une URL unique en une cartographie de l’ensemble du site.

<div id="usage">
  ### Utilisation
</div>

<CodeGroup>
  <MapPython />

  <MapJavaScript />

  <MapGo />

  <MapRust />

  <MapCURL />
</CodeGroup>

<div id="response">
  ### Réponse
</div>

Les SDK renvoient directement l’objet de données. cURL renvoie la charge utile exactement comme indiqué ci-dessous.

<MapResponse />

<div id="websockets">
  ## WebSockets
</div>

Pour explorer un site web via WebSockets, utilisez la méthode `Crawl URL and Watch`.

<CodeGroup>
  <CrawlWebSocketPythonBase />

  <CrawlWebSocketNodeBase />
</CodeGroup>

<div id="json-format">
  ## Format JSON
</div>

L’extraction via LLM est désormais disponible en v1 dans le format `json`. Pour extraire des données structurées depuis une page, vous pouvez fournir un schéma au point de terminaison ou simplement donner un prompt.

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

Résultat :

<ExtractOutput />

<div id="extracting-without-schema-new">
  ### Extraction sans schéma (Nouveau)
</div>

Vous pouvez désormais extraire sans schéma en passant simplement un `prompt` au point de terminaison. Le LLM détermine la structure des données.

<CodeGroup>
  <ExtractNoSchemaCURL />
</CodeGroup>

Résultat :

<ExtractNoSchemaOutput />

<div id="new-crawl-webhook">
  ## Nouveau webhook de crawl
</div>

Vous pouvez désormais passer un paramètre `webhook` au point de terminaison `/crawl`. Cela enverra une requête POST à l’URL que vous indiquez lorsque le crawl est lancé, mis à jour et terminé.

Le webhook se déclenchera désormais pour chaque page explorée, et plus seulement pour le résultat global à la fin.

<CrawlWebhookCURL />

<div id="webhook-events">
  ### Événements de webhook
</div>

Il existe désormais 4 types d’événements :

* `crawl.started` - Déclenché au démarrage du crawl.
* `crawl.page` - Déclenché pour chaque page parcourue.
* `crawl.completed` - Déclenché à la fin du crawl pour indiquer qu’il est terminé.
* `crawl.failed` - Déclenché lorsque le crawl échoue.

<div id="webhook-response">
  ### Réponse du webhook
</div>

* `success` - Indique si le webhook a réussi à explorer correctement la page.
* `type` - Le type d’événement survenu.
* `id` - L’identifiant de l’exploration.
* `data` - Les données extraites (Array). Ce champ n’est renseigné que pour `crawl.page` et contiendra 1 élément si la page a été extraite avec succès. La réponse est identique à celle du point de terminaison `/scrape`.
* `error` - En cas d’échec du webhook, contient le message d’erreur.

<div id="migrating-from-v0">
  ## Migration depuis V0
</div>

> ⚠️ **Avis d’obsolescence** : Les points de terminaison V0 seront abandonnés le 1er avril 2025. Veuillez migrer vers les points de terminaison V1 d’ici là afin de garantir la continuité du service.

<div id="scrape-endpoint">
  ## point de terminaison /scrape
</div>

Le point de terminaison `/scrape` a été repensé pour offrir une fiabilité renforcée et une utilisation plus simple. La structure du nouveau corps de requête pour `/scrape` est la suivante :

```json
{
  "url": "<string>",
  "formats": ["markdown", "html", "rawHtml", "links", "screenshot", "json"],
  "includeTags": ["<string>"],
  "excludeTags": ["<string>"],
  "headers": { "<key>": "<value>" },
  "waitFor": 123,
  "timeout": 123
}
```

<div id="formats">
  ### Formats
</div>

Vous pouvez désormais choisir les formats de sortie souhaités. Vous pouvez spécifier plusieurs formats. Les formats pris en charge sont :

* Markdown (markdown)
* HTML (html)
* HTML brut (rawHtml) (sans modifications)
* Capture d’écran (screenshot ou screenshot@fullPage)
* Liens (links)
* JSON (json)

Par défaut, seule la sortie au format markdown est incluse.

<div id="details-on-the-new-request-body">
  ### Détails sur le nouveau corps de requête
</div>

Le tableau ci-dessous présente les modifications des paramètres du corps de requête pour le point de terminaison `/scrape` en V1.

| Paramètre | Changement | Description |
| --------- | ------ | ----------- |
| `onlyIncludeTags` | Déplacé et renommé | Déplacé au niveau racine et renommé `includeTags`. |
| `removeTags` | Déplacé et renommé | Déplacé au niveau racine et renommé `excludeTags`. |
| `onlyMainContent`| Déplacé | Déplacé au niveau racine. `true` par défaut. |
| `waitFor`| Déplacé | Déplacé au niveau racine. |
| `headers`| Déplacé | Déplacé au niveau racine. |
| `parsePDF`| Déplacé | Déplacé au niveau racine. |
| `extractorOptions`| Aucun changement ||
| `timeout`| Aucun changement ||
| `pageOptions` | Supprimé | Le paramètre `pageOptions` n’est plus nécessaire. Les options de scrape ont été déplacées au niveau racine. |
| `replaceAllPathsWithAbsolutePaths` | Supprimé | `replaceAllPathsWithAbsolutePaths` n’est plus nécessaire. Chaque chemin est désormais absolu par défaut. |
| `includeHtml`| Supprimé | Ajoutez `"html"` à `formats` à la place. |
| `includeRawHtml`| Supprimé | Ajoutez `"rawHtml"` à `formats` à la place. |
| `screenshot`| Supprimé | Ajoutez `"screenshot"` à `formats` à la place. |
| `fullPageScreenshot`| Supprimé | Ajoutez `"screenshot@fullPage"` à `formats` à la place. |
| `extractorOptions` | Supprimé | Utilisez le format `"json"` à la place avec l’objet `jsonOptions`. |

Le nouveau format `json` est décrit dans la section [llm-extract](/fr/features/extract).

<div id="crawl-endpoint">
  ## point de terminaison /crawl
</div>

Nous avons également mis à jour le point de terminaison `/crawl` sur `v1`. Découvrez ci-dessous le corps de requête amélioré :

```json
{
  "url": "<string>",
  "excludePaths": ["<string>"],
  "includePaths": ["<string>"],
  "maxDepth": 2,
  "ignoreSitemap": true,
  "limit": 10,
  "allowBackwardLinks": true,
  "allowExternalLinks": true,
  "scrapeOptions": {
    // mêmes options que pour /scrape
    "formats": ["markdown", "html", "rawHtml", "screenshot", "links"]
    "headers": { "<key>": "<value>" },
    "includeTags": ["<string>"],
    "excludeTags": ["<string>"],
    "onlyMainContent": true,
    "waitFor": 123
  }
}
```

<div id="details-on-the-new-request-body">
  ### Détails sur le nouveau corps de requête
</div>

Le tableau ci-dessous présente les modifications des paramètres du corps de requête pour le point de terminaison `/crawl` en V1.

| Paramètre | Changement | Description |
| --------- | ------ | ----------- |
| `pageOptions` | Renommé | Renommé en `scrapeOptions`. |
| `includes` | Déplacé et renommé | Déplacé au niveau racine. Renommé en `includePaths`. |
| `excludes` | Déplacé et renommé | Déplacé au niveau racine. Renommé en `excludePaths`. |
| `allowBackwardCrawling` | Déplacé et renommé | Déplacé au niveau racine. Renommé en `allowBackwardLinks`. |
| `allowExternalLinks` | Déplacé | Déplacé au niveau racine. |
| `maxDepth` | Déplacé | Déplacé au niveau racine. |
| `ignoreSitemap` | Déplacé | Déplacé au niveau racine. |
| `limit` | Déplacé | Déplacé au niveau racine. |
| `crawlerOptions` | Supprimé | Le paramètre `crawlerOptions` n’est plus nécessaire. Les options de crawl ont été déplacées au niveau racine. |
| `timeout` | Supprimé | Utilisez `timeout` dans `scrapeOptions` à la place. |