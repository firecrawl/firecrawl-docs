---
title: "抓取"
description: "将任意 URL 转换为干净的数据"
og:title: "抓取 | Firecrawl"
og:description: "将任意 URL 转换为干净的数据"
---

import InstallationPython from "/snippets/zh/v2/installation/python.mdx";
import InstallationNode from "/snippets/zh/v2/installation/js.mdx";
import InstallationCLI from "/snippets/zh/v2/installation/cli.mdx";
import ScrapePython from "/snippets/zh/v2/scrape/base/python.mdx";
import ScrapeNode from "/snippets/zh/v2/scrape/base/js.mdx";
import ScrapeCURL from "/snippets/zh/v2/scrape/base/curl.mdx";
import ScrapeCLI from "/snippets/zh/v2/scrape/base/cli.mdx";
import ScrapeResponse from "/snippets/zh/v2/scrape/base/output.mdx";
import ExtractCURL from "/snippets/zh/v2/scrape/json/base/curl.mdx";
import ExtractPython from "/snippets/zh/v2/scrape/json/base/python.mdx";
import ExtractNode from "/snippets/zh/v2/scrape/json/base/js.mdx";
import ExtractOutput from "/snippets/zh/v2/scrape/json/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/zh/v2/scrape/json/no-schema/curl.mdx";
import ExtractNoSchemaPython from "/snippets/zh/v2/scrape/json/no-schema/python.mdx";
import ExtractNoSchemaNode from "/snippets/zh/v2/scrape/json/no-schema/js.mdx";
import ExtractNoSchemaOutput from "/snippets/zh/v2/scrape/json/no-schema/output.mdx";
import ScrapeActionsPython from "/snippets/zh/v2/scrape/actions/python.mdx";
import ScrapeActionsNode from "/snippets/zh/v2/scrape/actions/js.mdx";
import ScrapeActionsCURL from "/snippets/zh/v2/scrape/actions/curl.mdx";
import ScrapeActionsOutput from "/snippets/zh/v2/scrape/actions/output.mdx";
import BatchScrapePython from "/snippets/zh/v2/batch-scrape/short/python.mdx";
import BatchScrapeNode from "/snippets/zh/v2/batch-scrape/short/js.mdx";
import BatchScrapeCURL from "/snippets/zh/v2/batch-scrape/short/curl.mdx";
import BatchScrapeOutput from "/snippets/zh/v2/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/zh/v2/batch-scrape/base/async-output.mdx";
import ScrapeLocationPython from "/snippets/zh/v2/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/zh/v2/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/zh/v2/scrape/location/curl.mdx";
import ScrapeBrandingPython from "/snippets/zh/v2/scrape/branding/base/python.mdx";
import ScrapeBrandingNode from "/snippets/zh/v2/scrape/branding/base/js.mdx";
import ScrapeBrandingCURL from "/snippets/zh/v2/scrape/branding/base/curl.mdx";
import ScrapeBrandingOutput from "/snippets/zh/v2/scrape/branding/base/output.mdx";
import ScrapeBrandingCombinedPython from "/snippets/zh/v2/scrape/branding/combined/python.mdx";
import ScrapeBrandingCombinedNode from "/snippets/zh/v2/scrape/branding/combined/js.mdx";
import ScrapeBrandingCombinedCURL from "/snippets/zh/v2/scrape/branding/combined/curl.mdx";

Firecrawl 将网页转换为 Markdown，非常适合 LLM 应用。

* 处理复杂环节：代理、缓存、速率限制、被 JS 阻止的内容
* 支持动态内容：动态网站、JS 渲染站点、PDF、图片
* 输出干净的 Markdown、结构化数据、截图或 HTML

详情请参阅 [Scrape Endpoint API 参考](https://docs.firecrawl.dev/api-reference/endpoint/scrape)。

<div id="scraping-a-url-with-firecrawl">
  ## 使用 Firecrawl 抓取 URL
</div>

<div id="scrape-endpoint">
  ### /scrape 端点
</div>

用于抓取指定 URL 并获取其内容。

<div id="installation">
  ### 安装
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationCLI />
</CodeGroup>

<div id="usage">
  ### 使用方式
</div>

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeCURL />

  <ScrapeCLI />
</CodeGroup>

有关参数的更多信息，请参阅 [API 参考](https://docs.firecrawl.dev/api-reference/endpoint/scrape)。

### 响应

各 SDK 会直接返回数据对象。cURL 将按下方所示原样返回载荷。

<ScrapeResponse />

<div id="scrape-formats">
  ## 抓取 formats
</div>

现在你可以选择输出所需的 formats。你可以指定多个输出 formats。支持的 formats 包括：

* Markdown（`markdown`）
* 摘要（`summary`）
* HTML（`html`）
* 原始 HTML（`rawHtml`，不做任何修改）
* 截图（`screenshot`，可选项包括 `fullPage`、`quality`、`viewport`）
* 链接（`links`）
* JSON（`json`）- 结构化输出
* 图片（`images`）- 提取页面中的所有图片 URL
* 品牌（`branding`）- 提取品牌识别与设计系统

输出键将与你选择的 format 对应。

<div id="extract-structured-data">
  ## 提取结构化数据
</div>

<div id="scrape-with-json-endpoint">
  ### /scrape（使用 JSON）端点
</div>

用于从抓取的页面中提取结构化数据。

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

输出：

<ExtractOutput />

<div id="extracting-without-schema">
  ### 无需 schema 的提取
</div>

现在，你只需向端点传入一个 `prompt`，即可在不提供 schema 的情况下进行提取。LLM 会自行决定数据结构。

<CodeGroup>
  <ExtractNoSchemaPython />

  <ExtractNoSchemaNode />

  <ExtractNoSchemaCURL />
</CodeGroup>

输出：

<ExtractNoSchemaOutput />

<div id="json-format-options">
  ### JSON 格式选项
</div>

使用 `json` 格式时，在 `formats` 中传入一个对象，包含以下参数：

* `schema`：用于结构化输出的 JSON Schema。
* `prompt`：可选提示；在提供 schema 时或仅需轻量指引时用于辅助抽取。

<div id="extract-brand-identity">
  ## 提取品牌识别度
</div>

<div id="scrape-with-branding-endpoint">
  ### /scrape（品牌信息）端点
</div>

branding 格式会从网页中提取完整的品牌识别信息，包括颜色、字体、排版、间距、UI 组件等。它适用于设计系统分析、品牌监测，或构建需要理解网站视觉风格的工具。

<CodeGroup>
  <ScrapeBrandingPython />

  <ScrapeBrandingNode />

  <ScrapeBrandingCURL />
</CodeGroup>

### 响应

品牌配置会返回一个完整的 `BrandingProfile` 对象，其结构如下：

<ScrapeBrandingOutput />

<div id="branding-profile-structure">
  ### 品牌档案结构
</div>

`branding` 对象包含以下属性：

* `colorScheme`: 检测到的配色方案（`"light"` 或 `"dark"`）
* `logo`: 主徽标的 URL
* `colors`: 包含品牌颜色的对象：
  * `primary`、`secondary`、`accent`: 主要品牌色
  * `background`、`textPrimary`、`textSecondary`: 界面颜色
  * `link`、`success`、`warning`、`error`: 语义颜色
* `fonts`: 页面中使用的字体系列数组
* `typography`: 详细的排版信息：
  * `fontFamilies`: 正文、标题与代码字体系列
  * `fontSizes`: 标题与正文的尺寸定义
  * `fontWeights`: 字重定义（细体、常规、中等、粗体）
  * `lineHeights`: 不同文本类型的行高值
* `spacing`: 间距与布局信息：
  * `baseUnit`: 基础间距单位（像素）
  * `borderRadius`: 默认圆角
  * `padding`、`margins`: 间距值
* `components`: UI 组件样式：
  * `buttonPrimary`、`buttonSecondary`: 按钮样式
  * `input`: 输入框样式
* `icons`: 图标样式信息
* `images`: 品牌图像（logo、favicon、og:image）
* `animations`: 动画与过渡设置
* `layout`: 布局配置（栅格、页眉/页脚高度）
* `personality`: 品牌个性特征（语气、调性、目标受众）

<div id="combining-with-other-formats">
  ### 与其他 formats 结合使用
</div>

你可以将 branding 格式与其他 formats 组合，以获取更全面的页面数据：

<CodeGroup>
  <ScrapeBrandingCombinedPython />

  <ScrapeBrandingCombinedNode />

  <ScrapeBrandingCombinedCURL />
</CodeGroup>

<div id="interacting-with-the-page-with-actions">
  ## 使用 actions 与页面交互
</div>

Firecrawl 允许你在抓取页面内容之前对网页执行各种 actions。这对于与动态内容交互、在页面之间导航，或访问需要用户操作的内容特别有用。

下面是一个示例，演示如何使用 actions 访问 google.com，搜索 Firecrawl，点击第一个结果，并截取页面截图。

在执行其他 actions 之前或之后，几乎都应使用 `wait` action，为页面加载预留足够时间。

<div id="example">
  ### 示例
</div>

<CodeGroup>
  <ScrapeActionsPython />

  <ScrapeActionsNode />

  <ScrapeActionsCURL />
</CodeGroup>

<div id="output">
  ### 输出
</div>

<CodeGroup>
  <ScrapeActionsOutput />
</CodeGroup>

有关 actions 参数的更多信息，请参见 [API 参考](https://docs.firecrawl.dev/api-reference/endpoint/scrape)。

<div id="location-and-language">
  ## 位置与语言
</div>

指定国家/地区和首选语言，以根据你的目标位置和语言偏好获取相关内容。

<div id="how-it-works">
  ### 工作原理
</div>

当你指定位置设置时，Firecrawl 会在可用时使用合适的代理，并仿真相应的语言和时区设置。默认情况下，若未指定位置，位置将设为“US”。

### 用法

要使用位置和语言设置，请在请求体中包含 `location` 对象，并提供以下属性：

* `country`：ISO 3166-1 alpha-2 国家/地区代码（例如“US”“AU”“DE”“JP”）。默认值为“US”。
* `languages`：按优先级排序的首选语言和区域设置数组。默认使用所设位置对应的语言。

<CodeGroup>
  <ScrapeLocationPython />

  <ScrapeLocationNode />

  <ScrapeLocationCURL />
</CodeGroup>

有关受支持位置的更多详情，请参阅[代理文档](/zh/features/proxies)。

<div id="caching-and-maxage">
  ## 缓存与 maxAge（缓存）
</div>

为加快请求速度，当有较新的副本可用时，Firecrawl 默认会直接从缓存返回结果。

* **默认新鲜度窗口**：`maxAge = 172800000` 毫秒（2 天）。如果缓存页面仍在该窗口内，将立即返回；否则会重新抓取页面并写入缓存。
* **性能**：在数据对时效性要求不高时，抓取速度可提升至最多 5 倍。
* **始终获取最新**：将 `maxAge` 设为 `0`。
* **避免存储**：如果不希望 Firecrawl 为本次请求缓存/存储结果，将 `storeInCache` 设为 `false`。

示例（强制获取最新内容）：

<CodeGroup>
  ```python Python
  from firecrawl import Firecrawl
  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  doc = firecrawl.scrape(url='https://example.com', maxAge=0, formats=['markdown'])
  print(doc)
  ```

  ```js Node
  import Firecrawl from '@mendable/firecrawl-js';

  const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

  const doc = await firecrawl.scrape('https://example.com', { maxAge: 0, formats: ['markdown'] });
  console.log(doc);
  ```

  ```bash cURL
  curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
    -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "url": "https://example.com",
      "maxAge": 0,
      "formats": ["markdown"]
    }'
  ```
</CodeGroup>

示例（使用 10 分钟缓存窗口）：

<CodeGroup>
  ```python Python
  from firecrawl import Firecrawl
  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  doc = firecrawl.scrape(url='https://example.com', maxAge=600000, formats=['markdown', 'html'])
  print(doc)
  ```

  ```js Node

  const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

  const doc = await firecrawl.scrape('https://example.com', { maxAge: 600000, formats: ['markdown', 'html'] });
  console.log(doc);
  ```

  ```bash cURL
  curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
    -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "url": "https://example.com",
      "maxAge": 600000,
      "formats": ["markdown", "html"]
    }'
  ```
</CodeGroup>

<div id="batch-scraping-multiple-urls">
  ## 批量抓取多个 URL
</div>

你现在可以同时批量抓取多个 URL。它以起始 URL 和可选参数作为输入。params 参数允许你为批量抓取任务指定其他选项，例如输出 formats。

<div id="how-it-works">
  ### 工作原理
</div>

它与 `/crawl` 端点的运行方式非常相似。它会提交一个批量抓取作业，并返回一个作业 ID，用于检查该批量抓取的状态。

SDK 提供两种方式：同步与异步。同步方式会直接返回批量抓取作业的结果，异步方式则会返回一个作业 ID，供您用于查询批量抓取的状态。

### 使用方法

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Response
</div>

如果你使用 SDK 的同步方法，将直接返回批量抓取任务的结果；否则会返回一个作业 ID，你可以用它来查询批量抓取的状态。

<div id="synchronous">
  #### 同步执行
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### 异步
</div>

你可以使用作业 ID 调用 `/batch/scrape/{id}` 端点来查看批量抓取的状态。该端点应在作业仍在运行期间或刚完成后使用，**因为批量抓取作业会在 24 小时后过期**。

<BatchScrapeAsyncOutput />

<div id="enhanced-mode">
  ## 隐身模式
</div>

对于复杂网站，Firecrawl 提供了隐身模式，在提高成功率的同时保护隐私。

了解更多关于[隐身模式](/zh/features/enhanced-mode)的信息。