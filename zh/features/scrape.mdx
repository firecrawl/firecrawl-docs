---
title: "抓取"
description: "将任意 URL 转换为干净的数据"
og:title: "抓取 | Firecrawl"
og:description: "将任意 URL 转换为干净的数据"
---

import InstallationPython from "/snippets/zh/v2/installation/python.mdx";
import InstallationNode from "/snippets/zh/v2/installation/js.mdx";
import ScrapePython from "/snippets/zh/v2/scrape/base/python.mdx";
import ScrapeNode from "/snippets/zh/v2/scrape/base/js.mdx";
import ScrapeCURL from "/snippets/zh/v2/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/zh/v2/scrape/base/output.mdx";
import ExtractCURL from "/snippets/zh/v2/scrape/json/base/curl.mdx";
import ExtractPython from "/snippets/zh/v2/scrape/json/base/python.mdx";
import ExtractNode from "/snippets/zh/v2/scrape/json/base/js.mdx";
import ExtractOutput from "/snippets/zh/v2/scrape/json/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/zh/v2/scrape/json/no-schema/curl.mdx";
import ExtractNoSchemaPython from "/snippets/zh/v2/scrape/json/no-schema/python.mdx";
import ExtractNoSchemaNode from "/snippets/zh/v2/scrape/json/no-schema/js.mdx";
import ExtractNoSchemaOutput from "/snippets/zh/v2/scrape/json/no-schema/output.mdx";
import ScrapeActionsPython from "/snippets/zh/v2/scrape/actions/python.mdx";
import ScrapeActionsNode from "/snippets/zh/v2/scrape/actions/js.mdx";
import ScrapeActionsCURL from "/snippets/zh/v2/scrape/actions/curl.mdx";
import ScrapeActionsOutput from "/snippets/zh/v2/scrape/actions/output.mdx";
import BatchScrapePython from "/snippets/zh/v2/batch-scrape/short/python.mdx";
import BatchScrapeNode from "/snippets/zh/v2/batch-scrape/short/js.mdx";
import BatchScrapeCURL from "/snippets/zh/v2/batch-scrape/short/curl.mdx";
import BatchScrapeOutput from "/snippets/zh/v2/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/zh/v2/batch-scrape/base/async-output.mdx";
import ScrapeLocationPython from "/snippets/zh/v2/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/zh/v2/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/zh/v2/scrape/location/curl.mdx";

Firecrawl 将网页转换为 Markdown，非常适合用于 LLM 应用。

* 代劳繁杂环节：代理、缓存、速率限制、被 JS 屏蔽的内容
* 处理动态内容：动态网站、JS 渲染页面、PDF、图片
* 输出整洁的 Markdown、结构化数据、截图或 HTML。

详情参见 [Scrape 端点 API 参考](https://docs.firecrawl.dev/api-reference/endpoint/scrape)。

<div id="scraping-a-url-with-firecrawl">
  ## 使用 Firecrawl 抓取 URL
</div>

<div id="scrape-endpoint">
  ### /scrape 端点
</div>

用于抓取指定 URL 并获取其内容。

<div id="installation">
  ### 安装
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />
</CodeGroup>

<div id="usage">
  ### 使用方式
</div>

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeCURL />
</CodeGroup>

有关参数的更多信息，请参阅 [API 参考](https://docs.firecrawl.dev/api-reference/endpoint/scrape)。

<div id="response">
  ### 响应
</div>

各 SDK 会直接返回数据对象。cURL 将按下方所示原样返回载荷。

<ScrapeResponse />

<div id="scrape-formats">
  ## 抓取 formats
</div>

你现在可以选择所需的输出 formats。你可以指定多个输出 formats。支持的 formats 包括：

* Markdown (`markdown`)
* Summary (`summary`)
* HTML (`html`)
* 原始 HTML（`rawHtml`，不做任何修改）
* 截图（`screenshot`，可用选项包括 `fullPage`、`quality`、`viewport`）
* 链接（`links`）
* JSON（`json`）— 结构化输出

输出的键将与所选的 formats 对应。

<div id="extract-structured-data">
  ## 提取结构化数据
</div>

<div id="scrape-with-json-endpoint">
  ### /scrape（使用 JSON）端点
</div>

用于从抓取的页面中提取结构化数据。

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

输出：

<ExtractOutput />

<div id="extracting-without-schema">
  ### 无需 schema 的提取
</div>

现在，你只需向端点传入一个 `prompt`，即可在不提供 schema 的情况下进行提取。LLM 会自行决定数据结构。

<CodeGroup>
  <ExtractNoSchemaPython />

  <ExtractNoSchemaNode />

  <ExtractNoSchemaCURL />
</CodeGroup>

输出：

<ExtractNoSchemaOutput />

<div id="json-format-options">
  ### JSON 格式选项
</div>

使用 `json` 格式时，在 `formats` 中传入一个对象，包含以下参数：

* `schema`：用于结构化输出的 JSON Schema。
* `prompt`：可选提示；在提供 schema 时或仅需轻量指引时用于辅助抽取。

<div id="interacting-with-the-page-with-actions">
  ## 使用 actions 与页面交互
</div>

Firecrawl 允许你在抓取页面内容之前对网页执行各种 actions。这对于与动态内容交互、在页面之间导航，或访问需要用户操作的内容特别有用。

下面是一个示例，演示如何使用 actions 访问 google.com，搜索 Firecrawl，点击第一个结果，并截取页面截图。

在执行其他 actions 之前或之后，几乎都应使用 `wait` action，为页面加载预留足够时间。

<div id="example">
  ### 示例
</div>

<CodeGroup>
  <ScrapeActionsPython />

  <ScrapeActionsNode />

  <ScrapeActionsCURL />
</CodeGroup>

<div id="output">
  ### 输出
</div>

<CodeGroup>
  <ScrapeActionsOutput />
</CodeGroup>

有关 actions 参数的更多信息，请参见 [API 参考](https://docs.firecrawl.dev/api-reference/endpoint/scrape)。

<div id="location-and-language">
  ## 位置与语言
</div>

指定国家/地区和首选语言，以根据你的目标位置和语言偏好获取相关内容。

<div id="how-it-works">
  ### 工作原理
</div>

当你指定位置设置时，Firecrawl 会在可用时使用合适的代理，并仿真相应的语言和时区设置。默认情况下，若未指定位置，位置将设为“US”。

<div id="usage">
  ### 用法
</div>

要使用位置和语言设置，请在请求体中包含 `location` 对象，并提供以下属性：

* `country`：ISO 3166-1 alpha-2 国家/地区代码（例如“US”“AU”“DE”“JP”）。默认值为“US”。
* `languages`：按优先级排序的首选语言和区域设置数组。默认使用所设位置对应的语言。

<CodeGroup>
  <ScrapeLocationPython />

  <ScrapeLocationNode />

  <ScrapeLocationCURL />
</CodeGroup>

有关受支持位置的更多详情，请参阅[代理文档](/zh/features/proxies)。

<div id="caching-and-maxage">
  ## 缓存与 maxAge（缓存）
</div>

为加快请求速度，当有较新的副本可用时，Firecrawl 默认会直接从缓存返回结果。

* **默认新鲜度窗口**：`maxAge = 172800000` 毫秒（2 天）。如果缓存页面仍在该窗口内，将立即返回；否则会重新抓取页面并写入缓存。
* **性能**：在数据对时效性要求不高时，抓取速度可提升至最多 5 倍。
* **始终获取最新**：将 `maxAge` 设为 `0`。
* **避免存储**：如果不希望 Firecrawl 为本次请求缓存/存储结果，将 `storeInCache` 设为 `false`。

示例（强制获取最新内容）：

<CodeGroup>
  ```python Python
  from firecrawl import Firecrawl
  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  doc = firecrawl.scrape(url='https://example.com', maxAge=0, formats=['markdown'])
  print(doc)
  ```

  ```js Node
  import Firecrawl from '@mendable/firecrawl-js';

  const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

  const doc = await firecrawl.scrape('https://example.com', { maxAge: 0, formats: ['markdown'] });
  console.log(doc);
  ```

  ```bash cURL
  curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
    -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "url": "https://example.com",
      "maxAge": 0,
      "formats": ["markdown"]
    }'
  ```
</CodeGroup>

示例（使用 10 分钟缓存窗口）：

<CodeGroup>
  ```python Python
  from firecrawl import Firecrawl
  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  doc = firecrawl.scrape(url='https://example.com', maxAge=600000, formats=['markdown', 'html'])
  print(doc)
  ```

  ```js Node

  const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

  const doc = await firecrawl.scrape('https://example.com', { maxAge: 600000, formats: ['markdown', 'html'] });
  console.log(doc);
  ```

  ```bash cURL
  curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
    -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
    -H "Content-Type: application/json" \
    -d '{
      "url": "https://example.com",
      "maxAge": 600000,
      "formats": ["markdown", "html"]
    }'
  ```
</CodeGroup>

<div id="batch-scraping-multiple-urls">
  ## 批量抓取多个 URL
</div>

你现在可以同时批量抓取多个 URL。它以起始 URL 和可选参数作为输入。params 参数允许你为批量抓取任务指定其他选项，例如输出 formats。

<div id="how-it-works">
  ### 工作原理
</div>

它与 `/crawl` 端点的运行方式非常相似。它会提交一个批量抓取作业，并返回一个作业 ID，用于检查该批量抓取的状态。

SDK 提供两种方式：同步与异步。同步方式会直接返回批量抓取作业的结果，异步方式则会返回一个作业 ID，供您用于查询批量抓取的状态。

<div id="usage">
  ### 使用方法
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### Response
</div>

如果你使用 SDK 的同步方法，将直接返回批量抓取任务的结果；否则会返回一个作业 ID，你可以用它来查询批量抓取的状态。

<div id="synchronous">
  #### 同步执行
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### 异步
</div>

你可以使用作业 ID 调用 `/batch/scrape/{id}` 端点来查看批量抓取的状态。该端点应在作业仍在运行期间或刚完成后使用，**因为批量抓取作业会在 24 小时后过期**。

<BatchScrapeAsyncOutput />

<div id="stealth-mode">
  ## 隐身模式
</div>

针对具有高级反机器人防护的网站，Firecrawl 提供隐身代理模式，可在抓取复杂站点时显著提升成功率。

了解更多：[隐身模式](/zh/features/stealth-mode)。