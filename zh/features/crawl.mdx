---
title: '爬取'
description: 'Firecrawl 可递归遍历某个 URL 的子域，并收集其内容'
og:title: '爬取 | Firecrawl'
og:description: 'Firecrawl 可递归遍历某个 URL 的子域，并收集其内容'
icon: 'spider'
---

import InstallationPython from '/snippets/zh/v2/installation/python.mdx';
import InstallationNode from '/snippets/zh/v2/installation/js.mdx';
import InstallationCLI from '/snippets/zh/v2/installation/cli.mdx';
import CrawlPython from '/snippets/zh/v2/crawl/base/python.mdx';
import CrawlNode from '/snippets/zh/v2/crawl/base/js.mdx';
import CrawlCURL from '/snippets/zh/v2/crawl/base/curl.mdx';
import CrawlCLI from '/snippets/zh/v2/crawl/base/cli.mdx';
import CheckCrawlJobPython from '/snippets/zh/v2/crawl-status/short/python.mdx';
import CheckCrawlJobNode from '/snippets/zh/v2/crawl-status/short/js.mdx';
import CheckCrawlJobCURL from '/snippets/zh/v2/crawl-status/short/curl.mdx';
import CheckCrawlJobCLI from '/snippets/zh/v2/crawl-status/short/cli.mdx';
import CheckCrawlJobOutputScraping from '/snippets/zh/v2/crawl-status/base/output-scraping.mdx';
import CheckCrawlJobOutputCompleted from '/snippets/zh/v2/crawl-status/base/output-completed.mdx';
import CrawlWebSocketPython from '/snippets/zh/v2/crawl-websocket/base/python.mdx';
import CrawlWebSocketNode from '/snippets/zh/v2/crawl-websocket/base/js.mdx';
import CrawlWebhookCURL from '/snippets/zh/v2/crawl-webhook/base/curl.mdx';
import PythonCrawlExample from '/snippets/zh/v2/crawl/sdk-example/python.mdx';
import NodeCrawlExample from '/snippets/zh/v2/crawl/sdk-example/js.mdx';
import PythonCrawlExampleResponse from '/snippets/zh/v2/crawl/sdk-example/python-response.mdx';
import NodeCrawlExampleResponse from '/snippets/zh/v2/crawl/sdk-example/js-response.mdx';
import StartCrawlPython from '/snippets/zh/v2/start-crawl/base/python.mdx';
import StartCrawlNode from '/snippets/zh/v2/start-crawl/base/js.mdx';
import StartCrawlCURL from '/snippets/zh/v2/start-crawl/base/curl.mdx';
import StartCrawlCLI from '/snippets/zh/v2/start-crawl/base/cli.mdx';
import StartCrawlOutput from '/snippets/zh/v2/start-crawl/base/output.mdx';

Firecrawl 高效爬取网站，在处理复杂的 Web 基础架构的同时提取全面数据。流程如下：

1. **URL 分析：** 扫描 sitemap 并爬取网站以识别链接
2. **遍历：** 递归跟随链接以发现所有子页面
3. **抓取：** 从各页面提取内容，处理 JS 与速率限制
4. **输出：** 将数据转换为干净的 Markdown 或结构化格式

确保可从任意起始 URL 全面采集数据。

<div id="crawling">
  ## 爬虫
</div>

<div id="crawl-endpoint">
  ### /crawl 端点
</div>

用于抓取某个 URL 及其所有可访问的子页面。该操作会提交一个抓取任务，并返回任务 ID 以便查询抓取状态。

<Warning>
  默认情况下，如果页面中的子链接并非你提供的 URL 的下级路径，Crawl 会忽略它们。因此，若你抓取 website.com/blogs/，则不会返回 website.com/other-parent/blog-1。若需要包含 website.com/other-parent/blog-1，请使用 `crawlEntireDomain` 参数。若在抓取 website.com 时需要抓取其子域名（如 blog.website.com），请使用 `allowSubdomains` 参数。
</Warning>

<div id="installation">
  ### 安装
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationCLI />
</CodeGroup>

<div id="usage">
  ### 用法
</div>

<CodeGroup>
  <CrawlPython />

  <CrawlNode />

  <CrawlCURL />

  <CrawlCLI />
</CodeGroup>

<Info>
  每抓取 1 个页面会消耗 1 个积分。抓取的默认 `limit` 为 10,000 个页面，你可以设置更低的 `limit` 来控制积分消耗（例如将 `limit` 设为 100）。某些选项会额外消耗积分：JSON 模式每个页面额外消耗 4 个积分，增强代理每个页面额外消耗 4 个积分，PDF 解析每个 PDF 页面额外消耗 1 个积分。
</Info>

<div id="scrape-options-in-crawl">
  ### 在 Crawl 中使用 Scrape 选项
</div>

Scrape 端点的所有选项都可通过 `scrapeOptions`（JS）/ `scrape_options`（Python）在 Crawl 中使用。它们将应用于爬虫抓取的每个页面：formats、proxy、caching、actions、location、tags 等。完整列表参见 [Scrape API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape)。

<CodeGroup>
  ```js Node
  import Firecrawl from '@mendable/firecrawl-js';

  const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR_API_KEY' });

  // 使用 scrape 选项进行爬取
  const crawlResponse = await firecrawl.crawl('https://example.com', {
    limit: 100,
    scrapeOptions: {
      formats: [
        'markdown',
        {
          type: 'json',
          schema: { type: 'object', properties: { title: { type: 'string' } } },
        },
      ],
      proxy: 'auto',
      maxAge: 600000,
      onlyMainContent: true,
    },
  });
  ```

  ```python Python
  from firecrawl import Firecrawl

  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  # 使用 scrape 选项进行爬取
  response = firecrawl.crawl('https://example.com',
      limit=100,
      scrape_options={
          'formats': [
              'markdown',
              { 'type': 'json', 'schema': { 'type': 'object', 'properties': { 'title': { 'type': 'string' } } } }
          ],
          'proxy': 'auto',
          'maxAge': 600000,
          'onlyMainContent': True
      }
  )
  ```
</CodeGroup>

<div id="api-response">
  ### API 响应
</div>

如果你使用 cURL 或 starter 方法，将返回一个用于检查爬取状态的 `ID`。

<Note>
  如果你使用 SDK，请参见下方方法，了解 waiter 与 starter 的行为差异。
</Note>

<StartCrawlOutput />

<div id="check-crawl-job">
  ### 检查爬取任务
</div>

用于检查爬取任务的状态并获取结果。

<Note>
  任务结果在完成后 24 小时内可通过 API 获取。此后，你仍可以在[活动日志](https://www.firecrawl.dev/app/logs)中查看你的爬取历史和结果。
</Note>

<Note>
  爬取结果中的 `data` 数组里包含的是 Firecrawl 成功抓取的页面 —— 即使目标站点返回了 404 等 HTTP 错误。`metadata.statusCode` 字段显示的是目标站点返回的 HTTP 状态码。若要获取 Firecrawl 本身未能成功抓取的页面（例如网络错误、超时或被 robots.txt 拦截），请使用专门的 [Get Crawl Errors](/zh/api-reference/endpoint/crawl-get-errors) 端点（`GET /crawl/{id}/errors`）。
</Note>

<CodeGroup>
  <CheckCrawlJobPython />

  <CheckCrawlJobNode />

  <CheckCrawlJobCURL />

  <CheckCrawlJobCLI />
</CodeGroup>

<div id="response-handling">
  #### 响应处理
</div>

响应会根据爬取任务的状态而有所不同。

对于未完成的任务或超过 10MB 的大型响应，会返回一个 `next` URL 参数。你需要请求该 URL 以获取后续的每 10MB 数据。如果没有 `next` 参数，则表示爬取数据已结束。

`skip` 参数用于设置每个结果分块所返回的最大条目数。

<Info>
  仅在直接调用 API 时，skip 和 next 参数才生效。
  如果你使用 SDK，我们会代为处理，并一次性返回全部结果。
</Info>

<CodeGroup>
  <CheckCrawlJobOutputScraping />

  <CheckCrawlJobOutputCompleted />
</CodeGroup>

<div id="sdk-methods">
  ### SDK 方法
</div>

使用 SDK 有两种方式：

1. **抓取并等待**（`crawl`）：
   * 等待爬取完成并返回完整响应
   * 自动处理分页
   * 适用于大多数场景，推荐使用

<CodeGroup>
  <PythonCrawlExample />

  <NodeCrawlExample />
</CodeGroup>

响应包括爬取状态及所有抓取到的数据：

<CodeGroup>
  <PythonCrawlExampleResponse />

  <NodeCrawlExampleResponse />
</CodeGroup>

2. **启动后轮询状态**（`startCrawl`/`start_crawl`）：
   * 立即返回一个爬取 ID
   * 支持手动检查进度/状态
   * 适合长时间运行的爬取或自定义轮询逻辑

<CodeGroup>
  <StartCrawlPython />

  <StartCrawlNode />

  <StartCrawlCURL />

  <StartCrawlCLI />
</CodeGroup>

<div id="crawl-websocket">
  ## 爬取 WebSocket
</div>

Firecrawl 基于 WebSocket 的方法 `Crawl URL and Watch` 支持实时数据提取与监控。以 URL 启动爬取，并可通过页面数量上限、允许的域名、输出 formats 等选项进行自定义，适用于即时数据处理需求。

<CodeGroup>
  <CrawlWebSocketPython />

  <CrawlWebSocketNode />
</CodeGroup>

<div id="crawl-webhook">
  ## 爬取 Webhook
</div>

你可以配置 webhook，在爬取过程中实时接收通知，从而在页面被抓取后立即进行处理，而无需等待整个爬取任务完成。

<CrawlWebhookCURL />

<div id="quick-reference">
  ### 快速参考
</div>

**事件类型：**

* `crawl.started` - 爬取开始时触发
* `crawl.page` - 每成功抓取一个页面时触发
* `crawl.completed` - 爬取完成时触发
* `crawl.failed` - 爬取出错时触发

**基本负载：**

```json
{
  "success": true,
  "type": "crawl.page",
  "id": "crawl-job-id",
  "data": [...], // 'page' 事件的页面数据
  "metadata": {}, // Your custom metadata
  "error": null
}
```

<div id="security-verifying-webhook-signatures">
  ### 安全：验证 Webhook 签名
</div>

来自 Firecrawl 的每个 webhook 请求都会包含一个 `X-Firecrawl-Signature` 请求头，其中含有一个 HMAC-SHA256 签名。**务必验证此签名**，以确保 webhook 为真实请求且未被篡改。

**工作原理：**

1. 在账户设置中的 [Advanced（高级）选项卡](https://www.firecrawl.dev/app/settings?tab=advanced) 获取你的 webhook 密钥（secret）
2. 从 `X-Firecrawl-Signature` 请求头中提取签名
3. 使用该密钥对原始请求体计算 HMAC-SHA256
4. 使用时间安全函数（timing-safe function）将计算结果与签名请求头中的值进行比较

<Warning>
  在验证签名之前，切勿处理任何 webhook。`X-Firecrawl-Signature` 请求头中的签名格式为：`sha256=abc123def456...`
</Warning>

有关 JavaScript 和 Python 的完整实现示例，请参阅 [Webhook 安全文档](/zh/webhooks/security)。

<div id="full-documentation">
  ### 完整文档
</div>

有关完整的 webhook 文档（包括事件负载详情、负载结构、高级配置和故障排除指南），请参阅[Webhook 文档](/zh/webhooks/overview)。