---
title: "批量抓取"
description: "批量抓取多个 URL"
og:title: "批量抓取 | Firecrawl"
og:description: "批量抓取多个 URL"
---

import BatchScrapePython from "/snippets/zh/v2/batch-scrape/base/python.mdx";
import BatchScrapeNode from "/snippets/zh/v2/batch-scrape/base/js.mdx";
import BatchScrapeCURL from "/snippets/zh/v2/batch-scrape/base/curl.mdx";
import BatchScrapeOutput from "/snippets/zh/v2/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/zh/v2/batch-scrape/base/async-output.mdx";
import BatchScrapeExtractPython from "/snippets/zh/v2/batch-scrape/json/python.mdx";
import BatchScrapeExtractNode from "/snippets/zh/v2/batch-scrape/json/js.mdx";
import BatchScrapeExtractCURL from "/snippets/zh/v2/batch-scrape/json/curl.mdx";
import BatchScrapeExtractOutput from "/snippets/zh/v2/batch-scrape/json/output.mdx";
import BatchScrapeExtractAsyncOutput from "/snippets/zh/v2/batch-scrape/json/async-output.mdx";
import BatchScrapeWebhookCURL from "/snippets/zh/v1/batch-scrape-webhook/base/curl.mdx";

<div id="batch-scraping-multiple-urls">
  ## 批量抓取多个 URL
</div>

现在你可以同时批量抓取多个 URL。该方法以起始 URL 和可选参数作为入参。通过 params 参数，你可以为批量抓取任务指定其他选项，例如输出 formats。

<div id="how-it-works">
  ### 工作原理
</div>

它与 `/crawl` 端点的工作方式非常相似。你可以启动批处理并等待其完成，或先启动再自行处理完成流程。

* `batchScrape`（JS）/ `batch_scrape`（Python）：启动批处理作业并等待完成，返回结果。
* `startBatchScrape`（JS）/ `start_batch_scrape`（Python）：启动批处理作业并返回作业 ID，便于你轮询或使用 webhooks。

<div id="usage">
  ### 使用方法
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### 响应
</div>

* 调用 `batchScrape`/`batch_scrape` 会在批处理完成后返回完整结果。

<BatchScrapeOutput />

* 调用 `startBatchScrape`/`start_batch_scrape` 会返回一个作业 ID。你可以通过 `getBatchScrapeStatus`/`get_batch_scrape_status`、API 端点 `/batch/scrape/{id}`，或 webhooks 来跟踪进度。该端点用于进行中检查或在完成后立即查询，**因为批处理作业会在 24 小时后过期**。

<BatchScrapeAsyncOutput />

<div id="batch-scrape-with-structured-extraction">
  ## 批量抓取并进行结构化提取
</div>

你也可以使用批量抓取端点从页面中提取结构化数据。如果你想从一组 URL 中获取相同的结构化数据，这将非常有用。

<CodeGroup>
  <BatchScrapeExtractPython />

  <BatchScrapeExtractNode />

  <BatchScrapeExtractCURL />
</CodeGroup>

<div id="response">
  ### 响应
</div>

* `batchScrape`/`batch_scrape` 返回完整结果：

<BatchScrapeExtractOutput />

* `startBatchScrape`/`start_batch_scrape` 返回任务 ID：

<BatchScrapeExtractAsyncOutput />

<div id="batch-scrape-with-webhooks">
  ## 使用 webhooks 批量抓取
</div>

你可以配置 webhooks，在批量中的每个 URL 被抓取时接收实时通知。这样你可以立即处理结果，而无需等待整个批次完成。

<BatchScrapeWebhookCURL />

有关完整的 webhooks 文档（包括事件类型、负载结构和实现示例），请参见 [Webhooks 文档](/zh/features/webhooks)。

<div id="quick-reference">
  ### 快速参考
</div>

**事件类型：**

* `batch_scrape.started` - 批量抓取开始时
* `batch_scrape.page` - 每个 URL 抓取成功时
* `batch_scrape.completed` - 所有 URL 处理完成时
* `batch_scrape.failed` - 批量抓取发生错误时

**基础载荷：**

```json
{
  "success": true,
  "type": "batch_scrape.page",
  "id": "batch-job-id",
  "data": [...], // “page” 事件的页面数据
  "metadata": {}, // 你的自定义元数据
  "error": null
}
```

<Note>
  如需了解更详细的 webhook 配置、安全性最佳实践和故障排除，请参阅 [Webhooks 文档](/zh/features/webhooks)。
</Note>
