---
title: '批量抓取'
description: '批量抓取多个 URL'
og:title: '批量抓取 | Firecrawl'
og:description: '批量抓取多个 URL'
---

import BatchScrapePython from '/snippets/zh/v2/batch-scrape/base/python.mdx';
import BatchScrapeNode from '/snippets/zh/v2/batch-scrape/base/js.mdx';
import BatchScrapeCURL from '/snippets/zh/v2/batch-scrape/base/curl.mdx';
import BatchScrapeOutput from '/snippets/zh/v2/batch-scrape/base/output.mdx';
import BatchScrapeAsyncOutput from '/snippets/zh/v2/batch-scrape/base/async-output.mdx';
import BatchScrapeExtractPython from '/snippets/zh/v2/batch-scrape/json/python.mdx';
import BatchScrapeExtractNode from '/snippets/zh/v2/batch-scrape/json/js.mdx';
import BatchScrapeExtractCURL from '/snippets/zh/v2/batch-scrape/json/curl.mdx';
import BatchScrapeExtractOutput from '/snippets/zh/v2/batch-scrape/json/output.mdx';
import BatchScrapeExtractAsyncOutput from '/snippets/zh/v2/batch-scrape/json/async-output.mdx';
import BatchScrapeWebhookCURL from '/snippets/zh/v1/batch-scrape-webhook/base/curl.mdx';

<div id="batch-scraping-multiple-urls">
  ## 批量抓取多个 URL
</div>

现在你可以同时批量抓取多个 URL。该方法以起始 URL 和可选参数作为入参。通过 params 参数，你可以为批量抓取任务指定其他选项，例如输出 formats。

<div id="how-it-works">
  ### 工作原理
</div>

它与 `/crawl` 端点的工作方式非常相似。你可以启动批处理并等待其完成，或先启动再自行处理完成流程。

* `batchScrape`（JS）/ `batch_scrape`（Python）：启动批处理作业并等待完成，返回结果。
* `startBatchScrape`（JS）/ `start_batch_scrape`（Python）：启动批处理作业并返回作业 ID，便于你轮询或使用 webhooks。

<div id="usage">
  ### 使用方法
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### 响应
</div>

调用 `batchScrape`/`batch_scrape` 会在批处理完成后返回完整结果。

<BatchScrapeOutput />

调用 `startBatchScrape`/`start_batch_scrape` 会返回一个作业 ID。你可以通过 `getBatchScrapeStatus`/`get_batch_scrape_status`、API 端点 `/batch/scrape/{id}`，或 webhooks 来跟踪进度。作业结果在完成后会通过 API 保留 24 小时。在此之后，你仍然可以在[活动日志](https://www.firecrawl.dev/app/logs)中查看批量抓取历史和结果。

<BatchScrapeAsyncOutput />

<div id="batch-scrape-with-structured-extraction">
  ## 批量抓取并进行结构化提取
</div>

你也可以使用批量抓取端点从页面中提取结构化数据。如果你想从一组 URL 中获取相同的结构化数据，这将非常有用。

<CodeGroup>
  <BatchScrapeExtractPython />

  <BatchScrapeExtractNode />

  <BatchScrapeExtractCURL />
</CodeGroup>

<div id="response">
  ### 响应
</div>

`batchScrape`/`batch_scrape` 返回完整结果：

<BatchScrapeExtractOutput />

`startBatchScrape`/`start_batch_scrape` 返回任务 ID：

<BatchScrapeExtractAsyncOutput />

<div id="batch-scrape-with-webhooks">
  ## 使用 Webhook 进行批量抓取
</div>

你可以配置 Webhook，在批次中的每个 URL 被抓取时接收实时通知。这样你可以立即处理结果，而无需等待整个批次完成。

<BatchScrapeWebhookCURL />

<div id="quick-reference">
  ### 快速参考
</div>

**事件类型：**

* `batch_scrape.started` - 批量抓取开始时
* `batch_scrape.page` - 每个 URL 成功抓取时
* `batch_scrape.completed` - 所有 URL 处理完成时
* `batch_scrape.failed` - 批量抓取出现错误时

**基本载荷：**

```json
{
  "success": true,
  "type": "batch_scrape.page",
  "id": "batch-job-id",
  "data": [...], // 'page' 事件的页面数据
  "metadata": {}, // Your custom metadata
  "error": null
}
```

<div id="security-verifying-webhook-signatures">
  ### 安全性：验证 Webhook 签名
</div>

来自 Firecrawl 的每个 webhook 请求都会包含一个 `X-Firecrawl-Signature` 请求头，其中包含一个 HMAC-SHA256 签名。**务必始终验证该签名**，以确保 webhook 是真实的且未被篡改。

**工作原理：**

1. 在你账号设置中的 [Advanced 选项卡](https://www.firecrawl.dev/app/settings?tab=advanced) 获取你的 webhook secret
2. 从 `X-Firecrawl-Signature` 请求头中提取签名
3. 使用你的 secret 对原始请求体计算 HMAC-SHA256
4. 使用时间安全（timing-safe）的比较函数将其与签名请求头的值进行比较

<Warning>
  切勿在未先验证签名的情况下处理 webhook。`X-Firecrawl-Signature` 请求头中的签名格式为：`sha256=abc123def456...`
</Warning>

如需查看 JavaScript 和 Python 的完整实现示例，请参阅 [Webhook 安全文档](/zh/webhooks/security)。

<div id="full-documentation">
  ### 完整文档
</div>

有关完整的 Webhook 文档（包括详细的事件载荷、高级配置和故障排查），请参阅[Webhook 文档](/zh/webhooks/overview)。