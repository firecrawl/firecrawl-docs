---
title: 欢迎来到 V1
description: "Firecrawl 让你将整站内容转换为适配 LLM 的 Markdown"
og:title: "欢迎来到 V1 | Firecrawl"
og:description: "Firecrawl 让你将整站内容转换为适配 LLM 的 Markdown"
---

import InstallationPython from "/snippets/zh/v1/installation/python.mdx";
import InstallationNode from "/snippets/zh/v1/installation/js.mdx";
import InstallationGo from "/snippets/zh/v1/installation/go.mdx";
import InstallationRust from "/snippets/zh/v1/installation/rust.mdx";
import ScrapePython from "/snippets/zh/v1/scrape/base/python.mdx";
import ScrapeNode from "/snippets/zh/v1/scrape/base/js.mdx";
import ScrapeGo from "/snippets/zh/v1/scrape/base/go.mdx";
import ScrapeRust from "/snippets/zh/v1/scrape/base/rust.mdx";
import ScrapeCURL from "/snippets/zh/v1/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/zh/v1/scrape/base/output.mdx";
import MapPython from "/snippets/zh/v1/map/base/python.mdx";
import MapJavaScript from "/snippets/zh/v1/map/base/js.mdx";
import MapGo from "/snippets/zh/v1/map/base/go.mdx";
import MapRust from "/snippets/zh/v1/map/base/rust.mdx";
import MapCURL from "/snippets/zh/v1/map/base/curl.mdx";
import MapResponse from "/snippets/zh/v1/map/base/output.mdx";
import CrawlWebSocketPythonBase from "/snippets/zh/v1/crawl-websocket/base/python.mdx";
import CrawlWebSocketNodeBase from "/snippets/zh/v1/crawl-websocket/base/js.mdx";
import ExtractCURL from "/snippets/zh/v1/llm-extract/base/curl.mdx";
import ExtractPython from "/snippets/zh/v1/llm-extract/base/python.mdx";
import ExtractNode from "/snippets/zh/v1/llm-extract/base/js.mdx";
import ExtractOutput from "/snippets/zh/v1/llm-extract/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/zh/v1/llm-extract/no-schema/curl.mdx";
import ExtractNoSchemaOutput from "/snippets/zh/v1/llm-extract/no-schema/output.mdx";
import CrawlWebhookCURL from "/snippets/zh/v1/crawl-webhook/base/curl.mdx";

Firecrawl V1 已发布！我们随之推出了更可靠、对开发者更友好的 API。

新功能包括：

* `/scrape` 的输出 formats，可按需选择输出 formats。
* 新增 [`/map` 端点](/zh/features/map)，用于获取网页的大部分 URL。
* 更易用的 `/crawl/{id}` 状态 API。
* 所有套餐的速率限制提高 2 倍。
* [Go SDK](/zh/sdks/go) 和 [Rust SDK](/zh/sdks/rust)
* 团队功能支持
* 在控制台中管理 API Key。
* `onlyMainContent` 现已默认设为 `true`。
* `/crawl` 的 webhook 与 WebSocket 支持。

<div id="scrape-formats">
  ## 抓取 formats
</div>

现在你可以选择所需的输出 formats，并且可以同时指定多个。支持的 formats 包括：

* Markdown（markdown）
* HTML（html）
* 原始 HTML（rawHtml）（不作任何修改）
* 截图（screenshot 或 screenshot@fullPage）
* 链接（links）
* JSON（json）——结构化输出

输出的键名将与您选择的 format 对应。

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeGo />

  <ScrapeRust />

  <ScrapeCURL />
</CodeGroup>

<div id="response">
  ### 响应
</div>

各 SDK 会直接返回数据对象。cURL 会按下方所示原样返回有效负载。

<ScrapeResponse />

<div id="introducing-map-alpha">
  ## 介绍 /map（Alpha）
</div>

从单个 URL 轻松生成整个网站地图的最简便方式。

<div id="usage">
  ### 使用方法
</div>

<CodeGroup>
  <MapPython />

  <MapJavaScript />

  <MapGo />

  <MapRust />

  <MapCURL />
</CodeGroup>

<div id="response">
  ### 响应
</div>

各 SDK 将直接返回数据对象。cURL 将按下方所示原样返回载荷。

<MapResponse />

<div id="websockets">
  ## WebSockets
</div>

要通过 WebSockets 爬取网站，请使用 `Crawl URL and Watch` 方法。

<CodeGroup>
  <CrawlWebSocketPythonBase />

  <CrawlWebSocketNodeBase />
</CodeGroup>

<div id="json-format">
  ## JSON 格式
</div>

LLM 抽取现已在 v1 中以 `json` 格式提供。要从页面提取结构化数据，你可以向端点传入一个 JSON schema，或仅提供一个提示词。

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

输出：

<ExtractOutput />

<div id="extracting-without-schema-new">
  ### 无需 schema 的提取（新）
</div>

现在你只需向端点传入一个 `prompt`，即可在无需 schema 的情况下完成提取。LLM 会自行确定数据结构。

<CodeGroup>
  <ExtractNoSchemaCURL />
</CodeGroup>

输出：

<ExtractNoSchemaOutput />

<div id="new-crawl-webhook">
  ## 新的爬取 Webhook
</div>

你现在可以向 `/crawl` 端点传入 `webhook` 参数。当爬取开始、更新或完成时，Firecrawl 会向你指定的 URL 发送一个 POST 请求。

现在，webhook 会针对每个被爬取的页面触发，而不仅仅是在最终返回整体结果时触发。

<CrawlWebhookCURL />

<div id="webhook-events">
  ### Webhook 事件
</div>

目前有 4 种事件类型：

* `crawl.started` - 当开始爬取时触发。
* `crawl.page` - 每爬取一个页面触发一次。
* `crawl.completed` - 当爬取完成时触发，用于告知已完成。
* `crawl.failed` - 当爬取失败时触发。

<div id="webhook-response">
  ### Webhook 响应
</div>

* `success` - 表示 webhook 是否成功正确地抓取了页面。
* `type` - 发生的事件类型。
* `id` - 抓取任务的 ID。
* `data` - 抓取到的数据（数组）。仅在 `crawl.page` 事件时非空，且当页面抓取成功时将包含 1 个项。其响应格式与 `/scrape` 端点相同。
* `error` - 若 webhook 失败，这里将包含错误信息。

<div id="migrating-from-v0">
  ## 从 V0 迁移
</div>

> ⚠️ **弃用通知**：V0 端点将于 2025 年 4 月 1 日起停止使用。请在此之前迁移到 V1 端点，以确保服务不中断。

<div id="scrape-endpoint">
  ## /scrape 端点
</div>

全新升级的 `/scrape` 端点经过重构，可靠性更高、使用更便捷。新的 `/scrape` 请求体结构如下：

```json
{
  "url": "<字符串>",
  "formats": ["markdown", "html", "rawHtml", "links", "screenshot", "json"],
  "includeTags": ["<字符串>"],
  "excludeTags": ["<字符串>"],
  "headers": { "<键>": "<值>" },
  "waitFor": 123,
  "timeout": 123
}
```

<div id="formats">
  ### formats
</div>

现在你可以选择希望输出的 formats。你可以指定多个输出 formats。支持的 formats 包括：

* Markdown (markdown)
* HTML (html)
* 原始 HTML（rawHtml，未作任何修改）
* 截图（screenshot 或 screenshot@fullPage）
* 链接（links）
* JSON（json）

默认情况下，输出仅包含 markdown 格式。

<div id="details-on-the-new-request-body">
  ### 新请求体的详细信息
</div>

下表概述了 V1 中 `/scrape` 端点的请求体参数变更。

| Parameter | Change | Description |
| --------- | ------ | ----------- |
| `onlyIncludeTags` | Moved and Renamed | 移至根级别，并重命名为 `includeTags`。 |
| `removeTags` | Moved and Renamed | 移至根级别，并重命名为 `excludeTags`。 |
| `onlyMainContent`| Moved | 移至根级别。默认值为 `true`。 |
| `waitFor`| Moved | 移至根级别。 |
| `headers`| Moved | 移至根级别。 |
| `parsePDF`| Moved | 移至根级别。 |
| `extractorOptions`| No Change | |
| `timeout`| No Change | |
| `pageOptions` | Removed | 不再需要 `pageOptions` 参数。抓取选项已移至根级别。 |
| `replaceAllPathsWithAbsolutePaths` | Removed | 不再需要 `replaceAllPathsWithAbsolutePaths`。现在每个路径默认为绝对路径。 |
| `includeHtml`| Removed | 请改为在 `formats` 中加入 `"html"`。 |
| `includeRawHtml`| Removed | 请改为在 `formats` 中加入 `"rawHtml"`。 |
| `screenshot`| Removed | 请改为在 `formats` 中加入 `"screenshot"`。 |
| `fullPageScreenshot`| Removed | 请改为在 `formats` 中加入 `"screenshot@fullPage"`。 |
| `extractorOptions` | Removed | 请改为使用 `"json"` 格式，并配合 `jsonOptions` 对象。 |

新的 `"json"` 格式在 [llm-extract](/zh/features/extract) 部分中有说明。

<div id="crawl-endpoint">
  ## /crawl 端点
</div>

我们也在 `v1` 中更新了 `/crawl` 端点。请查看下方改进后的请求体：

```json
{
  "url": "<string>",
  "excludePaths": ["<string>"],
  "includePaths": ["<string>"],
  "maxDepth": 2,
  "ignoreSitemap": true,
  "limit": 10,
  "allowBackwardLinks": true,
  "allowExternalLinks": true,
  "scrapeOptions": {
    // 与 /scrape 端点中的选项相同
    "formats": ["markdown", "html", "rawHtml", "screenshot", "links"]
    "headers": { "<key>": "<value>" },
    "includeTags": ["<string>"],
    "excludeTags": ["<string>"],
    "onlyMainContent": true,
    "waitFor": 123
  }
}
```

<div id="details-on-the-new-request-body">
  ### 新请求体的详细说明
</div>

下表概述了 V1 中 `/crawl` 端点请求体参数的变更。

| Parameter | Change | Description |
| --------- | ------ | ----------- |
| `pageOptions` | Renamed | 重命名为 `scrapeOptions`。 |
| `includes` | Moved and Renamed | 移至根级。重命名为 `includePaths`。 |
| `excludes` | Moved and Renamed | 移至根级。重命名为 `excludePaths`。 |
| `allowBackwardCrawling` | Moved and Renamed | 移至根级。重命名为 `allowBackwardLinks`。 |
| `allowExternalLinks` | Moved | 移至根级。 |
| `maxDepth` | Moved | 移至根级。 |
| `ignoreSitemap` | Moved | 移至根级。 |
| `limit` | Moved | 移至根级。 |
| `crawlerOptions` | Removed | 不再需要 `crawlerOptions` 参数。抓取选项已移至根级。 |
| `timeout` | Removed | 请改用 `scrapeOptions` 中的 `timeout`。 |