---
title: '爬取'
description: 'Firecrawl 可递归遍历指定 URL 的子域并收集内容'
og:title: '爬取 | Firecrawl'
og:description: 'Firecrawl 可递归遍历指定 URL 的子域并收集内容'
---

import InstallationPython from '/snippets/zh/v1/installation/python.mdx';
import InstallationNode from '/snippets/zh/v1/installation/js.mdx';
import InstallationGo from '/snippets/zh/v1/installation/go.mdx';
import InstallationRust from '/snippets/zh/v1/installation/rust.mdx';
import CrawlPython from '/snippets/zh/v1/crawl/base/python.mdx';
import CrawlNode from '/snippets/zh/v1/crawl/base/js.mdx';
import CrawlGo from '/snippets/zh/v1/crawl/base/go.mdx';
import CrawlRust from '/snippets/zh/v1/crawl/base/rust.mdx';
import CrawlCURL from '/snippets/zh/v1/crawl/base/curl.mdx';
import AsyncCrawlOutput from '/snippets/zh/v1/crawl-async/base/output.mdx';
import CheckCrawlJobPython from '/snippets/zh/v1/crawl-status/short/python.mdx';
import CheckCrawlJobNode from '/snippets/zh/v1/crawl-status/short/js.mdx';
import CheckCrawlJobGo from '/snippets/zh/v1/crawl-status/short/go.mdx';
import CheckCrawlJobRust from '/snippets/zh/v1/crawl-status/short/rust.mdx';
import CheckCrawlJobCURL from '/snippets/zh/v1/crawl-status/short/curl.mdx';
import CheckCrawlJobOutputScraping from '/snippets/zh/v1/crawl-status/base/output-scraping.mdx';
import CheckCrawlJobOutputCompleted from '/snippets/zh/v1/crawl-status/base/output-completed.mdx';
import CrawlWebSocketPython from '/snippets/zh/v1/crawl-websocket/base/python.mdx';
import CrawlWebSocketNode from '/snippets/zh/v1/crawl-websocket/base/js.mdx';
import CrawlWebhookCURL from '/snippets/zh/v1/crawl-webhook/base/curl.mdx';
import PythonCrawlExample from '/snippets/zh/v1/crawl/sdk-example/python.mdx';
import NodeCrawlExample from '/snippets/zh/v1/crawl/sdk-example/js.mdx';
import PythonCrawlExampleResponse from '/snippets/zh/v1/crawl/sdk-example/python-response.mdx';
import NodeCrawlExampleResponse from '/snippets/zh/v1/crawl/sdk-example/js-response.mdx';
import FastCrawlPython from '/snippets/zh/v1/crawl/fast/python.mdx';
import FastCrawlNode from '/snippets/zh/v1/crawl/fast/js.mdx';
import FastCrawlGo from '/snippets/zh/v1/crawl/fast/go.mdx';
import FastCrawlRust from '/snippets/zh/v1/crawl/fast/rust.mdx';
import FastCrawlCURL from '/snippets/zh/v1/crawl/fast/curl.mdx';

Firecrawl 高效爬取网站并提取全面数据，同时规避阻拦。流程如下：

1. **URL 分析：** 扫描站点地图并爬取网站以识别链接
2. **遍历：** 递归跟随链接以发现所有子页面
3. **抓取：** 从每个页面提取内容，处理 JS 与请求速率限制
4. **输出：** 将数据转换为整洁的 Markdown 或结构化格式

这确保能从任意起始 URL 全面收集数据。

<div id="crawling">
  ## 爬虫
</div>

<div id="crawl-endpoint">
  ### /crawl 端点
</div>

用于抓取指定 URL 及其所有可访问的子页面。该操作会提交一个抓取任务，并返回任务 ID 以供查询抓取状态。

<Warning>
  默认情况下，如果页面中的子链接不是你提供的 URL 的子级，Crawl 会忽略这些链接。
  因此，当你抓取 website.com/blogs/ 时，website.com/other-parent/blog-1
  不会返回。如果需要抓取 website.com/other-parent/blog-1，请使用
  `crawlEntireDomain` 参数。若要在抓取 website.com 时同时抓取类似 blog.website.com
  的子域，请使用 `allowSubdomains` 参数。
</Warning>

<div id="installation">
  ### 安装
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationGo />

  <InstallationRust />
</CodeGroup>

<div id="usage">
  ### 使用方法
</div>

<CodeGroup>
  <CrawlPython />

  <CrawlNode />

  <CrawlGo />

  <CrawlRust />

  <CrawlCURL />
</CodeGroup>

<div id="api-response">
  ### API 响应
</div>

如果你使用 cURL 或 SDK 的 `async crawl` 函数，将返回一个 `ID`，可用于查询抓取状态。

<Note>
  如果你使用 SDK，请查看下方的 SDK 响应部分
  [见此](#sdk-response)。
</Note>

<AsyncCrawlOutput />

<div id="check-crawl-job">
  ### 检查抓取任务
</div>

用于检查抓取任务的状态并获取其结果。

<Note>
  此端点仅适用于正在进行或刚刚完成的抓取任务。{' '}
</Note>

<CodeGroup>
  <CheckCrawlJobPython />

  <CheckCrawlJobNode />

  <CheckCrawlJobGo />

  <CheckCrawlJobRust />

  <CheckCrawlJobCURL />
</CodeGroup>

<div id="response-handling">
  #### 响应处理
</div>

响应内容会根据抓取任务的状态而变化。

对于未完成的抓取，或响应大小超过 10MB 的情况，会返回一个 `next` URL 参数。你需要请求该 URL 以获取下一段 10MB 的数据。若无 `next` 参数，则表示抓取数据已结束。

skip 参数用于设置每个结果分块中返回的最大条目数。

<Info>
  仅在直接调用 API 时，skip 和 next 参数才适用。
  若使用 SDK，我们会代为处理，并一次性返回所有
  结果。
</Info>

<CodeGroup>
  <CheckCrawlJobOutputScraping />

  <CheckCrawlJobOutputCompleted />
</CodeGroup>

<div id="sdk-response">
  ### SDK 响应
</div>

SDK 提供两种爬取 URL 的方式：

1. 同步爬取（`crawl_url`/`crawlUrl`）：
   * 等待爬取完成并返回完整响应
   * 自动处理分页
   * 推荐用于大多数场景

<CodeGroup>
  <PythonCrawlExample />

  <NodeCrawlExample />
</CodeGroup>

响应包含爬取状态和所有已抓取的数据：

<CodeGroup>
  <PythonCrawlExampleResponse />

  <NodeCrawlExampleResponse />
</CodeGroup>

2. 异步爬取（`async_crawl_url`/`asyncCrawlUrl`）：
   * 立即返回一个爬取 ID
   * 允许手动检查状态
   * 适用于长时间运行的爬取或自定义轮询逻辑

<CodeGroup>
  <AsyncCrawlPython />

  <AsyncCrawlNode />
</CodeGroup>

<div id="faster-crawling">
  ## 更快的爬取
</div>

在不需要最新数据时，可将爬取速度提升至 500%。在 `scrapeOptions` 中添加 `maxAge`，在可用时使用页面缓存数据。

<CodeGroup>
  <FastCrawlPython />

  <FastCrawlNode />

  <FastCrawlGo />

  <FastCrawlRust />

  <FastCrawlCURL />
</CodeGroup>

**工作原理：**

* 爬取中的每个页面都会检查我们是否有比 `maxAge` 更新的缓存数据
* 如果有，将立即从缓存返回（速度提升约 500%）
* 如果没有，则实时抓取页面并缓存结果
* 非常适合爬取文档站点、产品目录或其他相对静态的内容

有关 `maxAge` 的更多用法，请参阅 [Faster Scraping](/zh/features/fast-scraping) 文档。

<div id="crawl-websocket">
  ## 爬取 WebSocket
</div>

Firecrawl 提供基于 WebSocket 的方法 `Crawl URL and Watch`，用于实时抽取与监控数据。你可以从一个 URL 发起爬取，并通过页面上限、允许的域名、输出 formats 等选项进行自定义，适用于需要即时处理数据的场景。

<CodeGroup>
  <CrawlWebSocketPython />

  <CrawlWebSocketNode />
</CodeGroup>

<div id="crawl-webhook">
  ## 爬取 Webhook
</div>

你可以配置 webhook，在爬取过程中实时接收通知。这样可以在页面被抓取后立即处理，无需等待整个爬取完成。

<CrawlWebhookCURL />

有关事件类型、载荷结构和实现示例的完整 webhook 文档，请参阅[Webhook 文档](/zh/webhooks/overview)。

<div id="quick-reference">
  ### 快速参考
</div>

**事件类型：**

* `crawl.started` - 爬取开始时
* `crawl.page` - 每个成功抓取的页面
* `crawl.completed` - 爬取结束时
* `crawl.failed` - 爬取出错时

**基本载荷：**

```json
{
  "success": true,
  "type": "crawl.page",
  "id": "crawl-job-id",
  "data": [...], // 'page' 事件的页面数据
  "metadata": {}, // 您的自定义元数据
  "error": null
}
```

<Note>
  关于 webhook 的详细配置、安全最佳实践及故障排除，请参阅 [Webhooks 文档](/zh/webhooks/overview)。
</Note>
