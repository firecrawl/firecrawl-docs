---
title: "批量抓取"
description: "批量抓取多个 URL"
og:title: "批量抓取 | Firecrawl"
og:description: "批量抓取多个 URL"
---

import BatchScrapePython from "/snippets/zh/v1/batch-scrape/base/python.mdx";
import BatchScrapeNode from "/snippets/zh/v1/batch-scrape/base/js.mdx";
import BatchScrapeCURL from "/snippets/zh/v1/batch-scrape/base/curl.mdx";
import BatchScrapeOutput from "/snippets/zh/v1/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/zh/v1/batch-scrape/base/async-output.mdx";
import BatchScrapeExtractPython from "/snippets/zh/v1/batch-scrape/extract/python.mdx";
import BatchScrapeExtractNode from "/snippets/zh/v1/batch-scrape/extract/js.mdx";
import BatchScrapeExtractCURL from "/snippets/zh/v1/batch-scrape/extract/curl.mdx";
import BatchScrapeExtractOutput from "/snippets/zh/v1/batch-scrape/extract/output.mdx";
import BatchScrapeExtractAsyncOutput from "/snippets/zh/v1/batch-scrape/extract/async-output.mdx";
import BatchScrapeWebhookCURL from "/snippets/zh/v1/batch-scrape-webhook/base/curl.mdx";

<div id="batch-scraping-multiple-urls">
  ## 批量抓取多个 URL
</div>

你现在可以同时批量抓取多个 URL。该功能以起始 URL 和可选参数作为输入。params 参数允许你为批量抓取任务指定其他选项，例如输出 formats。

<div id="how-it-works">
  ### 工作原理
</div>

它与 `/crawl` 端点的工作方式非常相似。它会提交一个批量抓取作业，并返回一个作业 ID，用于检查该批量抓取的状态。

SDK 提供两种方式：同步与异步。同步方式会直接返回批量抓取作业的结果；异步方式则返回一个作业 ID，您可以据此检查批量抓取的状态。

<div id="usage">
  ### 使用
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### 响应
</div>

如果你使用 SDK 的同步方法，将直接返回批量抓取任务的结果。否则会返回一个任务 ID，供你用于查询该批量抓取的状态。

<div id="synchronous">
  #### 同步
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### 异步
</div>

随后你可以使用作业 ID 调用 `/batch/scrape/{id}` 端点来查看批量抓取的状态。该端点适用于作业仍在运行期间，或刚完成后使用，**因为批量抓取作业会在 24 小时后过期**。

<BatchScrapeAsyncOutput />

<div id="batch-scrape-with-extraction">
  ## 批量抓取与提取
</div>

你也可以使用批量抓取端点从页面提取结构化数据。如果你想对一组 URL 获取相同的结构化数据，这会很有帮助。

<CodeGroup>
  <BatchScrapeExtractPython />

  <BatchScrapeExtractNode />

  <BatchScrapeExtractCURL />
</CodeGroup>

<div id="response">
  ### 响应
</div>

<div id="synchronous">
  #### 同步
</div>

<BatchScrapeExtractOutput />

<div id="asynchronous">
  #### 异步
</div>

<BatchScrapeExtractAsyncOutput />

<div id="batch-scrape-with-webhooks">
  ## 通过 Webhooks 批量抓取
</div>

你可以配置 webhooks，在批量任务中每个 URL 被抓取时接收实时通知。这样可以立即处理结果，无需等待整个批次完成。

<BatchScrapeWebhookCURL />

有关完整的 webhook 文档（包括事件类型、payload 结构和实现示例），请参阅 [Webhooks 文档](/zh/features/webhooks)。

<div id="quick-reference">
  ### 快速参考
</div>

**事件类型：**

* `batch_scrape.started` - 批量抓取开始时
* `batch_scrape.page` - 每个 URL 抓取成功时
* `batch_scrape.completed` - 所有 URL 处理完成时
* `batch_scrape.failed` - 批量抓取发生错误时

**基础负载：**

```json
{
  "success": true,
  "type": "batch_scrape.page",
  "id": "batch-job-id",
  "data": [...], // “page” 事件的页面数据
  "metadata": {}, // 你的自定义元数据
  "error": null
}
```

<Note>
  有关 webhook 的详细配置、安全最佳实践以及故障排除，请参阅[Webhooks 文档](/zh/features/webhooks)。
</Note>
