---
title: "批量抓取"
description: "批量抓取多个 URL"
og:title: "批量抓取 | Firecrawl"
og:description: "批量抓取多个 URL"
---

import BatchScrapePython from '/snippets/zh/v1/batch-scrape/base/python.mdx';
import BatchScrapeNode from '/snippets/zh/v1/batch-scrape/base/js.mdx';
import BatchScrapeCURL from '/snippets/zh/v1/batch-scrape/base/curl.mdx';
import BatchScrapeOutput from '/snippets/zh/v1/batch-scrape/base/output.mdx';
import BatchScrapeAsyncOutput from '/snippets/zh/v1/batch-scrape/base/async-output.mdx';
import BatchScrapeExtractPython from '/snippets/zh/v1/batch-scrape/extract/python.mdx';
import BatchScrapeExtractNode from '/snippets/zh/v1/batch-scrape/extract/js.mdx';
import BatchScrapeExtractCURL from '/snippets/zh/v1/batch-scrape/extract/curl.mdx';
import BatchScrapeExtractOutput from '/snippets/zh/v1/batch-scrape/extract/output.mdx';
import BatchScrapeExtractAsyncOutput from '/snippets/zh/v1/batch-scrape/extract/async-output.mdx';
import BatchScrapeWebhookCURL from '/snippets/zh/v1/batch-scrape-webhook/base/curl.mdx';

<div id="batch-scraping-multiple-urls">
  ## 批量抓取多个 URL
</div>

你现在可以同时批量抓取多个 URL。该功能以起始 URL 和可选参数作为输入。params 参数允许你为批量抓取任务指定其他选项，例如输出 formats。

<div id="how-it-works">
  ### 工作原理
</div>

它的工作方式与 `/crawl` 端点非常相似：提交一个批量抓取任务，并返回一个任务 ID，用于检查该批量抓取的状态。

SDK 提供两种用法：同步和异步。同步方式会直接返回批量抓取任务的结果；异步方式则返回一个任务 ID，供您查询批量抓取的状态。

<div id="usage">
  ### 使用
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### 响应
</div>

如果你使用 SDK 的同步方法，将直接返回批量抓取任务的结果。否则会返回一个任务 ID，供你用于查询该批量抓取的状态。

<div id="synchronous">
  #### 同步
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### 异步
</div>

随后你可以使用作业 ID 调用 `/batch/scrape/{id}` 端点来查看批量抓取的状态。该端点适用于作业仍在运行期间，或刚完成后使用，**因为批量抓取作业会在 24 小时后过期**。

<BatchScrapeAsyncOutput />

<div id="batch-scrape-with-extraction">
  ## 批量抓取与提取
</div>

你也可以使用批量抓取端点从页面提取结构化数据。如果你想对一组 URL 获取相同的结构化数据，这会很有帮助。

<CodeGroup>
  <BatchScrapeExtractPython />

  <BatchScrapeExtractNode />

  <BatchScrapeExtractCURL />
</CodeGroup>

<div id="response">
  ### 响应
</div>

<div id="synchronous">
  #### 同步
</div>

<BatchScrapeExtractOutput />

<div id="asynchronous">
  #### 异步
</div>

<BatchScrapeExtractAsyncOutput />

<div id="batch-scrape-with-webhooks">
  ## 通过 Webhook 批量抓取
</div>

你可以配置 Webhook，在批次中的每个 URL 被抓取时接收实时通知。这样你可以立即处理结果，而无需等待整个批次完成。

<BatchScrapeWebhookCURL />

有关完整的 Webhook 文档（包括事件类型、负载结构和实现示例），请参阅[Webhook 文档](/zh/webhooks/overview)。

<div id="quick-reference">
  ### 快速参考
</div>

**事件类型：**

* `batch_scrape.started` - 批量抓取开始时触发
* `batch_scrape.page` - 每个成功抓取的 URL 触发一次
* `batch_scrape.completed` - 所有 URL 处理完成时触发
* `batch_scrape.failed` - 批量抓取出错时触发

**基本载荷：**

```json
{
  "success": true,
  "type": "batch_scrape.page",
  "id": "batch-job-id",
  "data": [...], // 'page' 事件的页面数据
  "metadata": {}, // 您的自定义元数据
  "error": null
}
```

<Note>
  有关 webhook 的详细配置、安全最佳实践和
  故障排除，请参阅[Webhooks 文档](/zh/webhooks/overview)。
</Note>
