---
title: '批量抓取'
description: '批量抓取多个 URL'
og:title: '批量抓取 | Firecrawl'
og:description: '批量抓取多个 URL'
---

import BatchScrapePython from '/snippets/zh/v1/batch-scrape/base/python.mdx';
import BatchScrapeNode from '/snippets/zh/v1/batch-scrape/base/js.mdx';
import BatchScrapeCURL from '/snippets/zh/v1/batch-scrape/base/curl.mdx';
import BatchScrapeOutput from '/snippets/zh/v1/batch-scrape/base/output.mdx';
import BatchScrapeAsyncOutput from '/snippets/zh/v1/batch-scrape/base/async-output.mdx';
import BatchScrapeExtractPython from '/snippets/zh/v1/batch-scrape/extract/python.mdx';
import BatchScrapeExtractNode from '/snippets/zh/v1/batch-scrape/extract/js.mdx';
import BatchScrapeExtractCURL from '/snippets/zh/v1/batch-scrape/extract/curl.mdx';
import BatchScrapeExtractOutput from '/snippets/zh/v1/batch-scrape/extract/output.mdx';
import BatchScrapeExtractAsyncOutput from '/snippets/zh/v1/batch-scrape/extract/async-output.mdx';
import BatchScrapeWebhookCURL from '/snippets/zh/v1/batch-scrape-webhook/base/curl.mdx';

<div id='batch-scraping-multiple-urls'>## 批量抓取多个 URL</div>

你现在可以同时批量抓取多个 URL。该功能以起始 URL 和可选参数作为输入。params 参数允许你为批量抓取任务指定其他选项，例如输出 formats。

<div id='how-it-works'>### 工作原理</div>

它与 `/crawl` 端点的工作方式非常相似。它会提交一个批量抓取作业，并返回一个作业 ID，用于检查该批量抓取的状态。

SDK 提供两种方式：同步与异步。同步方式会直接返回批量抓取作业的结果；异步方式则返回一个作业 ID，您可以据此检查批量抓取的状态。

<div id='usage'>### 使用</div>

<CodeGroup>
  <BatchScrapePython />

{' '}
<BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id='response'>### 响应</div>

如果你使用 SDK 的同步方法，将直接返回批量抓取任务的结果。否则会返回一个任务 ID，供你用于查询该批量抓取的状态。

<div id='synchronous'>#### 同步</div>

<BatchScrapeOutput />

<div id='asynchronous'>#### 异步</div>

随后你可以使用作业 ID 调用 `/batch/scrape/{id}` 端点来查看批量抓取的状态。该端点适用于作业仍在运行期间，或刚完成后使用，**因为批量抓取作业会在 24 小时后过期**。

<BatchScrapeAsyncOutput />

<div id='batch-scrape-with-extraction'>## 批量抓取与提取</div>

你也可以使用批量抓取端点从页面提取结构化数据。如果你想对一组 URL 获取相同的结构化数据，这会很有帮助。

<CodeGroup>
  <BatchScrapeExtractPython />

{' '}
<BatchScrapeExtractNode />

  <BatchScrapeExtractCURL />
</CodeGroup>

<div id='response'>### 响应</div>

<div id='synchronous'>#### 同步</div>

<BatchScrapeExtractOutput />

<div id='asynchronous'>#### 异步</div>

<BatchScrapeExtractAsyncOutput />

<div id='batch-scrape-with-webhooks'>## 通过 Webhooks 批量抓取</div>

你可以配置 webhooks，在批量任务中每个 URL 被抓取时接收实时通知。这样可以立即处理结果，无需等待整个批次完成。

<BatchScrapeWebhookCURL />

有关完整的 webhook 文档（包括事件类型、payload 结构和实现示例），请参阅 [Webhooks 文档](/zh/webhooks/overview)。

<div id='quick-reference'>### 快速参考</div>

**事件类型：**

- `batch_scrape.started` - 批量抓取开始时
- `batch_scrape.page` - 每个 URL 抓取成功时
- `batch_scrape.completed` - 所有 URL 处理完成时
- `batch_scrape.failed` - 批量抓取发生错误时

**基础负载：**

```json
{
  "success": true,
  "type": "batch_scrape.page",
  "id": "batch-job-id",
  "data": [...], // “page” 事件的页面数据
  "metadata": {}, // 你的自定义元数据
  "error": null
}
```

<Note>
  有关 webhook 的详细配置、安全最佳实践以及故障排除，请参阅[Webhooks
  文档](/zh/webhooks/overview)。
</Note>
