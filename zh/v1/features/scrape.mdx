---
title: "抓取"
description: "将任意 URL 转换为清洁数据"
og:title: "抓取 | Firecrawl"
og:description: "将任意 URL 转换为清洁数据"
---

import InstallationPython from "/snippets/zh/v1/installation/python.mdx";
import InstallationNode from "/snippets/zh/v1/installation/js.mdx";
import InstallationGo from "/snippets/zh/v1/installation/go.mdx";
import InstallationRust from "/snippets/zh/v1/installation/rust.mdx";
import ScrapePython from "/snippets/zh/v1/scrape/base/python.mdx";
import ScrapeNode from "/snippets/zh/v1/scrape/base/js.mdx";
import ScrapeGo from "/snippets/zh/v1/scrape/base/go.mdx";
import ScrapeRust from "/snippets/zh/v1/scrape/base/rust.mdx";
import ScrapeCURL from "/snippets/zh/v1/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/zh/v1/scrape/base/output.mdx";
import ExtractCURL from "/snippets/zh/v1/llm-extract/base/curl.mdx";
import ExtractPython from "/snippets/zh/v1/llm-extract/base/python.mdx";
import ExtractNode from "/snippets/zh/v1/llm-extract/base/js.mdx";
import ExtractOutput from "/snippets/zh/v1/llm-extract/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/zh/v1/llm-extract/no-schema/curl.mdx";
import ExtractNoSchemaOutput from "/snippets/zh/v1/llm-extract/no-schema/output.mdx";
import ScrapeActionsPython from "/snippets/zh/v1/scrape/actions/python.mdx";
import ScrapeActionsNode from "/snippets/zh/v1/scrape/actions/js.mdx";
import ScrapeActionsCURL from "/snippets/zh/v1/scrape/actions/curl.mdx";
import ScrapeActionsOutput from "/snippets/zh/v1/scrape/actions/output.mdx";
import BatchScrapePython from "/snippets/zh/v1/batch-scrape/base/python.mdx";
import BatchScrapeNode from "/snippets/zh/v1/batch-scrape/base/js.mdx";
import BatchScrapeCURL from "/snippets/zh/v1/batch-scrape/base/curl.mdx";
import BatchScrapeOutput from "/snippets/zh/v1/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/zh/v1/batch-scrape/base/async-output.mdx";
import ScrapeLocationPython from "/snippets/zh/v1/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/zh/v1/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/zh/v1/scrape/location/curl.mdx";

Firecrawl 将网页转换为 markdown，非常适合 LLM 应用。

* 代管复杂问题：代理、缓存、速率限制、被 JS 屏蔽的内容
* 处理动态内容：动态网站、JS 渲染页面、PDF、图片
* 输出干净的 markdown、结构化数据、截图或 HTML。

详细请参见 [Scrape 端点 API 参考](https://docs.firecrawl.dev/api-reference/endpoint/scrape)。

<div id="scraping-a-url-with-firecrawl">
  ## 使用 Firecrawl 抓取 URL
</div>

<div id="scrape-endpoint">
  ### /scrape 端点
</div>

用于抓取指定 URL 的内容。

<div id="installation">
  ### 安装
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationGo />

  <InstallationRust />
</CodeGroup>

<div id="usage">
  ### 使用方法
</div>

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeGo />

  <ScrapeRust />

  <ScrapeCURL />
</CodeGroup>

有关参数的更多信息，请参见 [API 参考](https://docs.firecrawl.dev/api-reference/endpoint/scrape)。

<div id="response">
  ### 响应
</div>

各 SDK 会直接返回数据对象。cURL 将按下方所示原样返回载荷。

<ScrapeResponse />

<div id="scrape-formats">
  ## 抓取 formats
</div>

你现在可以选择输出所需的 formats。你可以指定多个输出 formats。支持的 formats 有：

* Markdown (markdown)
* HTML (html)
* 原始 HTML（rawHtml，未经任何修改）
* 截图（screenshot 或 screenshot@fullPage）
* 链接（links）
* JSON（json）- 结构化输出

输出的键将与您选择的 format 相匹配。

<div id="extract-structured-data">
  ## 提取结构化数据
</div>

<div id="scrape-with-json-endpoint">
  ### /scrape（使用 json）端点
</div>

用于从抓取的页面中提取结构化数据。

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

输出：

<ExtractOutput />

<div id="extracting-without-schema-new">
  ### 无需 schema 的提取（新）
</div>

现在你只需向端点传入一个 `prompt`，即可在没有 schema 的情况下进行提取。LLM 会自行决定数据结构。

<CodeGroup>
  <ExtractNoSchemaCURL />
</CodeGroup>

输出：

<ExtractNoSchemaOutput />

<div id="json-options-object">
  ### JSON 选项对象
</div>

`jsonOptions` 对象接受以下参数：

* `schema`：用于抽取的模式（schema）。
* `systemPrompt`：用于抽取的系统提示词。
* `prompt`：在不使用 schema 时用于抽取的提示词。

<div id="interacting-with-the-page-with-actions">
  ## 使用 Actions 与页面交互
</div>

Firecrawl 允许你在抓取页面内容之前在网页上执行各种 actions。这对于与动态内容交互、在页面之间导航，或访问需要用户操作的内容特别有用。

下面是一个示例，展示如何使用 actions 访问 google.com，搜索 Firecrawl，点击第一个结果，并进行截图。

在执行其他 actions 前后，几乎总是需要使用 `wait` action，以留出足够的页面加载时间，这一点非常重要。

<div id="example">
  ### 示例
</div>

<CodeGroup>
  <ScrapeActionsPython />

  <ScrapeActionsNode />

  <ScrapeActionsCURL />
</CodeGroup>

<div id="output">
  ### 输出
</div>

<CodeGroup>
  <ScrapeActionsOutput />
</CodeGroup>

有关 actions 参数的更多信息，请参见[API 参考](https://docs.firecrawl.dev/api-reference/endpoint/scrape)。

<div id="location-and-language">
  ## 位置与语言
</div>

指定国家/地区和首选语言，以根据你的目标位置和语言偏好获取相关内容。

<div id="how-it-works">
  ### 工作原理
</div>

当你指定定位设置时，Firecrawl 会在可用情况下使用合适的代理，并模拟相应的语言和时区设置。默认情况下，如未指定定位，位置将设为“US”。

<div id="usage">
  ### 用法
</div>

要使用位置和语言设置，请在请求体中包含 `location` 对象，并添加以下属性：

* `country`：ISO 3166-1 alpha-2 国家/地区代码（例如，“US”、“AU”、“DE”、“JP”）。默认值为 “US”。
* `languages`：按优先级排列的首选语言和区域设置数组。默认与指定位置的语言一致。

<CodeGroup>
  <ScrapeLocationPython />

  <ScrapeLocationNode />

  <ScrapeLocationCURL />
</CodeGroup>

<div id="batch-scraping-multiple-urls">
  ## 批量抓取多个 URL
</div>

现在你可以同时批量抓取多个 URL。该方法以起始 URL 和可选参数作为入参。通过 params 参数，你可以为批量抓取任务指定其他选项，例如输出 formats。

<div id="how-it-works">
  ### 工作原理
</div>

其工作方式与 `/crawl` 端点非常相似。它会提交一个批量抓取作业，并返回一个作业 ID，供你检查该批量抓取的状态。

SDK 提供两种方式：同步和异步。同步方式会直接返回批量抓取作业的结果；异步方式则返回一个作业 ID，你可以用它来查询批量抓取的状态。

<div id="usage">
  ### 使用方法
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### 响应
</div>

如果你使用 SDK 的同步方法，将返回批量抓取任务的结果。否则，将返回一个作业 ID，供你用于检查批量抓取的状态。

<div id="synchronous">
  #### 同步
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### 异步
</div>

之后可使用作业 ID 调用 `/batch/scrape/{id}` 端点来检查批量抓取的状态。该端点旨在作业仍在运行期间或刚完成后使用，**因为批量抓取作业会在 24 小时后过期**。

<BatchScrapeAsyncOutput />

<div id="stealth-mode">
  ## 隐身模式
</div>

针对具备高级反爬/反机器人防护的网站，Firecrawl 提供隐身代理模式，在抓取高难度站点时能显著提升成功率。

了解更多：[隐身模式](/zh/features/stealth-mode)。

<div id="using-fire-1-with-scrape">
  ## 在 Scrape 中使用 FIRE-1
</div>

你可以将 FIRE-1（代理）与 `/scrape` 端点配合使用，在抓取最终内容前先进行智能导航。

启用 FIRE-1 很简单。只需在你的 `/scrape` 或 `/extract` API 请求中包含一个 `agent` 对象即可：

```json
"agent": {
  "model": "FIRE-1",
  "prompt": "在此输入你的详细导航指令。"
}
```

*注意：* 发起抓取请求时必须提供 `prompt` 字段，它用于精确指示 FIRE-1 如何与网页交互。

<div id="example-usage-with-scrape-endpoint">
  ### 使用 /scrape 端点的示例
</div>

下面是一个使用 FIRE-1 搭配 /scrape 端点，从 Y Combinator 获取消费类公司列表的快速示例：

```bash
curl -X POST https://api.firecrawl.dev/v1/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer 你的 API 密钥' \
  -d '{
    "url": "https://ycombinator.com/companies",
    "formats": ["markdown"],
    "agent": {
      "model": "FIRE-1",
      "prompt": "点击相应按钮，获取消费领域的 W22 公司"
    }
  }'
```
