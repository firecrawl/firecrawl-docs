---
title: "高级抓取指南"
description: "了解如何通过高级选项优化你的 Firecrawl 抓取。"
og:title: "高级抓取指南 | Firecrawl"
og:description: "了解如何通过高级选项优化你的 Firecrawl 抓取。"
---

本指南将带你了解 Firecrawl 的各个端点，以及如何结合其所有参数充分使用它们。

<div id="basic-scraping-with-firecrawl-scrape">
  ## 使用 Firecrawl 进行基础抓取（/scrape）
</div>

要抓取单个页面并获取干净的 Markdown 内容，可以使用 `/scrape` 端点。

<CodeGroup>

```python Python
# pip install firecrawl-py

from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR_API_KEY")

content = app.scrape_url("https://docs.firecrawl.dev")
```

```JavaScript JavaScript
// npm install @mendable/firecrawl-js

import { FirecrawlApp } from 'firecrawl-js';

const app = new FirecrawlApp({ apiKey: 'YOUR_API_KEY' });

const content = await app.scrapeUrl('https://docs.firecrawl.dev');
```

```go Go
// go get github.com/mendableai/firecrawl-go

import (
  "fmt"
  "log"

  "github.com/mendableai/firecrawl-go"
)

func main() {
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
    log.Fatalf("Failed to initialize FirecrawlApp: %v", err)
  }

  content, err := app.ScrapeURL("docs.firecrawl.dev", nil)
  if err != nil {
    log.Fatalf("Failed)
  }
}
```

```rust Rust
// Install the firecrawl_rs crate with Cargo

use firecrawl_rs::FirecrawlApp;
#[tokio::main]
async fn main() {
  // Initialize the FirecrawlApp with the API key
  let api_key = "YOUR_API_KEY";
  let api_url = "https://api.firecrawl.dev";
  let app = FirecrawlApp::new(api_key, api_url).expect("Failed to initialize FirecrawlApp");

  let scrape_result = app.scrape_url("https://docs.firecrawl.dev", None).await;
  match scrape_result {
    Ok(data) => println!("Scrape Result:\n{}", data["markdown"]),
    Err(e) => eprintln!("Scrape failed: {}", e),
  }
}
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

</CodeGroup>

<div id="scraping-pdfs">
  ## 抓取 PDF
</div>

**Firecrawl 默认支持抓取 PDF。** 你可以使用 `/scrape` 端点抓取 PDF 链接并获取其文本内容。可通过将 `pageOptions.parsePDF` 设置为 `false` 来禁用。

<div id="page-options">
  ## 页面选项
</div>

使用 `/scrape` 端点时，可通过 `pageOptions` 参数自定义抓取行为。可用选项如下：

<div id="getting-cleaner-content-with-onlymaincontent">
  ### 使用 `onlyMainContent` 获取更纯净的内容
</div>

- **类型**: `boolean`
- **说明**: 仅返回页面主体内容，不包含页眉、导航、页脚等。
- **默认**: `false`

<div id="getting-the-html-with-includehtml">
  ### 使用 `includeHtml` 获取 HTML
</div>

- **类型**: `boolean`
- **描述**: 在响应中包含页面的 HTML 版本内容，这将添加一个 `html` 字段。
- **默认值**: `false`

<div id="getting-the-raw-html-with-includerawhtml">
  ### 使用 `includeRawHtml` 获取原始 HTML
</div>

- **类型**: `boolean`
- **描述**: 在响应中包含页面的原始 HTML 内容。启用后，响应会新增 `rawHtml` 字段。
- **默认值**: `false`

<div id="getting-a-screenshot-of-the-page-with-screenshot">
  ### 使用 `screenshot` 获取页面截图
</div>

- **类型**: `boolean`
- **描述**: 在抓取的页面中包含顶部区域的截图。
- **默认值**: `false`

<div id="waiting-for-the-page-to-load-with-waitfor">
  ### 使用 `waitFor` 等待页面加载
</div>

- **类型**: `integer`
- **描述**: 仅在万不得已时使用。在获取内容之前，等待指定的毫秒数以确保页面加载完成。
- **默认值**: `0`

<div id="example-usage">
  ### 使用示例
</div>

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "pageOptions": {
        "onlyMainContent": true,
        "includeHtml": true,
        "includeRawHtml": true,
        "screenshot": true,
        "waitFor": 5000
      }
    }'
```

在此示例中，scraper 将：

* 仅返回页面的主要内容。
* 在响应的 `html` 字段中包含原始 HTML 内容。
* 在获取内容前等待 5000 毫秒（5 秒），以便页面完成加载。

对应的 API 参考文档：[Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)

<div id="extractor-options">
  ## 提取器选项
</div>

使用 `/scrape` 端点时，可以通过 `extractorOptions` 参数为从页面内容中**提取结构化信息**配置选项。以下是可用的选项：

### mode

- **类型**: `string`
- **枚举**: `["llm-extraction", "llm-extraction-from-raw-html"]`
- **描述**: 要使用的提取模式。

  - `llm-extraction`: 从清理并解析后的内容中提取信息。
  - `llm-extraction-from-raw-html`: 直接从原始 HTML 提取信息。

- **类型**: `string`
- **描述**: 用于描述应从页面提取哪些信息的提示。

<div id="extractionschema">
  ### extractionSchema
</div>

- **类型**: `object`
- **描述**: 提取数据所用的模式（schema），用于定义提取结果的数据结构。

<div id="example-usage">
  ### 使用示例
</div>

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev/",
      "extractorOptions": {
        "mode": "llm-extraction",
        "extractionPrompt": "根据页面信息，从该 schema 中提取相应字段的数据。 ",
        "extractionSchema": {
          "type": "object",
          "properties": {
            "company_mission": {
                      "type": "string"
            },
            "supports_sso": {
                      "type": "boolean"
            },
            "is_open_source": {
                      "type": "boolean"
            },
            "is_in_yc": {
                      "type": "boolean"
            }
          },
          "required": [
            "company_mission",
            "supports_sso",
            "is_open_source",
            "is_in_yc"
          ]
        }
      }
    }'
```

```json
{
  "success": true,
  "data": {
    "content": "原始内容",
    "metadata": {
      "title": "Mendable",
      "description": "Mendable 让你轻松构建 AI 聊天应用。导入、定制，然后用一行代码即可在任意位置部署。由 SideGuide 提供",
      "robots": "follow, index",
      "ogTitle": "Mendable",
      "ogDescription": "Mendable 让你轻松构建 AI 聊天应用。导入、定制，然后用一行代码即可在任意位置部署。由 SideGuide 提供",
      "ogUrl": "https://docs.firecrawl.dev/",
      "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",
      "ogLocaleAlternate": [],
      "ogSiteName": "Mendable",
      "sourceURL": "https://docs.firecrawl.dev/"
    },
    "llm_extraction": {
      "company_mission": "基于你的技术资源训练一个安全的 AI，用于回答客户和员工的问题，让你的团队无需亲自处理"
      "supports_sso": true,
      "is_open_source": false,
      "is_in_yc": true
    }
  }
}
```

<div id="adjusting-timeout">
  ## 调整超时
</div>

你可以使用以毫秒为单位的 `timeout` 参数来调整抓取过程的超时时长。

<div id="example-usage">
  ### 使用示例
</div>

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "timeout": 50000
    }'
```

<div id="crawling-multiple-pages">
  ## 爬取多个页面
</div>

要爬取多个页面，可以使用 `/crawl` 端点。该端点允许你指定要爬取的起始 URL，并自动爬取所有可访问的子页面。

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

返回 jobId

```json
{ "jobId": "1234-5678-9101" }
```

<div id="check-crawl-job">
  ### 检查抓取任务
</div>

用于查看抓取任务的状态并获取其结果。

```bash
curl -X GET https://api.firecrawl.dev/v0/crawl/status/1234-5678-9101 \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_API_KEY'
```

<div id="crawler-options">
  ### 爬取器选项
</div>

使用 `/crawl` 端点时，可通过 `crawlerOptions` 参数自定义爬取行为。以下为可用选项：

<div id="includes">
  #### `includes`
</div>

- **类型**: `array`
- **描述**: 爬取时要包含的 URL 模式。仅会爬取与这些模式匹配的 URL。
- **示例**: `["/blog/*", "/products/*"]`

<div id="excludes">
  #### `excludes`
</div>

- **类型**: `array`
- **描述**: 爬取时需排除的 URL 模式。匹配这些模式的 URL 将被跳过。
- **示例**: `["/admin/*", "/login/*"]`

<div id="returnonlyurls">
  #### `returnOnlyUrls`
</div>

- **类型**: `boolean`
- **描述**: 如果设为 `true`，响应将只包含 URL 列表，而非完整的文档数据。
- **默认值**: `false`

<div id="maxdepth">
  #### `maxDepth`
</div>

- **类型**: `integer`
- **描述**: 相对于输入 URL 的最大爬取深度。`maxDepth` 为 0 时仅抓取输入的 URL；为 1 时抓取输入的 URL 及所有一层深的页面；为 2 时抓取输入的 URL 及所有至两层深的页面。更高的取值以此类推。
- **示例**: `2`

<div id="mode">
  #### `mode`
</div>

- **类型**: `string`
- **枚举**: `["default", "fast"]`
- **描述**: 爬取模式。`fast` 模式在网站无 sitemap 时爬取速度可提升 4 倍，但准确性可能较低，且不建议用于高度依赖 JavaScript 渲染的网站。
- **默认值**: `default`

<div id="limit">
  #### `limit`
</div>

- **类型**: `integer`
- **描述**: 爬取的最大页面数。
- **默认值**: `10000`

<div id="example-usage">
  ### 使用示例
</div>

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "crawlerOptions": {
        "includes": ["/blog/*", "/products/*"],
        "excludes": ["/admin/*", "/login/*"],
        "returnOnlyUrls": false,
        "maxDepth": 2,
        "mode": "fast",
        "limit": 1000
      }
    }'
```

在此示例中，爬虫将：

* 仅抓取与模式 `/blog/*` 和 `/products/*` 匹配的 URL。
* 跳过与模式 `/admin/*` 和 `/login/*` 匹配的 URL。
* 返回每个页面的完整文档数据。
* 最大抓取深度为 2。
* 使用快速抓取模式。
* 最多抓取 1000 个页面。

<div id="page-options-crawler-options">
  ## 页面选项 + 爬虫选项
</div>

你可以将 `pageOptions` 与 `crawlerOptions` 参数搭配使用，以自定义完整的爬取流程与行为。

<div id="example-usage">
  ### 使用示例
</div>

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "pageOptions": {
        "onlyMainContent": true,
        "includeHtml": true,
        "includeRawHtml": true,
        "screenshot": true,
        "waitFor": 5000
      },
      "crawlerOptions": {
        "includes": ["/blog/*", "/products/*"],
        "maxDepth": 2,
        "mode": "fast",
      }
    }'
```

在此示例中，爬虫将：

* 仅返回每个页面的主体内容。
* 包含每个页面的原始 HTML 内容。
* 在获取内容前，等待每个页面加载 5000 毫秒。
* 仅爬取与 `/blog/*` 和 `/products/*` 模式匹配的 URL。
* 最多爬取至深度 2。
* 使用快速爬取模式。

<div id="extractor-options-crawler-options">
  ## 提取器选项 + 爬虫选项
</div>

敬请期待……