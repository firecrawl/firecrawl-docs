---
title: "Python"
description: "Firecrawl Python SDK 是 Firecrawl API 的封装，帮助你轻松将网站转换为 Markdown。"
icon: "python"
og:title: "Python SDK | Firecrawl"
og:description: "Firecrawl Python SDK 是 Firecrawl API 的封装，帮助你轻松将网站转换为 Markdown。"
---

import InstallationPython from '/snippets/zh/v2/installation/python.mdx'
import ScrapePythonShort from '/snippets/zh/v2/scrape/short/python.mdx'
import CrawlPythonShort from '/snippets/zh/v2/crawl/short/python.mdx'
import CheckCrawlStatusPythonShort from '/snippets/zh/v2/crawl-status/short/python.mdx'
import StartCrawlPythonShort from '/snippets/zh/v2/start-crawl/short/python.mdx'
import CancelCrawlPythonShort from '/snippets/zh/v2/crawl-delete/short/python.mdx'
import MapPythonShort from '/snippets/zh/v2/map/short/python.mdx'
import ExtractPythonShort from '/snippets/v2/extract/short/python.mdx'
import ScrapeAndCrawlExamplePython from '/snippets/zh/v2/scrape-and-crawl/python.mdx'
import CrawlWebSocketPythonBase from '/snippets/zh/v2/crawl-websocket/base/python.mdx'
import AIOPython from '/snippets/zh/v2/async/base/python.mdx'

<div id="installation">
  ## 安装
</div>

要安装 Firecrawl 的 Python SDK，可以使用 pip：

<InstallationPython />

<div id="usage">
  ## 使用
</div>

1. 在 [firecrawl.dev](https://firecrawl.dev) 获取 API key
2. 将该 API key 设置为名为 `FIRECRAWL_API_KEY` 的环境变量，或在实例化 `Firecrawl` 类时作为参数传入。

以下是使用 SDK 的示例：

<ScrapeAndCrawlExamplePython />

<div id="scraping-a-url">
  ### 抓取单个 URL
</div>

要抓取单个 URL，请使用 `scrape` 方法。它将该 URL 作为参数并返回抓取到的文档。

<ScrapePythonShort />

<div id="crawl-a-website">
  ### 爬取网站
</div>

要爬取网站，请使用 `crawl` 方法。它接收起始 URL 和可选的 options 作为参数。通过 options，你可以为爬取任务指定其他设置，例如爬取的最大页面数、允许的域名，以及输出 formats。有关自动/手动分页与限制，请参见 [Pagination](#pagination)。

<CrawlPythonShort />

<div id="start-a-crawl">
  ### 开始 Crawl
</div>

<Tip>想要非阻塞方式？请查看下方的[异步类](#async-class)部分。</Tip>

使用 `start_crawl` 启动任务，无需等待。它会返回一个用于检查状态的任务 `ID`。需要直到完成才返回的阻塞式等待器时，请使用 `crawl`。分页行为与限制见[分页](#pagination)。

<StartCrawlPythonShort />

<div id="checking-crawl-status">
  ### 检查爬取状态
</div>

要查看爬取任务的状态，请使用 `get_crawl_status` 方法。该方法接收任务 ID 作为参数，并返回该爬取任务的当前状态。

<CheckCrawlStatusPythonShort />

<div id="cancelling-a-crawl">
  ### 取消爬取
</div>

要取消一个爬取任务，使用 `cancel_crawl` 方法。传入由 `start_crawl` 返回的任务 ID 作为参数，该方法会返回取消结果。

<CancelCrawlPythonShort />

<div id="map-a-website">
  ### 网站映射
</div>

使用 `map` 生成网站的 URL 列表。你可以通过选项自定义映射过程，例如排除子域或利用 sitemap。

<MapPythonShort />

{/* ### 从网站提取结构化数据

  要从网站提取结构化数据，请使用 `extract` 方法。它接收要提取数据的 URL、prompt 和 schema 作为参数。schema 是一个 Pydantic 模型，用于定义提取数据的结构。

  <ExtractPythonShort /> */}

<div id="crawling-a-website-with-websockets">
  ### 使用 WebSockets 爬取网站
</div>

要通过 WebSockets 爬取网站，先用 `start_crawl` 启动任务，并使用 `watcher` 辅助工具订阅。调用 `start()` 之前，使用任务 ID 创建一个 watcher，并附加处理器（例如：page、completed、failed）。

<CrawlWebSocketPythonBase />

<div id="pagination">
  ### 分页
</div>

当有更多数据可用时，Firecrawl 的 crawl 和 batch 端点会返回一个 `next` URL。Python SDK 默认会自动分页并汇总所有文档；此时 `next` 为 `None`。你可以禁用自动分页或设置上限。

<div id="crawl">
  #### 爬取
</div>

使用 waiter 方法 `crawl` 可获得最简便的体验，或者启动一个作业并手动翻页。

<div id="simple-crawl-auto-pagination-default">
  ##### 简单抓取（自动分页，默认）
</div>

* 参见[抓取网站](#crawl-a-website)中的默认流程。

<div id="manual-crawl-with-pagination-control-single-page">
  ##### 手动爬取并控制分页（单页）
</div>

* 先启动任务，然后将 `auto_paginate=False`，按页逐次获取。

```python Python
crawl_job = client.start_crawl("https://example.com", limit=100)

status = client.get_crawl_status(crawl_job.id, pagination_config=PaginationConfig(auto_paginate=False))
print("抓取单页：", status.status, "文档数：", len(status.data), "下一页：", status.next)
```

<div id="manual-crawl-with-limits-auto-pagination-early-stop">
  ##### 手动抓取并设定限制（自动分页 + 提前停止）
</div>

* 保持自动分页开启，但可通过 `max_pages`、`max_results` 或 `max_wait_time` 提前停止。

```python Python
status = client.get_crawl_status(
    crawl_job.id,
    pagination_config=PaginationConfig(max_pages=2, max_results=50, max_wait_time=15),
)
print("爬取受限：", status.status, "文档数：", len(status.data), "下一页：", status.next)
```

<div id="batch-scrape">
  #### 批量抓取
</div>

使用 waiter 方法 `batch_scrape`，或启动任务后手动分页处理。

<div id="simple-batch-scrape-auto-pagination-default">
  ##### 简单批量爬取（自动分页，默认）
</div>

* 参见默认流程：[Batch Scrape](/zh/features/batch-scrape)。

<div id="manual-batch-scrape-with-pagination-control-single-page">
  ##### 手动批量抓取并控制分页（单页）
</div>

* 先启动作业，然后将 `auto_paginate=False`，一次获取一页。

```python Python
batch_job = client.start_batch_scrape(urls)
status = client.get_batch_scrape_status(batch_job.id, pagination_config=PaginationConfig(auto_paginate=False))
print("批处理单页：", status.status, "文档数：", len(status.data), "下一页：", status.next)
```

<div id="manual-batch-scrape-with-limits-auto-pagination-early-stop">
  ##### 受限的手动批量抓取（自动分页 + 提前停止）
</div>

* 保持自动分页开启，但可通过 `max_pages`、`max_results` 或 `max_wait_time` 提前停止。

```python Python
status = client.get_batch_scrape_status(
    batch_job.id,
    pagination_config=PaginationConfig(max_pages=2, max_results=100, max_wait_time=20),
)
print("批处理受限：", status.status, "文档数：", len(status.data), "下一页：", status.next)
```

<div id="error-handling">
  ## 错误处理
</div>

SDK 会处理 Firecrawl API 返回的错误并抛出相应异常。如果在请求过程中发生错误，将抛出包含详细错误信息的异常。

<div id="async-class">
  ## 异步类
</div>

进行异步操作时，请使用 `AsyncFirecrawl` 类。其方法与 `Firecrawl` 一致，但不会阻塞主线程。

<AIOPython />