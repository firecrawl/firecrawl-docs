---
title: "选择数据提取器"
description: "比较 /agent、/extract 和 /scrape（JSON 模式），选择最适合结构化数据抽取的工具"
og:title: "选择数据提取器 | Firecrawl"
og:description: "比较 /agent、/extract 和 /scrape（JSON 模式），选择最适合结构化数据抽取的工具"
sidebarTitle: "选择数据提取器"
---

import AgentWithSchemaPython from "/snippets/zh/v2/agent/with-schema/python.mdx";
import AgentWithSchemaJS from "/snippets/zh/v2/agent/with-schema/js.mdx";
import AgentWithSchemaCURL from "/snippets/zh/v2/agent/with-schema/curl.mdx";
import ExtractPython from "/snippets/zh/v2/extract/base/python.mdx";
import ExtractNode from "/snippets/zh/v2/extract/base/js.mdx";
import ExtractCURL from "/snippets/zh/v2/extract/base/curl.mdx";
import ScrapeJsonPython from "/snippets/zh/v2/scrape/json/base/python.mdx";
import ScrapeJsonNode from "/snippets/zh/v2/scrape/json/base/js.mdx";
import ScrapeJsonCURL from "/snippets/zh/v2/scrape/json/base/curl.mdx";

Firecrawl 提供三种从网页中提取结构化数据的方法。每种方法适用于不同的使用场景，在自动化程度和可控性上各不相同。

<div id="quick-comparison">
  ## 快速对比
</div>

| 功能 | `/agent` | `/extract` | `/scrape` (JSON 模式) |
|---------|----------|------------|----------------------|
| **状态** | 可用 | 请改用 `/agent` | 可用 |
| **是否需要 URL** | 否（可选） | 是（支持通配符） | 是（单个 URL） |
| **作用范围** | 全网级发现 | 多页面/多域名 | 单个页面 |
| **URL 发现方式** | 自主网页搜索 | 从给定 URL 进行爬取 | 无 |
| **处理方式** | 异步 | 异步 | 同步 |
| **是否需要 Schema** | 否（提示词或 Schema） | 否（提示词或 Schema） | 否（提示词或 Schema） |
| **计费** | 动态计费（每天 5 次免费运行） | 基于 Token（1 点额度 = 15 个 token） | 1 点额度/页面 |
| **最适合用于** | 调研、发现、复杂信息收集 | 多页面抽取（当你已知 URL 时） | 已知单页面抽取 |

<div id="1-agent-endpoint">
  ## 1. `/agent` 端点
</div>

`/agent` 端点是 Firecrawl 最先进的功能，是 `/extract` 的后继者。它使用 AI Agent 自主在整个网络中搜索、导航并收集数据。

<div id="key-characteristics">
  ### 关键特性
</div>

* **URL 可选**：只需通过 `prompt` 描述你的需求；URL 完全可选
* **自主导航**：智能代理会搜索并深入浏览站点以找到你的数据
* **深度 Web 搜索**：自主在多个域名和页面间发现信息
* **并行处理**：可同时处理多个数据源，以更快返回结果
* **可用模型**：`spark-1-mini`（默认，成本降低 60%）和 `spark-1-pro`（更高准确率）

<div id="example">
  ### 示例
</div>

<CodeGroup>
  <AgentWithSchemaPython />

  <AgentWithSchemaJS />

  <AgentWithSchemaCURL />
</CodeGroup>

<div id="best-use-case-autonomous-research-discovery">
  ### 最佳适用场景：自主研究与发现
</div>

**场景**：你需要查找关于获得 A 轮融资的 AI 初创公司的信息，包括创始人以及融资金额。

**为什么选择 `/agent`**：你不确定哪些网站包含这些信息。`agent` 会自主在网上搜索，访问相关来源（Crunchbase、新闻网站、公司官网等），并为你整理汇总结构化数据。

更多详情请查看 [Agent 文档](/zh/features/agent)。

***

<div id="2-extract-endpoint">
  ## 2. `/extract` 端点
</div>

<Note>
  **请改用 `/agent`**：我们建议迁移到 [`/agent`](/zh/features/agent)——它更快、更可靠、不需要提供 URL，并且在覆盖所有 `/extract` 用例的基础上还能处理更多场景。
</Note>

`/extract` 端点通过 LLM 驱动的提取，从指定的 URL 或整个域名收集结构化数据。

<div id="key-characteristics">
  ### 关键特性
</div>

* **通常需要提供 URL**：至少提供一个 URL（支持使用通配符，如 `example.com/*`）
* **域名级爬取**：可以爬取并解析某个域名内发现的所有 URL
* **网页搜索增强**：可选 `enableWebSearch`，用于跟随跳转到指定域名之外的链接
* **Schema 可选**：同时支持严格的 JSON Schema 或自然语言提示词
* **异步处理**：返回任务 ID，用于查询处理状态

<div id="the-url-limitation">
  ### URL 的限制
</div>

`/extract` 的根本问题在于，你通常需要事先知道 URL：

1. **发现上的空白**：对于「查找 YC W24 公司」这样的任务，你并不知道哪些 URL 包含数据。你需要在调用 `/extract` 之前先额外做一步搜索。
2. **笨拙的网页搜索**：虽然有 `enableWebSearch`，但它只能从你提供的 URL 开始——对于以发现为主的任务来说，这是一种比较笨拙的流程。
3. **为什么会有 `/agent`**：`/extract` 擅长从已知位置提取数据，但在发现数据所在位置方面就不那么有效。

<div id="example">
  ### 示例
</div>

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

<div id="best-use-case-targeted-multi-page-extraction">
  ### 最佳适用场景：有针对性的多页面提取
</div>

**场景**：你有竞争对手文档的 URL，想从 `docs.competitor.com/*` 中提取出他们所有的 API 端点。

**为什么这里使用 `/extract` 很合适**：你已经明确知道目标域名。但即便如此，如今通常使用传入 URL 的 `/agent` 会得到更好的结果。

更多详情，请参阅 [Extract 文档](/zh/features/extract)。

***

<div id="3-scrape-endpoint-with-json-mode">
  ## 3. 使用 JSON 模式的 `/scrape` 端点
</div>

使用 JSON 模式的 `/scrape` 端点是控制力最强的方案——它通过 LLM 将页面内容解析为你指定的 schema，从单个已知 URL 中抽取结构化数据。

<div id="key-characteristics">
  ### 关键特性
</div>

* **仅支持单个 URL**：专为一次从一个特定页面提取数据而设计
* **需要精确 URL**：你必须知道包含数据的精确 URL
* **Schema 可选**：可以使用 JSON schema，或仅提供提示词（由 LLM 决定结构）
* **同步**：立即返回数据（无需轮询任务状态）
* **额外 formats**：可以在一次请求中同时获取 JSON 抽取结果以及 markdown、HTML、截图等

<div id="example">
  ### 示例
</div>

<CodeGroup>
  <ScrapeJsonPython />

  <ScrapeJsonNode />

  <ScrapeJsonCURL />
</CodeGroup>

<div id="best-use-case-single-page-precision-extraction">
  ### 最佳适用场景：单页精确提取
</div>

**场景**：你正在构建一个价格监控工具，需要从一个你已掌握 URL 的特定产品页面中提取价格、库存状态和产品详情。

**为什么在 `/scrape` 中使用 JSON 模式**：你清楚知道哪一页包含所需数据，需要对单个页面进行精确提取，并希望以同步方式获取结果，而无需处理任务管理的开销。

更多详情请参阅 [JSON 模式文档](/zh/features/llm-extract)。

***

<div id="decision-guide">
  ## 决策指南
</div>

**你是否知道包含目标数据的具体 URL？**

* **不知道** → 使用 `/agent`（自主网页发现）
* **知道**
  * **单个页面？** → 使用 `/scrape` 并启用 JSON 模式
  * **多个页面？** → 使用 `/agent` 并指定 URL（或批量调用 `/scrape`）

<div id="recommendations-by-scenario">
  ### 按场景推荐
</div>

| 场景 | 推荐的 Endpoint |
|----------|---------------------|
| 「查找所有 AI 初创公司及其融资情况」 | `/agent` |
| 「从这个特定产品页面中提取数据」 | `/scrape` (JSON 模式) |
| 「获取 competitor.com 上的所有博客文章」 | `/agent`（配合 URL） |
| 「监控多个已知 URL 的价格」 | `/scrape`（批量处理） |
| 「调研特定行业内的公司」 | `/agent` |
| 「从 50 个已知公司页面中提取联系信息」 | `/scrape`（批量处理） |

***

<div id="pricing">
  ## 价格
</div>

| Endpoint | 费用 | 说明 |
|----------|------|-------|
| `/scrape` (JSON 模式) | 1 积分/页 | 固定、可预测 |
| `/extract` | 按 token 计费（1 积分 = 15 个 token） | 费用随内容而变动 |
| `/agent` | 动态 | 每天可免费运行 5 次；费用随复杂度变化 |

<div id="example-find-the-founders-of-firecrawl">
  ### 示例：「查找 Firecrawl 的创始人」
</div>

| Endpoint | 工作方式 | 消耗积分 |
|----------|--------------|--------------|
| `/scrape` | 你手动找到 URL，然后抓取 1 个页面 | 约 1 积分 |
| `/extract` | 你提供一个或多个 URL，它会抽取结构化数据 | 不固定（按 token 计费） |
| `/agent` | 只需发送自然语言指令——agent 会自动查找并抽取数据 | 约 15 积分 |

**取舍**：`/scrape` 最便宜，但要求你已经知道 URL。`/agent` 成本更高，但会自动完成 URL 发现和抽取。

要查看详细定价，请参见 [Firecrawl Pricing](https://firecrawl.dev/pricing)。

***

<div id="migration-extract-agent">
  ## 迁移：`/extract` → `/agent`
</div>

如果你目前使用的是 `/extract`，迁移过程非常简单：

**迁移前（extract）：**

```python
result = app.extract(
    urls=["https://example.com/*"],
    prompt="提取产品信息",
    schema=schema
)
```

**使用 Agent 之后：**

```python
result = app.agent(
    urls=["https://example.com"],  # 可选 - 可完全省略
    prompt="Extract product information from example.com",
    schema=schema,
    model="spark-1-mini"  # 或使用 "spark-1-pro" 获得更高准确度
)
```

最大优势在于：使用 `/agent` 时，你可以完全省略 URL，只需描述你的需求即可。

***

<div id="key-takeaways">
  ## 关键要点
</div>

1. **已经确切知道目标 URL？** 使用 `/scrape` 搭配 JSON 模式——这是最便宜（1 积分/页面）、最快（同步）、且最可预测的选项。

2. **需要自动化调研？** 使用 `/agent`——它会自动完成发现流程，每天有 5 次免费运行，其后按复杂度动态计费。

3. **在新项目中从 `/extract` 迁移到 `/agent`**——`/agent` 是功能更强的后继方案。

4. **成本与便利性的权衡**：当你已知所有 URL 时，`/scrape` 的性价比最高；`/agent` 成本更高，但免去了手动发现 URL 的工作。

***

<div id="further-reading">
  ## 延伸阅读
</div>

* [Agent 文档](/zh/features/agent)
* [Agent 模型](/zh/features/models)
* [JSON 模式文档](/zh/features/llm-extract)
* [Extract 文档](/zh/features/extract)
* [批量抓取](/zh/features/batch-scrape)