```python Python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Extrae varios sitios web:
batch_scrape_result = app.batch_scrape_urls(
    ['https://docs.firecrawl.dev', 'https://docs.firecrawl.dev/sdks/overview'], 
    formats=['json'],
    jsonOptions={
        'prompt': 'Extrae el título y la descripción de la página.',
        'schema': {
            'type': 'object',
            'properties': {
                'title': {'type': 'string'},
                'description': {'type': 'string'}
            },
            'required': ['title', 'description']
        }
    }
)
print(batch_scrape_result)

# O bien, puedes usar el método asíncrono:
batch_scrape_job = app.async_batch_scrape_urls(
    ['https://docs.firecrawl.dev', 'https://docs.firecrawl.dev/sdks/overview'], 
    formats=['json'],
    jsonOptions={
    'prompt': 'Extrae el título y la descripción de la página.',
    'schema': {
        'type': 'object',
            'properties': {
                'title': {'type': 'string'},
                'description': {'type': 'string'}
            },
            'required': ['title', 'description']
        }
    }
)
print(batch_scrape_job)

# (async) Luego puedes usar el ID del job para consultar el estado de la extracción por lotes:
batch_scrape_status = app.check_batch_scrape_status(batch_scrape_job.id)
print(batch_scrape_status)
```
