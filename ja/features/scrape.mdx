---
title: "スクレイプ"
description: "あらゆるURLをクリーンなデータに変換する"
og:title: "スクレイプ | Firecrawl"
og:description: "あらゆるURLをクリーンなデータに変換する"
---

import InstallationPython from "/snippets/v2/installation/python.mdx";
import InstallationNode from "/snippets/v2/installation/js.mdx";
import ScrapePython from "/snippets/v2/scrape/base/python.mdx";
import ScrapeNode from "/snippets/v2/scrape/base/js.mdx";
import ScrapeCURL from "/snippets/v2/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/v2/scrape/base/output.mdx";
import ExtractCURL from "/snippets/v2/scrape/json/base/curl.mdx";
import ExtractPython from "/snippets/v2/scrape/json/base/python.mdx";
import ExtractNode from "/snippets/v2/scrape/json/base/js.mdx";
import ExtractOutput from "/snippets/v2/scrape/json/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/v2/scrape/json/no-schema/curl.mdx";
import ExtractNoSchemaPython from "/snippets/v2/scrape/json/no-schema/python.mdx";
import ExtractNoSchemaNode from "/snippets/v2/scrape/json/no-schema/js.mdx";
import ExtractNoSchemaOutput from "/snippets/v2/scrape/json/no-schema/output.mdx";
import ScrapeActionsPython from "/snippets/v2/scrape/actions/python.mdx";
import ScrapeActionsNode from "/snippets/v2/scrape/actions/js.mdx";
import ScrapeActionsCURL from "/snippets/v2/scrape/actions/curl.mdx";
import ScrapeActionsOutput from "/snippets/v2/scrape/actions/output.mdx";
import BatchScrapePython from "/snippets/v2/batch-scrape/short/python.mdx";
import BatchScrapeNode from "/snippets/v2/batch-scrape/short/js.mdx";
import BatchScrapeCURL from "/snippets/v2/batch-scrape/short/curl.mdx";
import BatchScrapeOutput from "/snippets/v2/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/v2/batch-scrape/base/async-output.mdx";
import ScrapeLocationPython from "/snippets/v2/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/v2/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/v2/scrape/location/curl.mdx";

FirecrawlはウェブページをMarkdownに変換し、LLMアプリケーションに最適です。

* 複雑な処理を吸収：プロキシ、キャッシュ、レート制限、JSでブロックされたコンテンツ
* 動的コンテンツにも対応：動的サイト、JSレンダリングサイト、PDF、画像
* クリーンなMarkdown、構造化データ、スクリーンショット、HTMLを出力

詳細は [Scrape Endpoint API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape) を参照してください。


## FirecrawlでURLをスクレイピングする

### /scrape エンドポイント

URL をスクレイピングして、その内容を取得するために使用します。

### インストール

<CodeGroup>

  <InstallationPython />

  <InstallationNode />

</CodeGroup>

### 使い方

<CodeGroup>

  <ScrapePython />

  <ScrapeNode />

  <ScrapeCURL />

</CodeGroup>

パラメータの詳細は、[APIリファレンス](https://docs.firecrawl.dev/api-reference/endpoint/scrape)を参照してください。

### レスポンス

SDK はデータオブジェクトを直接返します。cURL は以下のとおり、ペイロードをそのまま返します。

<ScrapeResponse />

## スクレイプのフォーマット

出力のフォーマットを選択できます。複数の出力フォーマットを指定可能です。サポートされているフォーマットは以下のとおりです:

- Markdown (`markdown`)
- サマリー (`summary`)
- HTML (`html`)
- 生の HTML (`rawHtml`)（変更なし）
- スクリーンショット (`screenshot`、`fullPage`、`quality`、`viewport` などのオプションあり）
- リンク (`links`)
- JSON (`json`) - 構造化出力
- 画像 (`images`) - ページ内のすべての画像 URL を抽出

出力のキーは、選択したフォーマットに対応します。

## 構造化データの抽出

### /scrape（json あり）エンドポイント

スクレイピングしたページから構造化データを抽出するために使用します。

<CodeGroup>

<ExtractPython />
<ExtractNode />
<ExtractCURL />

</CodeGroup>

出力:

<ExtractOutput />

### スキーマなしでの抽出

エンドポイントに `prompt` を渡すだけで、スキーマなしで抽出できます。LLM がデータ構造を決定します。

<CodeGroup>


<ExtractNoSchemaPython />
<ExtractNoSchemaNode />
<ExtractNoSchemaCURL />

</CodeGroup>

出力:

<ExtractNoSchemaOutput />

### JSON フォーマットのオプション

`json` フォーマットを使用する場合は、`formats` 内に以下のパラメータを含むオブジェクトを渡します:

- `schema`: 構造化出力のための JSON Schema。
- `prompt`: スキーマがある場合や、軽い指示で十分な場合に抽出を補助する任意のプロンプト。

## アクションを使ってページとやり取りする

Firecrawl を使うと、スクレイピングの前に Web ページ上でさまざまなアクションを実行できます。これは、動的コンテンツとのインタラクション、ページ遷移、ユーザー操作が必要なコンテンツへのアクセスに特に有効です。

以下は、アクションを使って google.com に移動し、Firecrawl を検索し、最初の結果をクリックしてスクリーンショットを取得する例です。

ページの読み込み時間を確保するため、他のアクションの前後には基本的に `wait` アクションを使用することが重要です。

### 例

<CodeGroup>

<ScrapeActionsPython />
<ScrapeActionsNode /> 
<ScrapeActionsCURL />

</CodeGroup>

### 出力

<CodeGroup>

<ScrapeActionsOutput />


</CodeGroup>

アクションのパラメーターの詳細は、[APIリファレンス](https://docs.firecrawl.dev/api-reference/endpoint/scrape)を参照してください。

## ロケーションと言語

ターゲットの地域と言語設定に基づいて関連性の高いコンテンツを得るため、国と言語の優先順を指定します。

### 仕組み

ロケーション設定を指定すると、Firecrawl は利用可能な場合は適切なプロキシを使用し、対応する言語とタイムゾーンをエミュレートします。指定がない場合、ロケーションのデフォルトは「US」です。

### 使い方

場所と言語の設定を使うには、リクエストボディに `location` オブジェクトを含め、次のプロパティを指定します:

- `country`: ISO 3166-1 alpha-2 の国コード（例: 'US', 'AU', 'DE', 'JP'）。既定値は 'US'。
- `languages`: 優先度順に並べた、リクエストで使用する希望言語およびロケールの配列。既定値は指定した location の言語。

<CodeGroup>
<ScrapeLocationPython />
<ScrapeLocationNode />
<ScrapeLocationCURL />
</CodeGroup>

対応している地域の詳細は、[Proxies ドキュメント](/features/proxies)を参照してください。

## キャッシュと maxAge

リクエストを高速化するため、Firecrawl は最近のコピーがある場合、デフォルトでキャッシュから結果を返します。

- **デフォルトの鮮度ウィンドウ**: `maxAge = 172800000` ms（2日）。キャッシュされたページがこの値より新しければ即時に返し、そうでなければスクレイプしてからキャッシュします。
- **パフォーマンス**: データが厳密な最新性を要さない場合、スクレイプを最大5倍高速化できます。
- **常に最新を取得**: `maxAge` を `0` に設定します。
- **保存しない**: このリクエストの結果を Firecrawl にキャッシュ/保存させたくない場合は、`storeInCache` を `false` に設定します。

例（常に最新コンテンツを取得）:

<CodeGroup>

```python Python
from firecrawl import Firecrawl
firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

doc = firecrawl.scrape(url='https://example.com', maxAge=0, formats=['markdown'])
print(doc)
```

```js Node
import Firecrawl from '@mendable/firecrawl-js';

const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

const doc = await firecrawl.scrape('https://example.com', { maxAge: 0, formats: ['markdown'] });
console.log(doc);
```

```bash cURL
curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
  -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "maxAge": 0,
    "formats": ["markdown"]
  }'
```
</CodeGroup>

例（10分のキャッシュウィンドウを使用）:

<CodeGroup>

```python Python
from firecrawl import Firecrawl
firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

doc = firecrawl.scrape(url='https://example.com', maxAge=600000, formats=['markdown', 'html'])
print(doc)
```

```js Node

const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

const doc = await firecrawl.scrape('https://example.com', { maxAge: 600000, formats: ['markdown', 'html'] });
console.log(doc);
```

```bash cURL
curl -s -X POST "https://api.firecrawl.dev/v2/scrape" \
  -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://example.com",
    "maxAge": 600000,
    "formats": ["markdown", "html"]
  }'
```
</CodeGroup>

## 複数のURLのバッチスクレイピング

複数のURLを同時にバッチスクレイピングできるようになりました。開始URLと任意のパラメータを引数として受け取ります。params引数では、出力フォーマットなど、バッチスクレイピングジョブの追加オプションを指定できます。

### 仕組み

これは `/crawl` エンドポイントの動作に非常によく似ています。バッチスクレイプのジョブを送信し、進行状況を確認するためのジョブIDを返します。

SDK は同期型と非同期型の2つのメソッドを提供します。同期型はバッチスクレイプジョブの結果を返し、非同期型はバッチスクレイプのステータス確認に使えるジョブIDを返します。

### 使い方

<CodeGroup>

<BatchScrapePython />
<BatchScrapeNode />
<BatchScrapeCURL />

</CodeGroup>

### Response

SDK の同期メソッドを使用している場合は、バッチスクレイプジョブの結果が返ります。同期メソッド以外では、バッチスクレイプのステータス確認に使用できるジョブ ID が返ります。

#### 同期処理

<BatchScrapeOutput />

#### 非同期

その後、ジョブIDを使って `/batch/scrape/{id}` エンドポイントを呼び出し、バッチスクレイプのステータスを確認できます。 このエンドポイントは、ジョブの実行中、または完了直後に使用することを想定しています。**バッチスクレイプのジョブは24時間で有効期限が切れるため**です。

<BatchScrapeAsyncOutput />

## ステルスモード

高度なボット対策を備えたウェブサイト向けに、Firecrawl は難易度の高いサイトでのスクレイピング成功率を向上させるステルスプロキシモードを提供します。

[ステルスモード](/features/stealth-mode)の詳細を見る。