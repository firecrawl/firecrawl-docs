---
title: 'クロール'
description: 'Firecrawl は URL のサブドメインを再帰的にクロールし、コンテンツを収集できます'
og:title: 'クロール | Firecrawl'
og:description: 'Firecrawl は URL のサブドメインを再帰的にクロールし、コンテンツを収集できます'
icon: 'spider'
---

import InstallationPython from '/snippets/ja/v2/installation/python.mdx';
import InstallationNode from '/snippets/ja/v2/installation/js.mdx';
import CrawlPython from '/snippets/ja/v2/crawl/base/python.mdx';
import CrawlNode from '/snippets/ja/v2/crawl/base/js.mdx';
import CrawlCURL from '/snippets/ja/v2/crawl/base/curl.mdx';
import CheckCrawlJobPython from '/snippets/ja/v2/crawl-status/short/python.mdx';
import CheckCrawlJobNode from '/snippets/ja/v2/crawl-status/short/js.mdx';
import CheckCrawlJobCURL from '/snippets/ja/v2/crawl-status/short/curl.mdx';
import CheckCrawlJobOutputScraping from '/snippets/ja/v2/crawl-status/base/output-scraping.mdx';
import CheckCrawlJobOutputCompleted from '/snippets/ja/v2/crawl-status/base/output-completed.mdx';
import CrawlWebSocketPython from '/snippets/ja/v2/crawl-websocket/base/python.mdx';
import CrawlWebSocketNode from '/snippets/ja/v2/crawl-websocket/base/js.mdx';
import CrawlWebhookCURL from '/snippets/ja/v2/crawl-webhook/base/curl.mdx';
import PythonCrawlExample from '/snippets/ja/v2/crawl/sdk-example/python.mdx';
import NodeCrawlExample from '/snippets/ja/v2/crawl/sdk-example/js.mdx';
import PythonCrawlExampleResponse from '/snippets/ja/v2/crawl/sdk-example/python-response.mdx';
import NodeCrawlExampleResponse from '/snippets/ja/v2/crawl/sdk-example/js-response.mdx';
import StartCrawlPython from '/snippets/ja/v2/start-crawl/base/python.mdx';
import StartCrawlNode from '/snippets/ja/v2/start-crawl/base/js.mdx';
import StartCrawlCURL from '/snippets/ja/v2/start-crawl/base/curl.mdx';
import StartCrawlOutput from '/snippets/ja/v2/start-crawl/base/output.mdx';

Firecrawl は複雑なウェブインフラストラクチャに対応しながら、網羅的なデータ抽出のためにウェブサイトを効率よくクロールします。プロセスは次のとおりです:

1. **URL 分析:** サイトマップをスキャンし、サイト全体をクロールしてリンクを特定
2. **トラバーサル:** すべてのサブページを見つけるためにリンクを再帰的にたどる
3. **スクレイピング:** 各ページからコンテンツを抽出し、JS やレート制限に対応
4. **出力:** データをクリーンな Markdown または構造化フォーマットに変換

これにより、任意の開始 URL からの徹底的なデータ収集を実現します。


<div id="crawling">
  ## クローリング
</div>

<div id="crawl-endpoint">
  ### /crawl エンドポイント
</div>

指定した URL と、その下にあるアクセス可能なサブページをクロールします。クロールジョブを送信し、進行状況を確認するためのジョブ ID を返します。

<Warning>
  既定では、指定した URL の子ページでないサブリンクはクロール対象外になります。たとえば、website.com/blogs/ をクロールしても website.com/other-parent/blog-1 は返されません。website.com/other-parent/blog-1 も取得したい場合は、`crawlEntireDomain` パラメータを使用してください。website.com をクロールする際に blog.website.com のようなサブドメインもクロールするには、`allowSubdomains` パラメータを使用してください。
</Warning>

<div id="installation">
  ### インストール
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />
</CodeGroup>

<div id="usage">
  ### 使い方
</div>

<CodeGroup>
  <CrawlPython />

  <CrawlNode />

  <CrawlCURL />
</CodeGroup>

<div id="scrape-options-in-crawl">
  ### クロールでのスクレイプオプション
</div>

Scrape エンドポイントのすべてのオプションは、`scrapeOptions`（JS）/ `scrape_options`（Python）経由で Crawl でも利用できます。これらはクローラーがスクレイプするすべてのページに適用されます（フォーマット、プロキシ、キャッシュ、アクション、ロケーション、タグなど）。詳細は [Scrape API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape) を参照してください。

<CodeGroup>
  ```js Node
  import Firecrawl from '@mendable/firecrawl-js';

  const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR_API_KEY' });

  // スクレイプオプション付きでクロール
  const crawlResponse = await firecrawl.crawl('https://example.com', {
    limit: 100,
    scrapeOptions: {
      formats: [
        'markdown',
        {
          type: 'json',
          schema: { type: 'object', properties: { title: { type: 'string' } } },
        },
      ],
      proxy: 'auto',
      maxAge: 600000,
      onlyMainContent: true,
    },
  });
  ```

  ```python Python
  from firecrawl import Firecrawl

  firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

  # スクレイプオプション付きでクロール
  response = firecrawl.crawl('https://example.com',
      limit=100,
      scrape_options={
          'formats': [
              'markdown',
              { 'type': 'json', 'schema': { 'type': 'object', 'properties': { 'title': { 'type': 'string' } } } }
          ],
          'proxy': 'auto',
          'maxAge': 600000,
          'onlyMainContent': True
      }
  )
  ```
</CodeGroup>

<div id="api-response">
  ### API レスポンス
</div>

cURL または starter メソッドを使用している場合、クロールのステータスを確認するための `ID` が返されます。

<Note>
  SDK を使用している場合は、以下のメソッドで waiter と starter の動作の違いをご確認ください。
</Note>

<StartCrawlOutput />

<div id="check-crawl-job">
  ### クロールジョブの確認
</div>

クロールジョブのステータスを確認し、結果を取得します。

<Note>
  このエンドポイントは、進行中のクロール、または直近に完了したクロールに対してのみ機能します。{' '}
</Note>

<CodeGroup>
  <CheckCrawlJobPython />

  <CheckCrawlJobNode />

  <CheckCrawlJobCURL />
</CodeGroup>

<div id="response-handling">
  #### レスポンスの処理
</div>

レスポンスはクロールのステータスによって異なります。

未完了のレスポンス、またはサイズが10MBを超える大きなレスポンスの場合は、`next` URLパラメータが付与されます。次の10MBのデータを取得するには、このURLにリクエストしてください。`next` パラメータがない場合は、クロールデータの終端を示します。

skip パラメータは、返却される各チャンクに含まれる結果の最大件数を設定します。

<Info>
  skip と next のパラメータが関係するのは、API を直接呼び出す場合のみです。
  SDK を使用している場合は、こちらで処理し、すべての
  結果を一度に返します。
</Info>

<CodeGroup>
  <CheckCrawlJobOutputScraping />

  <CheckCrawlJobOutputCompleted />
</CodeGroup>

<div id="sdk-methods">
  ### SDK メソッド
</div>

SDK の利用方法は 2 通りあります:

1. **クロールして待つ**（`crawl`）:
   * クロールの完了を待機し、完全なレスポンスを返します
   * ページネーションを自動処理します
   * ほとんどのユースケースで推奨

<CodeGroup>
  <PythonCrawlExample />

  <NodeCrawlExample />
</CodeGroup>

レスポンスには、クロールのステータスと収集された全データが含まれます:

<CodeGroup>
  <PythonCrawlExampleResponse />

  <NodeCrawlExampleResponse />
</CodeGroup>

2. **開始してステータス確認**（`startCrawl`/`start_crawl`）:
   * 即時にクロール ID を返します
   * ステータスを手動で確認可能
   * 長時間のクロールや独自のポーリングロジックに有用

<CodeGroup>
  <StartCrawlPython />

  <StartCrawlNode />

  <StartCrawlCURL />
</CodeGroup>

<div id="crawl-websocket">
  ## Crawl WebSocket
</div>

Firecrawl の WebSocket ベースのメソッド「Crawl URL and Watch」は、リアルタイムでのデータ抽出と監視を実現します。URL を指定してクロールを開始し、ページ上限、許可ドメイン、出力フォーマットなどのオプションでカスタマイズできます。即時のデータ処理に最適です。

<CodeGroup>
  <CrawlWebSocketPython />

  <CrawlWebSocketNode />
</CodeGroup>

<div id="crawl-webhook">
  ## クロール Webhook
</div>

クロールの進行に合わせてリアルタイム通知を受け取れるよう、webhook を設定できます。これにより、クロール全体の完了を待たずに、スクレイプされたページを随時処理できます。

<CrawlWebhookCURL />

イベントタイプ、ペイロード構造、実装例などを含む包括的な webhook のドキュメントは、[Webhooks ドキュメント](/ja/webhooks/overview)を参照してください。

<div id="quick-reference">
  ### クイックリファレンス
</div>

**イベントタイプ:**

* `crawl.started` - クロールが開始されたとき
* `crawl.page` - 各ページのスクレイピングに成功したとき
* `crawl.completed` - クロールが終了したとき
* `crawl.failed` - クロール中にエラーが発生した場合

**基本ペイロード:**

```json
{
  "success": true,
  "type": "crawl.page",
  "id": "crawl-job-id",
  "data": [...], // 'page'イベントのページデータ
  "metadata": {}, // カスタムメタデータ
  "error": null
}
```

<Note>
  Webhook の詳細な設定、セキュリティのベストプラクティス、トラブルシューティングについては、[Webhooks ドキュメント](/ja/webhooks/overview)をご覧ください。
</Note>
