---
title: 'バッチスクレイピング'
description: '複数のURLをバッチでスクレイピングする'
og:title: 'バッチスクレイピング | Firecrawl'
og:description: '複数のURLをバッチでスクレイピングする'
---

import BatchScrapePython from '/snippets/ja/v2/batch-scrape/base/python.mdx';
import BatchScrapeNode from '/snippets/ja/v2/batch-scrape/base/js.mdx';
import BatchScrapeCURL from '/snippets/ja/v2/batch-scrape/base/curl.mdx';
import BatchScrapeOutput from '/snippets/ja/v2/batch-scrape/base/output.mdx';
import BatchScrapeAsyncOutput from '/snippets/ja/v2/batch-scrape/base/async-output.mdx';
import BatchScrapeExtractPython from '/snippets/ja/v2/batch-scrape/json/python.mdx';
import BatchScrapeExtractNode from '/snippets/ja/v2/batch-scrape/json/js.mdx';
import BatchScrapeExtractCURL from '/snippets/ja/v2/batch-scrape/json/curl.mdx';
import BatchScrapeExtractOutput from '/snippets/ja/v2/batch-scrape/json/output.mdx';
import BatchScrapeExtractAsyncOutput from '/snippets/ja/v2/batch-scrape/json/async-output.mdx';
import BatchScrapeWebhookCURL from '/snippets/ja/v1/batch-scrape-webhook/base/curl.mdx';

<div id="batch-scraping-multiple-urls">
  ## 複数のURLを一括スクレイピング
</div>

複数のURLを同時に一括スクレイピングできます。開始URLと任意のパラメータを引数に取ります。params 引数では、出力フォーマットなど、一括スクレイピングジョブの追加オプションを指定できます。

<div id="how-it-works">
  ### 仕組み
</div>

`/crawl` エンドポイントの動作とほぼ同じです。バッチを開始して完了まで待つことも、開始して完了処理を自分で行うこともできます。

* `batchScrape`（JS）/ `batch_scrape`（Python）：バッチジョブを開始し、完了まで待って結果を返します。
* `startBatchScrape`（JS）/ `start_batch_scrape`（Python）：バッチジョブを開始し、ポーリングやウェブフックに使えるジョブIDを返します。

<div id="usage">
  ### 使い方
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### レスポンス
</div>

`batchScrape`/`batch_scrape` を呼び出すと、バッチ完了時に完全な結果が返されます。

<BatchScrapeOutput />

`startBatchScrape`/`start_batch_scrape` を呼び出すと、`getBatchScrapeStatus`/`get_batch_scrape_status`、API エンドポイント `/batch/scrape/{id}`、または Webhook を使って追跡できるジョブ ID が返されます。このエンドポイントは進行中の確認や完了直後の確認を想定しています。**バッチジョブは 24 時間で有効期限が切れるため**。

<BatchScrapeAsyncOutput />

<div id="batch-scrape-with-structured-extraction">
  ## 構造化抽出を伴うバッチスクレイプ
</div>

バッチスクレイプのエンドポイントを使って、ページから構造化データを抽出することもできます。これは、複数のURLから同一の構造化データを取得したい場合に便利です。

<CodeGroup>
  <BatchScrapeExtractPython />

  <BatchScrapeExtractNode />

  <BatchScrapeExtractCURL />
</CodeGroup>

<div id="response">
  ### レスポンス
</div>

`batchScrape`/`batch_scrape` は完全な結果を返します：

<BatchScrapeExtractOutput />

`startBatchScrape`/`start_batch_scrape` はジョブ ID を返します：

<BatchScrapeExtractAsyncOutput />

<div id="batch-scrape-with-webhooks">
  ## Webhooks を使ったバッチスクレイプ
</div>

バッチ内の各 URL がスクレイプされるたびにリアルタイムで通知を受け取れるよう、webhook を設定できます。これにより、バッチ全体の完了を待たずに結果を即時に処理できます。

<BatchScrapeWebhookCURL />

イベントタイプ、ペイロード構造、実装例などを含む webhook の詳細なドキュメントは、[Webhooks ドキュメント](/ja/webhooks/overview)を参照してください。

<div id="quick-reference">
  ### クイックリファレンス
</div>

**イベントタイプ:**

* `batch_scrape.started` - バッチスクレイプが開始されたとき
* `batch_scrape.page` - 各URLのスクレイプに成功したとき
* `batch_scrape.completed` - すべてのURLの処理が完了したとき
* `batch_scrape.failed` - バッチスクレイプでエラーが発生した場合

**基本ペイロード:**

```json
{
  "success": true,
  "type": "batch_scrape.page",
  "id": "batch-job-id",
  "data": [...], // 'page'イベントのページデータ
  "metadata": {}, // カスタムメタデータ
  "error": null
}
```

<Note>
  Webhook の詳細な構成方法、セキュリティのベストプラクティス、トラブルシューティングについては、[Webhooks のドキュメント](/ja/webhooks/overview)をご覧ください。
</Note>
