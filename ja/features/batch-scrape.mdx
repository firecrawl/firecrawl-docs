---
title: 'バッチスクレイピング'
description: '複数のURLをバッチでスクレイピング'
og:title: 'バッチスクレイピング | Firecrawl'
og:description: '複数のURLをバッチでスクレイピング'
---

import BatchScrapePython from '/snippets/ja/v2/batch-scrape/base/python.mdx';
import BatchScrapeNode from '/snippets/ja/v2/batch-scrape/base/js.mdx';
import BatchScrapeCURL from '/snippets/ja/v2/batch-scrape/base/curl.mdx';
import BatchScrapeOutput from '/snippets/ja/v2/batch-scrape/base/output.mdx';
import BatchScrapeAsyncOutput from '/snippets/ja/v2/batch-scrape/base/async-output.mdx';
import BatchScrapeExtractPython from '/snippets/ja/v2/batch-scrape/json/python.mdx';
import BatchScrapeExtractNode from '/snippets/ja/v2/batch-scrape/json/js.mdx';
import BatchScrapeExtractCURL from '/snippets/ja/v2/batch-scrape/json/curl.mdx';
import BatchScrapeExtractOutput from '/snippets/ja/v2/batch-scrape/json/output.mdx';
import BatchScrapeExtractAsyncOutput from '/snippets/ja/v2/batch-scrape/json/async-output.mdx';
import BatchScrapeWebhookCURL from '/snippets/ja/v1/batch-scrape-webhook/base/curl.mdx';

<div id='batch-scraping-multiple-urls'>## 複数のURLを一括スクレイピング</div>

複数の URL を同時に一括スクレイピングできます。開始 URL と任意のパラメータを引数に取ります。params 引数では、出力フォーマットなど、一括スクレイピングジョブの追加オプションを指定できます。

<div id='how-it-works'>### 仕組み</div>

`/crawl` エンドポイントの動作とほぼ同じです。バッチを開始して完了まで待つことも、開始して完了処理を自分で行うこともできます。

- `batchScrape`（JS）/ `batch_scrape`（Python）：バッチジョブを開始し、完了まで待って結果を返します。
- `startBatchScrape`（JS）/ `start_batch_scrape`（Python）：バッチジョブを開始し、ポーリングやウェブフックに使えるジョブ ID を返します。

<div id='usage'>### 使い方</div>

<CodeGroup>
  <BatchScrapePython />

{' '}
<BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id='response'>### レスポンス</div>

- `batchScrape`/`batch_scrape` を呼び出すと、バッチ完了時に全結果が返されます。

<BatchScrapeOutput />

- `startBatchScrape`/`start_batch_scrape` を呼び出すと、`getBatchScrapeStatus`/`get_batch_scrape_status`、API エンドポイント `/batch/scrape/{id}`、または Webhook で追跡できるジョブ ID が返されます。このエンドポイントは、進行中の確認や完了直後の確認を目的としています。**バッチ ジョブは 24 時間で期限切れになるため**です。

<BatchScrapeAsyncOutput />

<div id='batch-scrape-with-structured-extraction'>
  ## 構造化抽出を伴うバッチスクレイプ
</div>

バッチスクレイプのエンドポイントを使って、ページから構造化データを抽出することもできます。これは、複数の URL から同一の構造化データを取得したい場合に便利です。

<CodeGroup>
  <BatchScrapeExtractPython />

{' '}
<BatchScrapeExtractNode />

  <BatchScrapeExtractCURL />
</CodeGroup>

<div id='response'>### レスポンス</div>

- `batchScrape`/`batch_scrape` は完全な結果を返します:

<BatchScrapeExtractOutput />

- `startBatchScrape`/`start_batch_scrape` はジョブ ID を返します:

<BatchScrapeExtractAsyncOutput />

<div id='batch-scrape-with-webhooks'>## Webhook を使ったバッチスクレイプ</div>

バッチ内の各 URL がスクレイプされるたびにリアルタイム通知を受け取れるよう、Webhook を設定できます。これにより、バッチ全体の完了を待たずに結果を即時に処理できます。

<BatchScrapeWebhookCURL />

イベントタイプ、ペイロード構造、実装例を含む Webhook の詳細なドキュメントは、[Webhooks ドキュメント](/ja/webhooks/overview) を参照してください。

<div id='quick-reference'>### クイックリファレンス</div>

**イベントタイプ:**

- `batch_scrape.started` - バッチスクレイプが開始されたとき
- `batch_scrape.page` - 各 URL のスクレイプに成功したとき
- `batch_scrape.completed` - すべての URL の処理が完了したとき
- `batch_scrape.failed` - バッチスクレイプでエラーが発生したとき

**基本ペイロード:**

```json
{
  "success": true,
  "type": "batch_scrape.page",
  "id": "batch-job-id",
  "data": [...], // 「page」イベントのページデータ
  "metadata": {}, // 任意のメタデータ
  "error": null
}
```

<Note>
  Webhook
  の詳細な設定方法、セキュリティのベストプラクティス、トラブルシューティングについては、[Webhooks
  ドキュメント](/ja/webhooks/overview)をご覧ください。
</Note>
