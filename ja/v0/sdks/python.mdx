---
title: 'Python'
description: 'Firecrawl Python SDK は、Firecrawl API のラッパーで、ウェブサイトを簡単に Markdown に変換できます。'
icon: 'python'
og:title: "Python SDK | Firecrawl"
og:description: "Firecrawl Python SDK は、Firecrawl API のラッパーで、ウェブサイトを簡単に Markdown に変換できます。"
---

> 注意: これは、廃止予定の [Firecrawl API v0](/ja/v0/introduction) を使用しています。[v1](/ja/sdks/python) への移行を推奨します。

<div id="installation">
  ## インストール
</div>

Firecrawl の Python SDK をインストールするには、pip を使用します：

```bash
pip install firecrawl-py==0.0.16
```

<div id="usage">
  ## 使い方
</div>

1. [firecrawl.dev](https://firecrawl.dev) から API キーを取得します。
2. API キーを環境変数 `FIRECRAWL_API_KEY` に設定するか、`FirecrawlApp` クラスへパラメータとして渡します。

以下は SDK の使用例です:

```python
from firecrawl import FirecrawlApp

# APIキーを使って FirecrawlApp を初期化
app = FirecrawlApp(api_key='your_api_key')

# 単一のURLをスクレイピングする
url = 'https://docs.firecrawl.dev'
scraped_data = app.scrape_url(url)

# サイト全体をクロールする
crawl_url = 'https://docs.firecrawl.dev'
params = {
    'pageOptions': {
        'onlyMainContent': True
    }
}
crawl_result = app.crawl_url(crawl_url, params=params)
```

<div id="scraping-a-url">
  ### URLのスクレイピング
</div>

単一のURLをスクレイプするには、`scrape_url` メソッドを使用します。URLを引数に取り、スクレイプしたデータをディクショナリ（辞書型）として返します。

```python
url = 'https://example.com'
scraped_data = app.scrape_url(url)
```

<div id="extracting-structured-data-from-a-url">
  ### URLから構造化データを抽出する
</div>

LLM抽出を使えば、任意のURLから構造化データを簡単に取り出せます。Pydanticのスキーマもサポートしており、さらに手軽に扱えます。使い方は次のとおりです：

```python
class ArticleSchema(BaseModel):
    title: str
    points: int 
    by: str
    commentsURL: str

class TopArticlesSchema(BaseModel):
    top: List[ArticleSchema] = Field(..., max_items=5, description="上位5件のストーリー")

data = app.scrape_url('https://news.ycombinator.com', {
    'extractorOptions': {
        'extractionSchema': TopArticlesSchema.model_json_schema(),
        'mode': 'llm-extraction'
    },
    'pageOptions':{
        'onlyMainContent': True
    }
})
print(data["llm_extraction"])
```

<div id="crawling-a-website">
  ### ウェブサイトのクロール
</div>

ウェブサイトをクロールするには、`crawl_url` メソッドを使用します。開始URLと任意パラメータを引数に取ります。`params` 引数では、クロールする最大ページ数、許可ドメイン、出力フォーマットなど、クロールジョブの追加オプションを指定できます。

`wait_until_done` パラメータは、結果を返す前にメソッドがクロールジョブの完了を待つかどうかを制御します。`True` に設定すると、ジョブが完了するまで、または指定した `timeout`（秒）に達するまで、メソッドは定期的にジョブのステータスを確認します。`False` に設定すると、メソッドはジョブIDをすぐに返し、`check_crawl_status` メソッドで手動でジョブのステータスを確認できます。

```python
crawl_url = 'https://example.com'
params = {
    'crawlerOptions': {
        'excludes': ['blog/*'],
        'includes': [], # すべてのページを対象にする場合は空のままにする
        'limit': 1000,
    },
    'pageOptions': {
        'onlyMainContent': True
    }
}
crawl_result = app.crawl_url(crawl_url, params=params, wait_until_done=True, timeout=5)
```

`wait_until_done` が `True` の場合、`crawl_url` メソッドはジョブの完了後にクロール結果を返します。ジョブが失敗するか停止した場合は、例外がスローされます。

<div id="checking-crawl-status">
  ### クロールステータスの確認
</div>

クロールジョブのステータスを確認するには、`check_crawl_status` メソッドを使用します。ジョブ ID を引数に受け取り、クロールジョブの現在のステータスを返します。

```python
job_id = crawl_result['jobId']
status = app.check_crawl_status(job_id)
```

<div id="search-for-a-query">
  ### クエリを検索する
</div>

ウェブを検索し、最も関連性の高い結果を取得し、各ページをスクレイプしてMarkdownで返します。

```python
query = 'Mendableとは？'
search_result = app.search(query)
```

<div id="error-handling">
  ## エラー処理
</div>

SDK は Firecrawl API から返されるエラーを処理し、適切な例外をスローします。リクエスト中にエラーが発生した場合は、わかりやすいエラーメッセージとともに例外がスローされます。