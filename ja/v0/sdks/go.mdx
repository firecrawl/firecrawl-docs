---
title: 'Go'
description: 'Firecrawl Go SDK は、Firecrawl API のラッパーで、ウェブサイトを簡単に Markdown に変換できます。'
icon: 'golang'
og:title: "Go SDK | Firecrawl"
og:description: "Firecrawl Go SDK は、Firecrawl API のラッパーで、ウェブサイトを簡単に Markdown に変換できます。"
---

> 注: これは、廃止予定の [Firecrawl API v0](/ja/v0/introduction) を使用しています。[v1](/ja/sdks/go) への移行をおすすめします。

<div id="installation">
  ## インストール
</div>

Firecrawl の Go SDK をインストールするには、go get を使用します。

```bash
go get github.com/mendableai/firecrawl-go
```

<div id="usage">
  ## 使い方
</div>

1. [firecrawl.dev](https://firecrawl.dev) から API キーを取得します。
2. 環境変数 `FIRECRAWL_API_KEY` に API キーを設定するか、`FirecrawlApp` 構造体のパラメータとして渡します。

以下は、エラーハンドリング付きで SDK を使用する例です。

```go
import (
  "fmt"
  "log"

  "github.com/mendableai/firecrawl-go"
)

func main() {
  // APIキーで FirecrawlApp を初期化する
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
    log.Fatalf("FirecrawlApp の初期化に失敗しました: %v", err)
  }

  // 単一のURLをスクレイピングする
  scrapedData, err := app.ScrapeURL("docs.firecrawl.dev", nil)
  if err != nil {
    log.Fatalf("スクレイピング中にエラーが発生しました: %v", err)
  }
  fmt.Println(scrapedData)

  // ウェブサイトをクロールする
  params := map[string]any{
    "pageOptions": map[string]any{
      "onlyMainContent": true,
    },
  }

  crawlResult, err := app.CrawlURL("docs.firecrawl.dev", params)
  if err != nil {
    log.Fatalf("クロール中にエラーが発生しました: %v", err)
  }
  fmt.Println(crawlResult)
}
```

<div id="scraping-a-url">
  ### URLのスクレイピング
</div>

エラー処理付きで単一のURLをスクレイプするには、`ScrapeURL` メソッドを使用します。URLを引数に取り、スクレイプしたデータを辞書（dictionary）として返します。

```go
scrapedData, err := app.ScrapeURL("docs.firecrawl.dev", nil)
if err != nil {
  log.Fatalf("URL のスクレイピングに失敗しました: %v", err)
}
fmt.Println(scrapedData)
```

<div id="crawling-a-website">
  ### ウェブサイトのクローリング
</div>

ウェブサイトをクローリングするには、`CrawlUrl` メソッドを使用します。開始URLと任意のパラメータを引数に取ります。`params` 引数では、クローリングするページ数の上限、許可ドメイン、出力フォーマットなど、ジョブの追加オプションを指定できます。

```go
crawlParams := map[string]any{
  "crawlerOptions": map[string]any{
    "excludes": []string{"blog/*"},
    "includes": []string{}, // すべてのページを対象にする場合は空のままにする
    "limit": 1000,
  },
  "pageOptions": map[string]any{
    "onlyMainContent": true,
  },
}
crawlResult, err := app.CrawlURL("docs.firecrawl.dev", crawlParams, true, 2, idempotencyKey)
if err != nil {
  log.Fatalf("URLのクロールに失敗しました: %v", err)
}
fmt.Println(crawlResult)
```

<div id="checking-crawl-status">
  ### クロールステータスの確認
</div>

クロールジョブのステータスを確認するには、`CheckCrawlStatus` メソッドを使用します。ジョブIDを引数に取り、クロールジョブの現在のステータスを返します。

```go
status, err := app.CheckCrawlStatus(jobId)
if err != nil {
  log.Fatalf("クロールのステータスの確認に失敗しました: %v", err)
}
fmt.Println(status)
```

<div id="canceling-a-crawl-job">
  ### クロールジョブのキャンセル
</div>

クロールジョブをキャンセルするには、`CancelCrawlJob` メソッドを使用します。ジョブIDを引数に取り、該当ジョブのキャンセルステータスを返します。

```go
canceled, err := app.CancelCrawlJob(jobId)
if err != nil {
  log.Fatalf("クロールジョブのキャンセルに失敗しました: %v", err)
}
fmt.Println(canceled)
```

<div id="extracting-structured-data-from-a-url">
  ### URL から構造化データを抽出する
</div>

LLM 抽出を使えば、任意の URL から構造化データを簡単に取り出せます。使い方は次のとおりです:

```go
jsonSchema := map[string]any{
  "type": "object",
  "properties": map[string]any{
    "top": map[string]any{
      "type": "array",
      "items": map[string]any{
        "type": "object",
        "properties": map[string]any{
          "title":       map[string]string{"type": "string"},
          "points":      map[string]string{"type": "number"},
          "by":          map[string]string{"type": "string"},
          "commentsURL": map[string]string{"type": "string"},
        },
        "required": []string{"title", "points", "by", "commentsURL"},
      },
      "minItems":    5,
      "maxItems":    5,
      "description": "Hacker News のトップ5件のストーリー",
    },
  },
  "required": []string{"top"},
}

llmExtractionParams := map[string]any{
  "extractorOptions": firecrawl.ExtractorOptions{
    ExtractionSchema: jsonSchema,
  },
}

scrapeResult, err := app.ScrapeURL("https://news.ycombinator.com", llmExtractionParams)
if err != nil {
  log.Fatalf("LLMによる抽出に失敗しました: %v", err)
}
fmt.Println(scrapeResult)
```

<div id="search-for-a-query">
  ### クエリを検索する
</div>

Web を検索し、最も関連性の高い結果を取得し、各ページをスクレイピングして Markdown を返すには、`Search` メソッドを使用します。このメソッドはクエリをパラメータとして受け取り、検索結果を返します。

```go
query := "Firecrawl とは何ですか？"
searchResult, err := app.Search(query)
if err != nil {
  log.Fatalf("検索に失敗しました: %v", err)
}
fmt.Println(searchResult)
```

<div id="error-handling">
  ## エラー処理
</div>

SDK は Firecrawl API から返されたエラーを処理し、適切な例外をスローします。リクエスト中にエラーが発生した場合は、内容を説明するエラーメッセージ付きの例外がスローされます。