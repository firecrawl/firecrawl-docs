---
title: クイックスタート
description: "Firecrawl は、ウェブサイト全体を LLM 向けの Markdown データに変換できます"
og:title: "クイックスタート | Firecrawl"
og:description: "Firecrawl は、ウェブサイト全体を LLM 向けの Markdown データに変換できます"
---

<img className="block" src="/images/turn-websites-into-llm-ready-data--firecrawl.jpg" alt="Hero Light" />

<div id="welcome-to-firecrawl">
  ## Firecrawl へようこそ
</div>

[Firecrawl](https://firecrawl.dev?ref=github) は、URL を渡すだけでクロールし、クリーンな Markdown に変換する API サービスです。アクセス可能なサブページをすべてクロールし、各ページのクリーンな Markdown を提供します。サイトマップは不要です。

<div id="how-to-use-it">
  ## 使い方
</div>

ホスティング版で使いやすいAPIを提供しています。プレイグラウンドとドキュメントは[こちら](https://firecrawl.dev/playground)にあります。バックエンドをセルフホストすることも可能です。

まずは以下のリソースをご確認ください:

* [x] **API**: [ドキュメント](https://docs.firecrawl.dev/api-reference/introduction)
* [x] **SDKs**: [Python](https://docs.firecrawl.dev/sdks/python), [Node](https://docs.firecrawl.dev/sdks/node), [Go](https://docs.firecrawl.dev/sdks/go), [Rust](https://docs.firecrawl.dev/sdks/rust)
* [x] **LLMフレームワーク**: [LangChain (Python)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/), [LangChain (JS)](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl), [LlamaIndex](https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo/#using-firecrawl-reader), [Crew.ai](https://docs.crewai.com/), [Composio](https://composio.dev/tools/firecrawl/all), [PraisonAI](https://docs.praison.ai/firecrawl/), [Superinterface](https://superinterface.ai/docs/assistants/functions/firecrawl), [Vectorize](https://docs.vectorize.io/integrations/source-connectors/firecrawl)
* [x] **ローコードフレームワーク**: [Dify](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl), [Langflow](https://docs.langflow.org/), [Flowise AI](https://docs.flowiseai.com/integrations/langchain/document-loaders/firecrawl), [Cargo](https://docs.getcargo.io/integration/firecrawl), [Pipedream](https://pipedream.com/apps/firecrawl/)
* [x] **その他**: [Zapier](https://zapier.com/apps/firecrawl/integrations), [Pabbly Connect](https://www.pabbly.com/connect/integrations/firecrawl/)
* [ ] SDKやインテグレーションが必要ですか？Issueを作成してお知らせください。

**セルフホスト:** セルフホストする場合は[こちら](/ja/contributing/self-host)のガイドをご参照ください。

<div id="api-key">
  ### APIキー
</div>

APIを利用するには、[Firecrawl](https://firecrawl.dev) に登録し、APIキーを取得してください。

<div id="crawling">
  ## クローリング
</div>

URL とアクセス可能なすべてのサブページをクロールします。クロールジョブを送信し、進行状況の確認に使えるジョブ ID を返します。

<div id="installation">
  ### インストール
</div>

<CodeGroup>
  ```bash Python
  pip install firecrawl-py
  ```

  ```bash JavaScript
  npm install @mendable/firecrawl-js
  ```

  ```bash Go
  go get github.com/firecrawl/firecrawl-go
  ```

  ```toml Rust
  # 次を Cargo.toml に追加します

  [dependencies]
  firecrawl = "^0.1"
  tokio = { version = "^1", features = ["full"] }
  serde = { version = "^1.0", features = ["derive"] }
  serde_json = "^1.0"
  uuid = { version = "^1.10", features = ["v4"] }

  [build-dependencies]
  tokio = { version = "1", features = ["full"] }
  ```
</CodeGroup>

<div id="usage">
  ### 使い方
</div>

<CodeGroup>
  ```python Python
  from firecrawl import FirecrawlApp

  app = FirecrawlApp(api_key="YOUR_API_KEY")

  crawl_result = app.crawl_url('docs.firecrawl.dev', {'crawlerOptions': {'excludes': ['blog/*']}})

  # Markdown を取得
  for result in crawl_result:
      print(result['markdown'])
  ```

  ```js JavaScript
  import FirecrawlApp from "@mendable/firecrawl-js";

  // API キーで FirecrawlApp を初期化
  const app = new FirecrawlApp({ apiKey: "YOUR_API_KEY" });

  // Web サイトをクロール
  const crawlResult = await app.crawlUrl("docs.firecrawl.dev", {
    crawlerOptions: { excludes: ["blog/*"] },
  });

  // Markdown を出力
  console.log(crawlResult.map((result) => result.markdown));
  ```

  ```go Go
  import (
    "fmt"
    "log"

    "github.com/firecrawl/firecrawl-go"
  )

  func main() {
    // API キーで FirecrawlApp を初期化
    app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
    if err != nil {
      log.Fatalf("Failed to initialize FirecrawlApp: %v", err)
    }

    // Web サイトをクロール
    params := map[string]any{
      "crawlerOptions": map[string]any{
        "excludes": []string{"blog/*"},
      },
    }
    crawlResult, err := app.CrawlURL("docs.firecrawl.dev", params)
    if err != nil {
      log.Fatalf("Error occurred while crawling: %v", err)
    }

    // Markdown を取得
    for _, result := range crawlResult {
      fmt.Println(result.Markdown)
    }
  }
  ```

  ```rust Rust
  use firecrawl::FirecrawlApp;

  #[tokio::main]
  async fn main() {
    // API キーで FirecrawlApp を初期化
    let api_key = "YOUR_API_KEY";
    let api_url = "https://api.firecrawl.dev";
    let app = FirecrawlApp::new(api_key, api_url).expect("Failed to initialize FirecrawlApp");

    // URL をクロール
    let crawl_params = json!({
      "crawlerOptions": {
          "excludes": ["blog/*"]
      }
    });

    let crawl_result = app
        .crawl_url("https://example.com", Some(crawl_params), true, 2, None)
        .await;

    // クロール結果を出力
    match crawl_result {
        Ok(data) => println!("Crawl Result:\n{}", data),
        Err(e) => eprintln!("Crawl failed: {}", e),
    }
  }
  ```

  ```bash cURL
  curl -X POST https://api.firecrawl.dev/v0/crawl \
      -H 'Content-Type: application/json' \
      -H 'Authorization: Bearer YOUR_API_KEY' \
      -d '{
        "url": "https://docs.firecrawl.dev"
      }'
  ```
</CodeGroup>

SDK を使わない場合や、Webhook もしくは別のポーリング方式を利用したい場合は、`wait_until_done` を `false` に設定できます。
この場合は jobId が返されます。

cURL では、/crawl は常に jobId を返し、それを使ってクロール状況を確認できます。

```json
{ "jobId": "1234-5678-9101" }
```

<div id="check-crawl-job">
  ### クローラー ジョブの確認
</div>

クローラー ジョブのステータスを確認し、結果を取得します。

<CodeGroup>
  ```python Python
  status = app.check_crawl_status(job_id)
  ```

  ```js JavaScript
  const status = await app.checkCrawlStatus(jobId);
  ```

  ```go Go
  status, err := app.CheckCrawlStatus(jobId)
  if err != nil {
    log.Fatalf("Failed to check crawl status: %v", err)
  }
  ```

  ```rust Rust
  let status = match app.check_crawl_status(jobId).await {
      Ok(status) => status,
      Err(e) => panic!("Failed to check crawl status: {:?}", e),
  };

  println!("Crawl Status: {:?}", status);
  ```

  ```bash cURL
  curl -X GET https://api.firecrawl.dev/v0/crawl/status/1234-5678-9101 \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY'
  ```
</CodeGroup>

<div id="response">
  #### レスポンス
</div>

```json
{
  "status": "完了",
  "current": 22,
  "total": 22,
  "data": [
    {
      "content": "生データ",
      "markdown": "# Markdownコンテンツ",
      "provider": "web-scraper",
      "metadata": {
        "title": "Firecrawl | LLM向けに信頼性高くウェブをスクレイプ",
        "description": "CXと営業向けのAI"
        "language": null,
        "sourceURL": "https://docs.firecrawl.dev/"
      }
    }
  ]
}
```

<div id="scraping">
  ## スクレイピング
</div>

単一のURLをスクレイプするには、`scrape_url` メソッドを使用します。URLを引数に取り、スクレイプ結果を辞書（ディクショナリ）型で返します。

<CodeGroup>
  ```python Python
  from firecrawl import FirecrawlApp

  app = FirecrawlApp(api_key="YOUR_API_KEY")

  content = app.scrape_url("https://docs.firecrawl.dev")
  ```

  ```JavaScript JavaScript
  import { FirecrawlApp } from 'firecrawl-js';

  const app = new FirecrawlApp({ apiKey: 'YOUR_API_KEY' });

  const content = await app.scrapeUrl('https://docs.firecrawl.dev');
  ```

  ```go Go
  import (
    "log"

    "github.com/firecrawl/firecrawl-go"
  )

  func main() {
    app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
    if err != nil {
      log.Fatalf("Failed to initialize FirecrawlApp: %v", err)
    }

    content, err := app.ScrapeURL("docs.firecrawl.dev", nil)
    if err != nil {
      log.Fatalf("Failed to scrape URL: %v", err)
    }
  }
  ```

  ```rust Rust
  use firecrawl::FirecrawlApp;

  #[tokio::main]
  async fn main() {
      // Initialize the FirecrawlApp with the API key
      let api_key = "YOUR_API_KEY";
      let api_url = "https://api.firecrawl.dev";
      let app = FirecrawlApp::new(api_key, api_url).expect("Failed to initialize FirecrawlApp");

      // Scrape the URL
      let scrape_result = app.scrape_url("https://example.com", None).await;

      // Print the scrape result
    match scrape_result {
      Ok(data) => println!("Scrape Result:\n{}", data["markdown"]),
      Err(e) => eprintln!("Scrape failed: {}", e),
    }
  }
  ```

  ```bash cURL
  curl -X POST https://api.firecrawl.dev/v0/scrape \
      -H 'Content-Type: application/json' \
      -H 'Authorization: Bearer YOUR_API_KEY' \
      -d '{
        "url": "https://docs.firecrawl.dev"
      }'
  ```
</CodeGroup>

### レスポンス

```json
{
  "success": true,
  "data": {
    "markdown": "<文字列>",
    "content": "<文字列>",
    "html": "<文字列>",
    "rawHtml": "<文字列>",
    "metadata": {
      "title": "<文字列>",
      "description": "<文字列>",
      "language": "<文字列>",
      "sourceURL": "<文字列>",
      "<その他のメタデータ>": "<文字列>",
      "pageStatusCode": 123,
      "pageError": "<文字列>"
    },
    "llm_extraction": {},
    "warning": "<文字列>"
  }
}
```

<div id="extraction">
  ## 抽出
</div>

LLM抽出を使うと、任意のURLから構造化データを簡単に取り出せます。pydanticのスキーマもサポートしており、設定がより容易です。使い方は次のとおりです:

<CodeGroup>
  ```python Python
  class ArticleSchema(BaseModel):
      title: str
      points: int 
      by: str
      commentsURL: str

  class TopArticlesSchema(BaseModel):
  top: List[ArticleSchema] = Field(..., max_items=5, description="トップ5の記事")

  data = app.scrape_url('https://news.ycombinator.com', {
  'extractorOptions': {
  'extractionSchema': TopArticlesSchema.model_json_schema(),
  'mode': 'llm-extraction'
  },
  'pageOptions':{
  'onlyMainContent': True
  }
  })
  print(data["llm_extraction"])
  ```

  ```js JavaScript
  import FirecrawlApp from "@mendable/firecrawl-js";
  import { z } from "zod";

  const app = new FirecrawlApp({
    apiKey: "fc-YOUR_API_KEY",
  });

  // 抽出する内容のためのスキーマを定義
  const schema = z.object({
    top: z
      .array(
        z.object({
          title: z.string(),
          points: z.number(),
          by: z.string(),
          commentsURL: z.string(),
        })
      )
      .length(5)
      .describe("Hacker News のトップ5件のストーリー"),
  });

  const scrapeResult = await app.scrapeUrl("https://news.ycombinator.com", {
    extractorOptions: { extractionSchema: schema },
  });

  console.log(scrapeResult.data["llm_extraction"]);
  ```

  ```go Go
  import (
    "fmt"
    "log"

    "github.com/firecrawl/firecrawl-go"
  )

  app, err := NewFirecrawlApp(TEST_API_KEY, API_URL)
  if err != nil {
    log.Fatalf("FirecrawlApp の初期化に失敗しました: %v", err)
  }

  params := map[string]any{
    "extractorOptions": ExtractorOptions{
      Mode:             "llm-extraction",
      ExtractionPrompt: "ページの情報に基づき、会社のミッション、SSO 対応の有無、オープンソースかどうかを特定してください",
      ExtractionSchema: map[string]any{
        "type": "object",
        "properties": map[string]any{
          "company_mission": map[string]string{"type": "string"},
          "supports_sso":    map[string]string{"type": "boolean"},
          "is_open_source":  map[string]string{"type": "boolean"},
        },
        "required": []string{"company_mission", "supports_sso", "is_open_source"},
      },
    },
  }

  scrapeResult, err := app.ScrapeURL("https://news.ycombinator.com", params)
  if err != nil {
    log.Fatalf("URL のスクレイプに失敗しました: %v", err)
  }
  fmt.Println(scrapeResult.LLMExtraction)
  ```

  ```rust Rust
  use firecrawl::FirecrawlApp;

  #[tokio::main]
  async fn main() {
      // APIキーを使って FirecrawlApp を初期化
      let api_key = "YOUR_API_KEY";
      let api_url = "https://api.firecrawl.dev";
      let app = FirecrawlApp::new(api_key, api_url).expect("Failed to initialize FirecrawlApp");

      // 抽出用のスキーマを定義
      let json_schema = json!({
          "type": "object",
          "properties": {
              "top": {
                  "type": "array",
                  "items": {
                      "type": "object",
                      "properties": {
                          "title": {"type": "string"},
                          "points": {"type": "number"},
                          "by": {"type": "string"},
                          "commentsURL": {"type": "string"}
                      },
                      "required": ["title", "points", "by", "commentsURL"]
                  },
                  "minItems": 5,
                  "maxItems": 5,
                  "description": "Hacker News のトップ5件のストーリー"
              }
          },
          "required": ["top"]
      });

      let llm_extraction_params = json!({
          "extractorOptions": {
              "extractionSchema": json_schema,
              "mode": "llm-extraction"
          },
          "pageOptions": {
              "onlyMainContent": true
          }
      });

      let llm_extraction_result = app
          .scrape_url("https://news.ycombinator.com", Some(llm_extraction_params))
          .await;
      match llm_extraction_result {
          Ok(data) => println!("LLM抽出結果:\n{}", data["llm_extraction"]),
          Err(e) => eprintln!("LLM抽出に失敗しました: {}", e),
      }
  }
  ```

  ```bash cURL
  curl -X POST https://api.firecrawl.dev/v0/scrape \
      -H 'Content-Type: application/json' \
      -H 'Authorization: Bearer YOUR_API_KEY' \
      -d '{
        "url": "https://docs.firecrawl.dev/",
        "extractorOptions": {
          "mode": "llm-extraction",
          "extractionPrompt": "ページの内容に基づき、スキーマに沿って情報を抽出してください。 ",
          "extractionSchema": {
            "type": "object",
            "properties": {
              "company_mission": {
                        "type": "string"
              },
              "supports_sso": {
                        "type": "boolean"
              },
              "is_open_source": {
                        "type": "boolean"
              },
              "is_in_yc": {
                        "type": "boolean"
              }
            },
            "required": [
              "company_mission",
              "supports_sso",
              "is_open_source",
              "is_in_yc"
            ]
          }
        }
      }'
  ```
</CodeGroup>

<div id="contributing">
  ## コントリビューション
</div>

貢献を歓迎しています。プルリクエストを送信する前に、[貢献ガイド](https://github.com/firecrawl/firecrawl/blob/main/CONTRIBUTING.md)をお読みください。