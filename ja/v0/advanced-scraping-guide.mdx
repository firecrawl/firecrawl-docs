---
title: "高度なスクレイピングガイド"
description: "高度なオプションで Firecrawl のスクレイピングを強化する方法を学びます。"
og:title: "高度なスクレイピングガイド | Firecrawl"
og:description: "高度なオプションで Firecrawl のスクレイピングを強化する方法を学びます。"
---

このガイドでは、Firecrawl の各種エンドポイントと、全パラメータを活用した使い方を順を追って解説します。

<div id="basic-scraping-with-firecrawl-scrape">
  ## Firecrawl を使った基本的なスクレイピング（/scrape）
</div>

単一のページをスクレイピングして整形済みの Markdown コンテンツを取得するには、`/scrape` エンドポイントを使用します。

<CodeGroup>

```python Python
# pip install firecrawl-py

from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR_API_KEY")

content = app.scrape_url("https://docs.firecrawl.dev")
```

```JavaScript JavaScript
// npm install @mendable/firecrawl-js

import { FirecrawlApp } from 'firecrawl-js';

const app = new FirecrawlApp({ apiKey: 'YOUR_API_KEY' });

const content = await app.scrapeUrl('https://docs.firecrawl.dev');
```

```go Go
// go get github.com/mendableai/firecrawl-go

import (
  "fmt"
  "log"

  "github.com/mendableai/firecrawl-go"
)

func main() {
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
    log.Fatalf("Failed to initialize FirecrawlApp: %v", err)
  }

  content, err := app.ScrapeURL("docs.firecrawl.dev", nil)
  if err != nil {
    log.Fatalf("Failed)
  }
}
```

```rust Rust
// Install the firecrawl_rs crate with Cargo

use firecrawl_rs::FirecrawlApp;
#[tokio::main]
async fn main() {
  // Initialize the FirecrawlApp with the API key
  let api_key = "YOUR_API_KEY";
  let api_url = "https://api.firecrawl.dev";
  let app = FirecrawlApp::new(api_key, api_url).expect("Failed to initialize FirecrawlApp");

  let scrape_result = app.scrape_url("https://docs.firecrawl.dev", None).await;
  match scrape_result {
    Ok(data) => println!("Scrape Result:\n{}", data["markdown"]),
    Err(e) => eprintln!("Scrape failed: {}", e),
  }
}
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

</CodeGroup>

<div id="scraping-pdfs">
  ## PDFのスクレイピング
</div>

**Firecrawlは標準でPDFのスクレイピングに対応しています。** `/scrape` エンドポイントでPDFリンクを指定すると、PDFのテキストコンテンツを取得できます。無効にするには、`pageOptions.parsePDF` を `false` に設定してください。

<div id="page-options">
  ## ページオプション
</div>

`/scrape` エンドポイントを使用する際は、`pageOptions` パラメータでスクレイピングの挙動をカスタマイズできます。利用可能なオプションは次のとおりです。

<div id="getting-cleaner-content-with-onlymaincontent">
  ### `onlyMainContent` でコンテンツをよりクリーンに取得する
</div>

- **Type**: `boolean`
- **Description**: ヘッダー、ナビゲーションバー、フッターなどを除外し、ページの主要コンテンツのみを返します。
- **Default**: `false`

<div id="getting-the-html-with-includehtml">
  ### `includeHtml` で HTML を取得する
</div>

- **Type**: `boolean`
- **Description**: ページの HTML 版の内容を含めます。レスポンスに `html` キーが追加されます。
- **Default**: `false`

<div id="getting-the-raw-html-with-includerawhtml">
  ### `includeRawHtml` で生のHTMLを取得する
</div>

- **Type**: `boolean`
- **Description**: ページの生のHTMLコンテンツを含めます。これにより、レスポンスに `rawHtml` というキーが追加されます。
- **Default**: `false`

<div id="getting-a-screenshot-of-the-page-with-screenshot">
  ### `screenshot` を使ってページのスクリーンショットを取得する
</div>

- **Type**: `boolean`
- **Description**: スクレイピング対象ページの先頭部分のスクリーンショットを含めます。
- **Default**: `false`

<div id="waiting-for-the-page-to-load-with-waitfor">
  ### `waitFor` を使ったページ読み込みの待機
</div>

- **Type**: `integer`
- **Description**: 最終手段としてのみ使用します。コンテンツ取得前に、ページの読み込み完了まで指定ミリ秒だけ待機します。
- **Default**: `0`

### 使用例

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "pageOptions": {
        "onlyMainContent": true,
        "includeHtml": true,
        "includeRawHtml":true,
        "screenshot": true,
        "waitFor": 5000
      }
    }'
```

この例では、スクレイパーは次を行います:

* ページのメインコンテンツのみを返します。
* レスポンスの`html`キーに生のHTMLコンテンツを含めます。
* コンテンツの取得前に、ページの読み込み完了まで5000ミリ秒（5秒）待機します。

APIリファレンスはこちら: [Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)


<div id="extractor-options">
  ## Extractor Options
</div>

`/scrape` エンドポイントを使用する際は、`extractorOptions` パラメータで、ページのコンテンツから**構造化データを抽出**するためのオプションを指定できます。利用可能なオプションは次のとおりです：

### mode

- **Type**: `string`
- **Enum**: `["llm-extraction", "llm-extraction-from-raw-html"]`
- **Description**: 使用する抽出モード。

  - `llm-extraction`: 正規化・解析済みのコンテンツから情報を抽出します。
  - `llm-extraction-from-raw-html`: 生の HTML から直接情報を抽出します。

- **Type**: `string`
- **Description**: ページから抽出する情報を指示するプロンプト。

<div id="extractionschema">
  ### extractionSchema
</div>

- **タイプ**: `object`
- **説明**: 抽出対象データのスキーマ。抽出結果のデータ構造を定義します。

### 使用例

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev/",
      "extractorOptions": {
        "mode": "llm-extraction",
        "extractionPrompt": "ページの情報に基づき、スキーマに沿って情報を抽出してください。 ",
        "extractionSchema": {
          "type": "object",
          "properties": {
            "company_mission": {
                      "type": "string"
            },
            "supports_sso": {
                      "type": "boolean"
            },
            "is_open_source": {
                      "type": "boolean"
            },
            "is_in_yc": {
                      "type": "boolean"
            }
          },
          "required": [
            "company_mission",
            "supports_sso",
            "is_open_source",
            "is_in_yc"
          ]
        }
      }
    }'
```

```json
{
  "success": true,
  "data": {
    "content": "未加工コンテンツ",
    "metadata": {
      "title": "Mendable",
      "description": "Mendable を使えば、AI チャットアプリを簡単に構築できます。取り込み・カスタマイズし、あとは 1 行のコードでどこへでもデプロイ可能。SideGuide 提供。",
      "robots": "follow, index",
      "ogTitle": "Mendable",
      "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",
      "ogUrl": "https://docs.firecrawl.dev/",
      "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",
      "ogLocaleAlternate": [],
      "ogSiteName": "Mendable",
      "sourceURL": "https://docs.firecrawl.dev/",
    },
    "llm_extraction": {
      "company_mission": "技術リソースを基に安全な AI を学習させ、顧客や従業員の質問に自動で回答し、チームの対応を不要にします"
      "supports_sso": true,
      "is_open_source": false,
      "is_in_yc": true
    }
  }
}
```


<div id="adjusting-timeout">
  ## タイムアウトの調整
</div>

スクレイピング処理のタイムアウトは、ミリ秒単位の `timeout` パラメーターで調整できます。

### 使用例

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "timeout": 50000
    }'
```


<div id="crawling-multiple-pages">
  ## 複数ページのクロール
</div>

複数ページをクロールするには、`/crawl` エンドポイントを使用します。ベースとなるURLを指定すると、アクセス可能なサブページがすべてクロールされます。

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

jobId を返す

```json
{ "jobId": "1234-5678-9101" }
```


<div id="check-crawl-job">
  ### クロールジョブの確認
</div>

クロールジョブのステータスを確認し、結果を取得するために使用します。

```bash
curl -X GET https://api.firecrawl.dev/v0/crawl/status/1234-5678-9101 \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_API_KEY'
```


<div id="crawler-options">
  ### クローラーのオプション
</div>

`/crawl` エンドポイントを使用する際は、`crawlerOptions` パラメータでクロールの挙動をカスタマイズできます。利用可能なオプションは次のとおりです：

<div id="includes">
  #### `includes`
</div>

- **Type**: `array`
- **Description**: クロールに含める URL パターン。これらのパターンに一致する URL のみをクロールします。
- **Example**: `["/blog/*", "/products/*"]`

<div id="excludes">
  #### `excludes`
</div>

- **Type**: `array`
- **Description**: クロール対象から除外するURLパターン。これらのパターンに一致するURLはスキップされます。
- **Example**: `["/admin/*", "/login/*"]`

<div id="returnonlyurls">
  #### `returnOnlyUrls`
</div>

- **Type**: `boolean`
- **Description**: `true` に設定すると、レスポンスには完全なドキュメントデータではなく、URL のリストのみが含まれます。
- **Default**: `false`

<div id="maxdepth">
  #### `maxDepth`
</div>

- **Type**: `integer`
- **Description**: 入力したURLを基準にクロールする最大深度。maxDepth が 0 の場合は入力したURLのみをスクレイピングします。maxDepth が 1 の場合は入力したURLと、その1階層下のすべてのページをスクレイピングします。maxDepth が 2 の場合は入力したURLと、2階層下までのすべてのページをスクレイピングします。より大きい値でも同様のパターンに従います。
- **Example**: `2`

<div id="mode">
  #### `mode`
</div>

- **タイプ**: `string`
- **列挙**: `["default", "fast"]`
- **説明**: 使用するクロールモード。`fast` モードはサイトマップのないウェブサイトを通常の4倍でクロールしますが、精度が低下する場合があり、JavaScriptレンダリングが重いウェブサイトには推奨されません。
- **デフォルト**: `default`

<div id="limit">
  #### `limit`
</div>

- **タイプ**: `integer`
- **説明**: クロールするページ数の上限。
- **デフォルト**: `10000`

### 使い方例

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "crawlerOptions": {
        "includes": ["/blog/*", "/products/*"],
        "excludes": ["/admin/*", "/login/*"],
        "returnOnlyUrls": false,
        "maxDepth": 2,
        "mode": "fast",
        "limit": 1000
      }
    }'
```

この例では、クローラーは次のように動作します:

* `/blog/*` および `/products/*` に一致する URL のみをクロールします。
* `/admin/*` および `/login/*` に一致する URL はスキップします。
* 各ページのドキュメント全体のデータを返します。
* 最大深さ 2 までクロールします。
* 高速クロールモードを使用します。
* 最大 1000 ページをクロールします。


<div id="page-options-crawler-options">
  ## Page Options + Crawler Options
</div>

`pageOptions` と `crawlerOptions` のパラメータを組み合わせて、クロール全体の動作をカスタマイズできます。

<div id="example-usage">
  ### 使い方の例
</div>

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "pageOptions": {
        "onlyMainContent": true,
        "includeHtml": true,
        "includeRawHtml": true,
        "screenshot": true,
        "waitFor": 5000
      },
      "crawlerOptions": {
        "includes": ["/blog/*", "/products/*"],
        "maxDepth": 2,
        "mode": "fast"
      }
    }'
```

この例では、クローラーは次の動作を行います。

* 各ページの主要コンテンツのみを返します。
* 各ページの生の HTML コンテンツを含めます。
* コンテンツを取得する前に、各ページの読み込み完了まで 5000 ミリ秒待機します。
* `/blog/*` および `/products/*` のパターンに一致する URL のみをクロールします。
* 最大深さ 2 までクロールします。
* 高速クロールモードを使用します。


<div id="extractor-options-crawler-options">
  ## Extractor のオプション + Crawler のオプション
</div>

近日公開...