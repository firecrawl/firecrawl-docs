---
title: "データ抽出ツールの選び方"
description: "/agent、/extract、/scrape（JSONモード）を比較して、構造化データ抽出に最適なツールを選択する"
og:title: "データ抽出ツールの選び方 | Firecrawl"
og:description: "/agent、/extract、/scrape（JSONモード）を比較して、構造化データ抽出に最適なツールを選択する"
sidebarTitle: "データ抽出ツールの選び方"
---

import AgentWithSchemaPython from "/snippets/ja/v2/agent/with-schema/python.mdx";
import AgentWithSchemaJS from "/snippets/ja/v2/agent/with-schema/js.mdx";
import AgentWithSchemaCURL from "/snippets/ja/v2/agent/with-schema/curl.mdx";
import ExtractPython from "/snippets/ja/v2/extract/base/python.mdx";
import ExtractNode from "/snippets/ja/v2/extract/base/js.mdx";
import ExtractCURL from "/snippets/ja/v2/extract/base/curl.mdx";
import ScrapeJsonPython from "/snippets/ja/v2/scrape/json/base/python.mdx";
import ScrapeJsonNode from "/snippets/ja/v2/scrape/json/base/js.mdx";
import ScrapeJsonCURL from "/snippets/ja/v2/scrape/json/base/curl.mdx";

Firecrawl は、ウェブページから構造化データを抽出するために 3 つのアプローチを提供します。各アプローチは、自動化と制御のバランスが異なり、用途に応じて使い分けられます。

<div id="quick-comparison">
  ## クイック比較
</div>

| 機能 | `/agent` | `/extract` | `/scrape` (JSONモード) |
|---------|----------|------------|----------------------|
| **ステータス** | 稼働中 | `/agent` を使用 | 稼働中 |
| **URL 必須** | いいえ（任意） | はい（ワイルドカード対応） | はい（単一URL） |
| **スコープ** | Web全体の探索・発見 | 複数ページ／ドメイン | 単一ページ |
| **URL 発見** | 自動Web検索 | 指定URLからクロール | 行わない |
| **処理方式** | 非同期 | 非同期 | 同期 |
| **スキーマ必須** | いいえ（プロンプトまたはスキーマ） | いいえ（プロンプトまたはスキーマ） | いいえ（プロンプトまたはスキーマ） |
| **料金** | 動的（1日あたり5回無料実行） | トークンベース（1クレジット = 15トークン） | 1クレジット／ページ |
| **最適な用途** | リサーチ、探索、複雑な情報収集 | URL が分かっている場合の複数ページ抽出 | URL が分かっている単一ページ抽出 |

<div id="1-agent-endpoint">
  ## 1. `/agent` エンドポイント
</div>

`/agent` エンドポイントは、`/extract` の後継となる Firecrawl の最も高度な機能です。AI エージェントを使用して、自律的にウェブを横断して検索・ナビゲートし、データを収集します。

<Info>
  **リサーチプレビュー**: Agent は早期アクセス段階です。今後も大幅に改善されていきます。[フィードバックをお寄せください →](mailto:product@firecrawl.com)
</Info>

<div id="key-characteristics">
  ### 主な特長
</div>

* **URL は任意**: `prompt` に必要な内容を記述するだけでよく、URL の指定は完全に任意です
* **自律ナビゲーション**: エージェントがサイトを深く検索・巡回し、必要なデータを見つけます
* **ディープウェブ検索**: 複数のドメインやページにまたがる情報を自律的に探索・発見します
* **並列処理**: 複数のソースを同時に処理し、高速に結果を返します
* **利用可能なモデル**: `spark-1-mini`（デフォルト、60% 低コスト）と `spark-1-pro`（高精度）

### 例

<CodeGroup>
  <AgentWithSchemaPython />

  <AgentWithSchemaJS />

  <AgentWithSchemaCURL />
</CodeGroup>

<div id="best-use-case-autonomous-research-discovery">
  ### 最適なユースケース: 自律的なリサーチと発見
</div>

**シナリオ**: Series A 資金調達を行った AI スタートアップについて、その創業者や調達額を含む情報を見つけたい。

**なぜ `/agent` なのか**: どのウェブサイトにその情報があるか分からない場合でも、Agent が自律的にウェブを検索し、関連する情報源（Crunchbase、ニュースサイト、企業ページなど）にアクセスして、構造化データとして集約してくれる。

詳細については、[Agent のドキュメント](/ja/features/agent)を参照してください。

***

<div id="2-extract-endpoint">
  ## 2. `/extract` エンドポイント
</div>

<Note>
  **代わりに `/agent` を使用してください**: [`/agent`](/ja/features/agent) への移行を推奨します。`/agent` はより高速で信頼性が高く、URL を必要とせず、すべての `/extract` のユースケースに加えて、さらに幅広いユースケースに対応できます。
</Note>

`/extract` エンドポイントは、LLM を用いた抽出機能により、指定した URL やドメイン全体から構造化データを収集します。

### 主な特徴

* **通常は URL が必須**: 少なくとも 1 つの URL を指定する必要があります（`example.com/*` のようなワイルドカードに対応）
* **ドメインクロール**: ドメイン内で発見されたすべての URL をクロール・解析可能
* **ウェブ検索による拡張**: 指定ドメイン外のリンクをたどるオプション `enableWebSearch` を利用可能
* **スキーマは任意**: 厳密な JSON スキーマまたは自然言語プロンプトのどちらにも対応
* **非同期処理**: ステータス確認用のジョブ ID を返却

<Tip>
  `/extract` は、URL なしでもプロンプトだけで抽出が可能になりました。ただしこのユースケースでは、`/agent` の方がより良い結果を返します。
</Tip>

<div id="the-url-limitation">
  ### URL の制約
</div>

`/extract` の根本的な課題は、通常は事前に URL を把握しておく必要があることです：

1. **発見のギャップ**: 「YC W24 の企業を探す」のようなタスクでは、どの URL にデータが含まれているか分かりません。`/extract` を呼び出す前に、別途検索ステップが必要になります。
2. **扱いづらい Web 検索**: `enableWebSearch` は存在しますが、指定した URL からしか開始できず、探索タスクには不自然なワークフローになります。
3. **`/agent` が作られた理由**: `/extract` は既知の場所からの抽出には向いていますが、データがどこに存在するかを見つける用途にはあまり向いていません。

<div id="example">
  ### 使用例
</div>

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

<div id="best-use-case-targeted-multi-page-extraction">
  ### 最適なユースケース: ターゲットを絞ったマルチページ抽出
</div>

**シナリオ**: 競合他社のドキュメントの URL があり、`docs.competitor.com/*` からすべての API エンドポイントを抽出したい場合。

**ここで `/extract` が有効だった理由**: 対象のドメインが正確に分かっていたためです。ただし、その場合でも、現在では URL を指定して `/agent` を使うほうが、一般的により良い結果が得られます。

詳細については、[Extract のドキュメント](/ja/features/extract)を参照してください。

***

<div id="3-scrape-endpoint-with-json-mode">
  ## 3. JSONモード付きの `/scrape` エンドポイント
</div>

JSONモード付きの `/scrape` エンドポイントは、最も制御しやすいアプローチです。特定の1つのURLから構造化データを抽出し、ページ内容をLLMでパースして、指定したスキーマに変換します。

<div id="key-characteristics">
  ### 主な特長
</div>

* **単一URLのみ**: 一度に特定の1ページからデータを抽出するよう設計されています
* **正確なURLが必要**: データを含む正確なURLを把握している必要があります
* **スキーマは任意**: JSON Schemaを使うことも、プロンプトだけを使うことも可能（LLMが構造を選択）
* **同期処理**: データを即時に返します（ジョブのポーリングは不要）
* **追加フォーマット**: 1つのリクエストでJSON抽出をmarkdown、HTML、スクリーンショットと組み合わせ可能です

<div id="example">
  ### 使用例
</div>

<CodeGroup>
  <ScrapeJsonPython />

  <ScrapeJsonNode />

  <ScrapeJsonCURL />
</CodeGroup>

<div id="best-use-case-single-page-precision-extraction">
  ### 最適なユースケース：単一ページの高精度抽出
</div>

**シナリオ**: 価格監視ツールを構築しており、すでにURLがわかっている特定の商品ページから、価格・在庫状況・商品詳細を抽出する必要がある。

**なぜ `/scrape` と JSONモードなのか**: どのページにデータがあるかを正確に把握しており、単一ページから高精度に抽出したく、ジョブ管理のオーバーヘッドなしで同期的に結果を得たい場合に最適。

詳細については、[JSONモードのドキュメント](/ja/features/llm-extract)を参照してください。

***

<div id="decision-guide">
  ## 選択ガイド
</div>

**データを含む正確なURLが分かっていますか？**

* **いいえ** → `/agent` を使用（自律的なWeb検索）
* **はい**
  * **単一ページ？** → `/scrape` をJSONモードで使用
  * **複数ページ？** → `/agent` をURLと共に使用（または `/scrape` のバッチ処理）

<div id="recommendations-by-scenario">
  ### シナリオ別の推奨エンドポイント
</div>

| シナリオ | 推奨エンドポイント |
|----------|---------------------|
| 「すべてのAIスタートアップとその資金調達情報を取得する」 | `/agent` |
| 「この特定のプロダクトページからデータを抽出する」 | `/scrape` (JSONモード) |
| 「competitor.com のすべてのブログ投稿を取得する」 | `/agent`（URL指定） |
| 「既知の複数のURLの価格をモニタリングする」 | `/scrape`（バッチ処理） |
| 「特定の業界の企業をリサーチする」 | `/agent` |
| 「既知の50社の企業ページから連絡先情報を抽出する」 | `/scrape`（バッチ処理） |

***

<div id="pricing">
  ## 料金
</div>

| エンドポイント | コスト | 備考 |
|----------|------|-------|
| `/scrape` (JSONモード) | 1クレジット/ページ | 固定で予測可能 |
| `/extract` | トークンベース (1クレジット＝15トークン) | コンテンツに応じて変動 |
| `/agent` | 動的 | 1日あたり5回まで無料で実行可能、複雑さに応じて変動 |

<div id="example-find-the-founders-of-firecrawl">
  ### 例: 「Find the founders of Firecrawl」
</div>

| Endpoint | 動作内容 | 消費クレジット |
|----------|----------|----------------|
| `/scrape` | 自分でURLを見つけて、その1ページをスクレイプする | 約1クレジット |
| `/extract` | URLを指定すると、構造化データを抽出する | 可変（トークンベース） |
| `/agent` | プロンプトを送るだけで、エージェントが発見と抽出を行う | 約15クレジット |

**トレードオフ**: `/scrape` は最も低コストだが、URLを事前に把握している必要がある。`/agent` はコストが高いものの、対象の発見を自動で行ってくれる。

詳細な料金については、[Firecrawl Pricing](https://firecrawl.dev/pricing) を参照してください。

***

<div id="migration-extract-agent">
  ## 移行: `/extract` → `/agent`
</div>

現在 `/extract` を使用している場合、移行は容易です:

**移行前（extract）:**

```python
result = app.extract(
    urls=["https://example.com/*"],
    prompt="製品情報を抽出",
    schema=schema
)
```

**変更後（エージェント使用時）:**

```python
result = app.agent(
    urls=["https://example.com"],  # オプション - 完全に省略可能
    prompt="Extract product information from example.com",
    schema=schema,
    model="spark-1-mini"  # より高精度な場合は "spark-1-pro"
)
```

主な利点は、`/agent` を使えば URL をまったく指定せずに、必要なことを言葉で伝えるだけでよい点です。

***

<div id="key-takeaways">
  ## 重要なポイント
</div>

1. **正確なURLがわかっている場合は？** `/scrape` を `JSONモード` で使用してください — 最も安価（1クレジット/ページ）、最速（同期処理）、かつ最も予測しやすいオプションです。

2. **自律的なリサーチが必要ですか？** `/agent` を使用してください — 自動で探索を行い、1日5回まで無料で利用でき、その後は複雑さに応じた動的な料金体系になります。

3. 新しいプロジェクトでは **`/extract` から `/agent` へ移行** してください — `/agent` は、より高機能な後継エンドポイントです。

4. **コストと利便性のトレードオフ**: URLが分かっている場合は `/scrape` が最もコスト効率に優れています。一方で `/agent` はコストが高くなりますが、URLの手動探索が不要になります。

***

<div id="further-reading">
  ## 関連ドキュメント
</div>

* [エージェント機能のドキュメント](/ja/features/agent)
* [エージェントモデル](/ja/features/models)
* [JSONモードのドキュメント](/ja/features/llm-extract)
* [Extract 機能のドキュメント](/ja/features/extract)
* [バッチスクレイピングのドキュメント](/ja/features/batch-scrape)