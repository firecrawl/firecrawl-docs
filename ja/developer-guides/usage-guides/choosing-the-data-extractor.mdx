---
title: "データ抽出ツールの選び方"
description: "/agent、/extract、/scrape（JSONモード）を比較して、構造化データの抽出に最適なツールを選ぶ"
og:title: "データ抽出ツールの選び方 | Firecrawl"
og:description: "/agent、/extract、/scrape（JSONモード）を比較して、構造化データの抽出に最適なツールを選ぶ"
sidebarTitle: "データ抽出ツールの選び方"
---

import AgentWithSchemaPython from "/snippets/ja/v2/agent/with-schema/python.mdx";
import AgentWithSchemaJS from "/snippets/ja/v2/agent/with-schema/js.mdx";
import AgentWithSchemaCURL from "/snippets/ja/v2/agent/with-schema/curl.mdx";
import ExtractPython from "/snippets/ja/v2/extract/base/python.mdx";
import ExtractNode from "/snippets/ja/v2/extract/base/js.mdx";
import ExtractCURL from "/snippets/ja/v2/extract/base/curl.mdx";
import ScrapeJsonPython from "/snippets/ja/v2/scrape/json/base/python.mdx";
import ScrapeJsonNode from "/snippets/ja/v2/scrape/json/base/js.mdx";
import ScrapeJsonCURL from "/snippets/ja/v2/scrape/json/base/curl.mdx";

Firecrawl は、Web ページから構造化データを抽出するために 3 つの手法を提供します。いずれも自動化と制御の度合いが異なり、それぞれ異なるユースケースに対応します。

<div id="quick-comparison">
  ## クイック比較
</div>

| 機能 | `/agent` | `/extract` | `/scrape` (JSONモード) |
|---------|----------|------------|----------------------|
| **ステータス** | 利用可能 | 代わりに `/agent` を使用 | 利用可能 |
| **URL 必須** | 不要（任意） | 必須（ワイルドカード対応） | 必須（単一URL） |
| **スコープ** | Web 全体の探索 | 複数ページ／ドメイン | 単一ページ |
| **URL 検出** | 自律的なウェブ検索 | 指定された URL からのクロール | なし |
| **処理方式** | 非同期 | 非同期 | 同期 |
| **スキーマ必須** | 不要（プロンプトまたはスキーマ） | 不要（プロンプトまたはスキーマ） | 不要（プロンプトまたはスキーマ） |
| **料金** | 動的課金（1日5回まで無料実行） | トークン制（1クレジット = 15トークン） | 1ページあたり1クレジット |
| **最適な用途** | リサーチ、探索、複雑な情報収集 | URL が分かっている場合の複数ページ抽出 | 既知の単一ページ抽出 |

<div id="1-agent-endpoint">
  ## 1. `/agent` Endpoint
</div>

`/agent` エンドポイントは、`/extract` の後継となる、Firecrawl で最も高度な機能です。AI エージェントを用いてウェブ上を自律的に検索・巡回し、データを収集します。

<div id="key-characteristics">
  ### 主な特長
</div>

* **URL の指定は任意**: 必要な内容を `prompt` に書くだけでよく、URL は完全に任意
* **自律ナビゲーション**: エージェントがサイトを深く検索・巡回して必要なデータを発見
* **ディープ Web 検索**: 複数ドメインや複数ページにまたがる情報を自律的に探索
* **並列処理**: 複数の情報源を同時に処理して、結果を高速に取得
* **利用可能なモデル**: `spark-1-mini`（デフォルト、60% 安価）と `spark-1-pro`（高精度）

### 例

<CodeGroup>
  <AgentWithSchemaPython />

  <AgentWithSchemaJS />

  <AgentWithSchemaCURL />
</CodeGroup>

<div id="best-use-case-autonomous-research-discovery">
  ### 最適な利用シナリオ: 自律的なリサーチと探索
</div>

**シナリオ**: Series A ラウンドで資金調達を行った AI スタートアップについて、創業者や調達額などの情報を見つける必要がある。

**なぜ `/agent` なのか**: どの Web サイトにその情報があるか分からなくても、Agent が自律的にウェブを検索し、関連する情報源（Crunchbase、ニュースサイト、企業ページなど）をたどって、構造化データとしてまとめてくれる。

詳細については、[Agent ドキュメント](/ja/features/agent) を参照してください。

***

<div id="2-extract-endpoint">
  ## 2. `/extract` エンドポイント
</div>

<Note>
  **代わりに `/agent` を使ってください**：[`/agent`](/ja/features/agent) への移行を推奨します。より高速かつ信頼性が高く、URL も不要で、`/extract` のすべてのユースケースに加え、それ以上の機能もカバーします。
</Note>

`/extract` エンドポイントは、LLM を用いた抽出機能により、指定した URL やドメイン全体から構造化データを収集します。

### 主な特徴

* **通常はURL指定が必須**: 少なくとも1つのURLを指定（`example.com/*` のようなワイルドカードをサポート）
* **ドメインクロール**: ドメイン内で検出されたすべてのURLをクロール・解析可能
* **ウェブ検索の拡張**: 任意の `enableWebSearch` オプションで、指定ドメイン外のリンクもたどることが可能
* **スキーマは任意**: 厳密な JSON スキーマまたは自然言語プロンプトのどちらもサポート
* **非同期処理**: ステータス確認用のジョブIDを返す

<div id="the-url-limitation">
  ### URL に関する制約
</div>

`/extract` の根本的な課題は、通常あらかじめ URL を把握しておく必要がある点です。

1. **探索ギャップ**: 「YC W24 の企業を探す」のようなタスクでは、どの URL にデータが含まれているか分かりません。`/extract` を呼び出す前に、別途検索ステップが必要になります。
2. **扱いにくい Web 検索**: `enableWebSearch` は存在しますが、あくまで指定した URL から開始するという制約があり、探索タスクのワークフローとしては扱いづらい設計です。
3. **`/agent` が作られた理由**: `/extract` は既知の場所からの抽出には適していますが、データがどこに存在するかを見つける用途にはあまり向いていません。

<div id="example">
  ### 使用例
</div>

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

<div id="best-use-case-targeted-multi-page-extraction">
  ### 最適なユースケース: ターゲットを絞った複数ページからの抽出
</div>

**シナリオ**: 競合他社のドキュメント URL を持っており、`docs.competitor.com/*` からすべての API エンドポイントを抽出したい。

**ここで `/extract` が有効だった理由**: 正確なドメインを把握していたためです。ただし同じ状況でも、URL を指定して `/agent` を使う方が、現時点では一般的により良い結果が得られます。

詳細については、[Extract のドキュメント](/ja/features/extract)を参照してください。

***

<div id="3-scrape-endpoint-with-json-mode">
  ## 3. JSONモード付きの `/scrape` エンドポイント
</div>

JSONモード付きの `/scrape` エンドポイントは、最も制御しやすいアプローチです。既知の単一の URL から構造化データを抽出し、LLM を使ってページ内容を解析し、指定したスキーマに沿った形に整形します。

### 主な特徴

* **単一URLのみ**: 一度に特定の1ページからデータを抽出するように設計されています
* **正確なURLが必要**: データを含む正確なURLを知っている必要があります
* **スキーマは任意**: JSON Schema を使っても、プロンプトだけでもよい（LLM が構造を選択）
* **同期処理**: データを即時に返却（ジョブのポーリングは不要）
* **追加フォーマット**: 1つのリクエストで JSON 抽出と Markdown、HTML、スクリーンショットを組み合わせ可能

<div id="example">
  ### 使用例
</div>

<CodeGroup>
  <ScrapeJsonPython />

  <ScrapeJsonNode />

  <ScrapeJsonCURL />
</CodeGroup>

<div id="best-use-case-single-page-precision-extraction">
  ### 最適なユースケース: 単一ページの精密抽出
</div>

**シナリオ**: 価格監視ツールを構築しており、すでにURLがわかっている特定の商品ページから、価格・在庫状況・商品詳細を抽出する必要がある。

**`/scrape` を JSONモードで使う理由**: どのページにデータがあるかを正確に把握していて、単一ページからの精密な抽出が必要であり、ジョブ管理のオーバーヘッドなしに同期的に結果を取得したいため。

詳細については、[JSONモードのドキュメント](/ja/features/llm-extract)を参照してください。

***

<div id="decision-guide">
  ## 選択ガイド
</div>

**データを含む正確な URL が分かっていますか？**

* **いいえ** → `/agent` を使用（自動ウェブ探索）
* **はい**
  * **1ページのみ？** → `/scrape` を JSONモードで使用
  * **複数ページ？** → 対象URLを指定して `/agent` を使用（または `/scrape` をバッチ実行）

<div id="recommendations-by-scenario">
  ### シナリオ別の推奨エンドポイント
</div>

| シナリオ | 推奨エンドポイント |
|----------|---------------------|
| 「すべてのAIスタートアップとその資金調達情報を見つけたい」 | `/agent` |
| 「この特定のプロダクトページからデータを抽出したい」 | `/scrape` (JSONモード) |
| 「competitor.com のすべてのブログ記事を取得したい」 | `/agent`（URL指定） |
| 「既知の複数のURLの価格を監視したい」 | `/scrape`（バッチ処理） |
| 「特定の業界の企業をリサーチしたい」 | `/agent` |
| 「既知の50社分の企業ページから連絡先情報を抽出したい」 | `/scrape`（バッチ処理） |

***

<div id="pricing">
  ## 料金
</div>

| エンドポイント | コスト | 補足 |
|----------|------|-------|
| `/scrape` (JSONモード) | 1クレジット/ページ | 固定・予測しやすい |
| `/extract` | トークンベース (1クレジット = 15トークン) | コンテンツに応じて変動 |
| `/agent` | 変動制 | 1日5回まで無料実行可、複雑さにより変動 |

<div id="example-find-the-founders-of-firecrawl">
  ### 例: 「Firecrawl の創業者を探す」
</div>

| Endpoint | 動作概要 | 使用クレジット |
|----------|--------------|--------------|
| `/scrape` | 自分で URL を見つけて、1 ページをスクレイピングする | 約 1 クレジット |
| `/extract` | URL を渡すと、構造化データを抽出する | トークン数に応じて変動 |
| `/agent` | プロンプトを送るだけで、エージェントが発見と抽出を行う | 約 15 クレジット |

**トレードオフ**: `/scrape` は最も安価だが、URL を事前に知っている必要がある。`/agent` はコストが高いが、発見を自動で処理する。

詳細な料金については、[Firecrawl Pricing](https://firecrawl.dev/pricing) を参照してください。

***

<div id="migration-extract-agent">
  ## 移行: `/extract` → `/agent`
</div>

現在 `/extract` を使用している場合、移行は容易です。

**移行前 (`/extract`):**

```python
result = app.extract(
    urls=["https://example.com/*"],
    prompt="製品情報を抽出",
    schema=schema
)
```

**After（エージェント）：**

```python
result = app.agent(
    urls=["https://example.com"],  # オプション - 完全に省略可能
    prompt="Extract product information from example.com",
    schema=schema,
    model="spark-1-mini"  # より高精度な場合は "spark-1-pro"
)
```

最大の利点は、`/agent` を使えば URL を一切指定せず、必要な内容を説明するだけで済むことです。

***

<div id="key-takeaways">
  ## 重要なポイント
</div>

1. **正確なURLがわかっている？** `/scrape` と JSONモードを使用してください — 最も安価（1クレジット/ページ）で、最速（同期処理）、かつ最も予測しやすいオプションです。

2. **自律的なリサーチが必要？** `/agent` を使用してください — 自動でサイトを探索し、1日5回までは無料、その後は複雑さに応じた動的課金になります。

3. 新しいプロジェクトでは **`/extract` から `/agent` へ移行** してください — `/agent` はより高機能な後継エンドポイントです。

4. **コストと利便性のトレードオフ**: URLがわかっている場合は `/scrape` が最もコスト効率に優れます。一方で、`/agent` はコストが高くなりますが、URLの手動探索作業を不要にします。

***

<div id="further-reading">
  ## 関連ドキュメント
</div>

* [エージェントのドキュメント](/ja/features/agent)
* [エージェントモデル](/ja/features/models)
* [JSONモードのドキュメント](/ja/features/llm-extract)
* [Extract のドキュメント](/ja/features/extract)
* [バッチスクレイピング](/ja/features/batch-scrape)