---
title: 'Python'
description: 'Firecrawl Python SDK は、Firecrawl API のラッパーで、ウェブサイトを手軽に Markdown に変換できます。'
icon: 'python'
og:title: "Python SDK | Firecrawl"
og:description: "Firecrawl Python SDK は、Firecrawl API のラッパーで、ウェブサイトを手軽に Markdown に変換できます。"
---

import InstallationPython from '/snippets/ja/v2/installation/python.mdx'
import ScrapePythonShort from '/snippets/ja/v2/scrape/short/python.mdx'
import CrawlPythonShort from '/snippets/ja/v2/crawl/short/python.mdx'
import CheckCrawlStatusPythonShort from '/snippets/ja/v2/crawl-status/short/python.mdx'
import StartCrawlPythonShort from '/snippets/ja/v2/start-crawl/short/python.mdx'
import CancelCrawlPythonShort from '/snippets/ja/v2/crawl-delete/short/python.mdx'
import MapPythonShort from '/snippets/ja/v2/map/short/python.mdx'
import ExtractPythonShort from '/snippets/v2/extract/short/python.mdx'
import ScrapeAndCrawlExamplePython from '/snippets/ja/v2/scrape-and-crawl/python.mdx'
import CrawlWebSocketPythonBase from '/snippets/ja/v2/crawl-websocket/base/python.mdx'
import AIOPython from '/snippets/ja/v2/async/base/python.mdx'

<div id="installation">
  ## インストール
</div>

Firecrawl の Python SDK をインストールするには、pip を使用します：

<InstallationPython />

<div id="usage">
  ## 使い方
</div>

1. [firecrawl.dev](https://firecrawl.dev) で API キーを取得します
2. API キーを環境変数 `FIRECRAWL_API_KEY` に設定するか、`Firecrawl` クラスにパラメータとして渡します。

SDK の使用例:

<ScrapeAndCrawlExamplePython />

<div id="scraping-a-url">
  ### URLのスクレイピング
</div>

単一のURLをスクレイピングするには、`scrape` メソッドを使用します。URLを引数に取り、取得したドキュメントを返します。

<ScrapePythonShort />

<div id="crawl-a-website">
  ### ウェブサイトをクロールする
</div>

ウェブサイトをクロールするには、`crawl` メソッドを使用します。開始URLと任意のオプションを引数に取ります。オプションでは、クロールするページ数の上限、許可するドメイン、出力フォーマットなど、クロールジョブの追加設定を指定できます。自動/手動のページネーションや制限については [Pagination](#pagination) を参照してください。

<CrawlPythonShort />

<div id="start-a-crawl">
  ### クロールを開始
</div>

<Tip>ノンブロッキングがお好みですか？下の[Async Class](#async-class)セクションをご覧ください。</Tip>

`start_crawl` を使うと待たずにジョブを開始できます。ステータス確認に使えるジョブの `ID` を返します。完了までブロックして待機したい場合は `crawl` を使用してください。ページングの動作と制限は [Pagination](#pagination) を参照してください。

<StartCrawlPythonShort />

<div id="checking-crawl-status">
  ### クロールのステータス確認
</div>

クロールジョブのステータスを確認するには、`get_crawl_status` メソッドを使用します。ジョブIDを引数に取り、クロールジョブの現在のステータスを返します。

<CheckCrawlStatusPythonShort />

<div id="cancelling-a-crawl">
  ### クロールのキャンセル
</div>

クロールジョブをキャンセルするには、`cancel_crawl` メソッドを使用します。`start_crawl` のジョブIDを引数に取り、キャンセル結果のステータスを返します。

<CancelCrawlPythonShort />

<div id="map-a-website">
  ### ウェブサイトをマッピングする
</div>

`map` を使って、ウェブサイトから URL の一覧を生成します。オプションで、サブドメインの除外やサイトマップの利用など、マッピングの挙動をカスタマイズできます。

<MapPythonShort />

{/* ### ウェブサイトから構造化データを抽出する

  ウェブサイトから構造化データを抽出するには、`extract` メソッドを使用します。抽出対象の URL、プロンプト、スキーマを引数に取ります。スキーマは、抽出データの構造を定義する Pydantic モデルです。

  <ExtractPythonShort /> */}

<div id="crawling-a-website-with-websockets">
  ### WebSockets を使ったウェブサイトのクロール
</div>

WebSockets でウェブサイトをクロールするには、`start_crawl` でジョブを開始し、`watcher` ヘルパーで購読します。ジョブ ID を指定して watcher を作成し、`start()` を呼び出す前にハンドラー（例: page、completed、failed）を登録します。

<CrawlWebSocketPythonBase />

<div id="pagination">
  ### ページネーション
</div>

Firecrawl の /crawl および batch の各エンドポイントは、追加のデータがある場合に `next` URL を返します。Python SDK はデフォルトで自動ページネーションを行い、すべてのドキュメントを集約します。この場合、`next` は `None` になります。自動ページネーションを無効化したり、上限を設定することも可能です。

<div id="crawl">
  #### クロール
</div>

最も手軽なのはウェイター方式の `crawl` を使うことです。もしくはジョブを開始して手動でページ処理を行ってください。

<div id="simple-crawl-auto-pagination-default">
  ##### シンプルなクロール（自動ページネーション、デフォルト）
</div>

* 既定のフローについては[ウェブサイトをクロールする](#crawl-a-website)を参照してください。

<div id="manual-crawl-with-pagination-control-single-page">
  ##### ページネーションを手動制御するクロール（単一ページ）
</div>

* ジョブを開始し、`auto_paginate=False` を指定して1ページずつ取得します。

```python Python
crawl_job = client.start_crawl("https://example.com", limit=100)

status = client.get_crawl_status(crawl_job.id, pagination_config=PaginationConfig(auto_paginate=False))
print("単一ページのクロール:", status.status, "ドキュメント数:", len(status.data), "次:", status.next)
```

<div id="manual-crawl-with-limits-auto-pagination-early-stop">
  ##### 制限付きの手動クロール（自動ページネーション + 早期停止）
</div>

* 自動ページネーションは有効のまま、`max_pages`、`max_results`、または `max_wait_time` で早期停止します。

```python Python
status = client.get_crawl_status(
    crawl_job.id,
    pagination_config=PaginationConfig(max_pages=2, max_results=50, max_wait_time=15),
)
print("クロール制限:", status.status, "ドキュメント数:", len(status.data), "次のページ:", status.next)
```

<div id="batch-scrape">
  #### バッチスクレイプ
</div>

waiter メソッド `batch_scrape` を使うか、ジョブを開始して手動でページングします。

<div id="simple-batch-scrape-auto-pagination-default">
  ##### シンプルなバッチスクレイプ（自動ページネーション、デフォルト）
</div>

* 既定のフローは [Batch Scrape](/ja/features/batch-scrape) を参照してください。

<div id="manual-batch-scrape-with-pagination-control-single-page">
  ##### ページネーション制御付きの手動バッチスクレイプ（単一ページ）
</div>

* ジョブを開始し、`auto_paginate=False` を指定して1ページずつ取得します。

```python Python
batch_job = client.start_batch_scrape(urls)
status = client.get_batch_scrape_status(batch_job.id, pagination_config=PaginationConfig(auto_paginate=False))
print("バッチの単一ページ:", status.status, "ドキュメント数:", len(status.data), "次:", status.next)
```

<div id="manual-batch-scrape-with-limits-auto-pagination-early-stop">
  ##### 制限付きの手動バッチスクレイプ（自動ページネーション + 早期停止）
</div>

* 自動ページネーションは有効にしたまま、`max_pages`、`max_results`、または `max_wait_time` で早期に停止します。

```python Python
status = client.get_batch_scrape_status(
    batch_job.id,
    pagination_config=PaginationConfig(max_pages=2, max_results=100, max_wait_time=20),
)
print("バッチ制限:", status.status, "ドキュメント数:", len(status.data), "次:", status.next)
```

<div id="error-handling">
  ## エラーハンドリング
</div>

SDK は Firecrawl API から返されるエラーを処理し、適切な例外をスローします。リクエスト中にエラーが発生した場合は、わかりやすいエラーメッセージ付きの例外がスローされます。

<div id="async-class">
  ## 非同期クラス
</div>

非同期処理には `AsyncFirecrawl` クラスを使用します。メソッドは `Firecrawl` と同等ですが、メインスレッドをブロックしません。

<AIOPython />