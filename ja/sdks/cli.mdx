---
title: 'Skill + CLI'
description: 'Firecrawl Skill は、Claude Code、Antigravity、OpenCode などの AI エージェントが CLI を通じて Firecrawl を利用するための簡単な方法です。'
og:title: "CLI | Firecrawl"
og:description: "Firecrawl Skills は、AI エージェントが CLI を通じて Firecrawl を利用するための簡単な方法です。AI エージェントは、よりコンテキスト効率に優れたインターフェース経由で Web データを取得できます。"
---

import InstallationCLI from '/snippets/ja/v2/cli/installation/bash.mdx'
import AuthLogin from '/snippets/ja/v2/cli/auth/login.mdx'
import AuthLogout from '/snippets/ja/v2/cli/auth/logout.mdx'
import AuthConfig from '/snippets/ja/v2/cli/auth/config.mdx'
import AuthSelfHosted from '/snippets/ja/v2/cli/auth/self-hosted.mdx'
import ScrapeBasic from '/snippets/ja/v2/cli/scrape/basic.mdx'
import ScrapeFormats from '/snippets/ja/v2/cli/scrape/formats.mdx'
import ScrapeOptions from '/snippets/ja/v2/cli/scrape/options.mdx'
import CrawlBasic from '/snippets/ja/v2/cli/crawl/basic.mdx'
import CrawlStatus from '/snippets/ja/v2/cli/crawl/status.mdx'
import CrawlOptions from '/snippets/ja/v2/cli/crawl/options.mdx'
import MapBasic from '/snippets/ja/v2/cli/map/basic.mdx'
import MapOptions from '/snippets/ja/v2/cli/map/options.mdx'
import SearchBasic from '/snippets/ja/v2/cli/search/basic.mdx'
import SearchOptions from '/snippets/ja/v2/cli/search/options.mdx'
import AgentBasic from '/snippets/ja/v2/cli/agent/basic.mdx'
import AgentOptions from '/snippets/ja/v2/cli/agent/options.mdx'
import BrowserBasic from '/snippets/ja/v2/browser/cli/basic.mdx'
import BrowserOptions from '/snippets/ja/v2/browser/cli/options.mdx'


<div id="installation">
  ## インストール
</div>

npm を使って Firecrawl CLI をグローバルインストールします:

<InstallationCLI />

Claude Code などの AI エージェントを利用している場合は、下記の Firecrawl スキルをインストールすると、エージェントが自動的にセットアップを行います。

```bash
npx -y firecrawl-cli@latest init --all --browser
```

* `--all` は検出されたすべての AI コーディングエージェントに Firecrawl スキルをインストールします
* `--browser` は Firecrawl の認証を自動で行うためにブラウザを自動的に開きます

<Note>
  スキルをインストールした後、新しいスキルを認識させるために Claude Code を再起動してください。
</Note>


<div id="authentication">
  ## 認証
</div>

CLI を使用する前に、Firecrawl API キーを使って認証する必要があります。

<div id="login">
  ### ログイン
</div>

<AuthLogin />

<div id="view-configuration">
  ### 設定の表示
</div>

<AuthConfig />

<div id="logout">
  ### ログアウト
</div>

<AuthLogout />

<div id="self-hosted-local-development">
  ### セルフホスト / ローカル開発
</div>

セルフホスト環境の Firecrawl インスタンスやローカル開発では、`--api-url` オプションを使用してください:

<AuthSelfHosted />

カスタム API URL（`https://api.firecrawl.dev` 以外のもの）を使用している場合、API キー認証は自動的にスキップされるため、ローカルインスタンスでは API キーなしで利用できます。

<div id="check-status">
  ### ステータスの確認
</div>

インストールと認証が正しく行われているかを確認し、レート制限も確認します。

```bash CLI
firecrawl --status
```

準備完了時の出力:

```
  🔥 firecrawl cli v1.1.1

  ● FIRECRAWL_API_KEYで認証済み
  同時実行数: 0/100ジョブ (並列スクレイプ制限)
  クレジット: 残り500,000
```

* **同時実行数 (Concurrency)**: 並列に実行できるジョブの最大数。この上限付近まで並列処理を行ってもよいが、超えないようにする。
* **クレジット (Credits)**: 残りの API クレジット数。各 `scrape`/`crawl` はクレジットを消費する。


<div id="commands">
  ## コマンド
</div>

<div id="scrape">
  ### Scrape
</div>

1つのURLをスクレイピングし、そのコンテンツをさまざまなフォーマットで抽出します。

<Tip>
`--only-main-content` を使用すると、ナビゲーション、フッター、広告を除いたクリーンな出力を取得できます。記事やメインページのコンテンツのみが必要なほとんどのユースケースで推奨されます。
</Tip>

<ScrapeBasic />

<div id="output-formats">
  #### 出力フォーマット
</div>

<ScrapeFormats />

<div id="scrape-options">
  #### スクレイプのオプション
</div>

<ScrapeOptions />

**利用可能なオプション:**

| Option | Short | Description |
|--------|-------|-------------|
| `--url <url>` | `-u` | スクレイプする URL（位置引数の代わり） |
| `--format <formats>` | `-f` | 出力フォーマット（カンマ区切り）：`markdown`, `html`, `rawHtml`, `links`, `screenshot`, `json`, `images`, `summary`, `changeTracking`, `attributes`, `branding` |
| `--html` | `-H` | `--format html` のショートカット |
| `--only-main-content` | | メインのコンテンツのみを抽出 |
| `--wait-for <ms>` | | JS のレンダリングを待機する時間（ミリ秒） |
| `--screenshot` | | スクリーンショットを撮影 |
| `--include-tags <tags>` | | 含める HTML タグ（カンマ区切り） |
| `--exclude-tags <tags>` | | 除外する HTML タグ（カンマ区切り） |
| `--output <path>` | `-o` | 出力をファイルに保存 |
| `--json` | | 単一のフォーマット指定でも JSON 出力を強制 |
| `--pretty` | | JSON 出力を整形して表示 |
| `--timing` | | リクエストのタイミングやその他の有用な情報を表示 |

---

<div id="search">
  ### Search
</div>

ウェブ検索を行い、必要に応じて結果をスクレイピングします。

<SearchBasic />

<div id="search-options">
  #### 検索オプション
</div>

<SearchOptions />

**利用可能なオプション:**

| オプション | 説明 |
|--------|-------------|
| `--limit <number>` | 最大結果数（デフォルト: 5、最大: 100） |
| `--sources <sources>` | 検索対象のソース: `web`、`images`、`news`（カンマ区切り） |
| `--categories <categories>` | カテゴリでフィルタリング: `github`、`research`、`pdf`（カンマ区切り） |
| `--tbs <value>` | 時間フィルタ: `qdr:h`（時間）、`qdr:d`（日）、`qdr:w`（週）、`qdr:m`（月）、`qdr:y`（年） |
| `--location <location>` | ジオターゲティング（例: "Berlin,Germany"） |
| `--country <code>` | ISO 国コード（デフォルト: US） |
| `--timeout <ms>` | タイムアウト（ミリ秒単位、デフォルト: 60000） |
| `--ignore-invalid-urls` | 他の Firecrawl エンドポイントで利用できない URL を除外 |
| `--scrape` | 検索結果をスクレイピング |
| `--scrape-formats <formats>` | スクレイピングしたコンテンツのフォーマット（デフォルト: markdown） |
| `--only-main-content` | スクレイピング時にメインコンテンツのみを含める（デフォルト: true） |
| `--json` | JSON として出力 |
| `--output <path>` | 出力をファイルに保存 |
| `--pretty` | JSON 出力を見やすく整形して表示 |

---

<div id="map">
  ### Map
</div>

ウェブサイト内のすべてのURLを迅速に検出します。

<MapBasic />

<div id="map-options">
  #### Map オプション
</div>

<MapOptions />

**利用可能なオプション:**

| オプション | 説明 |
|--------|-------------|
| `--url <url>` | マッピング対象の URL（位置引数の代替） |
| `--limit <number>` | 検出する最大 URL 数 |
| `--search <query>` | 検索クエリで URL を絞り込み |
| `--sitemap <mode>` | サイトマップの処理モード: `include`, `skip`, `only` |
| `--include-subdomains` | サブドメインを含める |
| `--ignore-query-parameters` | クエリパラメータが異なる URL を同一として扱う |
| `--wait` | マップ処理の完了を待機 |
| `--timeout <seconds>` | タイムアウト時間（秒） |
| `--json` | JSON 形式で出力 |
| `--output <path>` | 出力をファイルに保存 |
| `--pretty` | JSON 出力を整形して表示 |

---

<div id="crawl">
  ### クロール
</div>

指定した URL を起点に、ウェブサイト全体をクロールします。

<CrawlBasic />

<div id="check-crawl-status">
  #### クロールのステータスを確認する
</div>

<CrawlStatus />

<div id="crawl-options">
  #### クロールオプション
</div>

<CrawlOptions />

**利用可能なオプション:**

| Option | Description |
|--------|-------------|
| `--url <url>` | クロールするURL（位置引数の代わり） |
| `--wait` | クロールの完了を待機 |
| `--progress` | 待機中に進行状況インジケーターを表示 |
| `--poll-interval <seconds>` | ポーリング間隔（デフォルト: 5） |
| `--timeout <seconds>` | 待機時のタイムアウト時間 |
| `--status` | 既存のクロールジョブのステータスを確認 |
| `--limit <number>` | クロールする最大ページ数 |
| `--max-depth <number>` | クロールの最大深さ |
| `--include-paths <paths>` | 含めるパス（カンマ区切り） |
| `--exclude-paths <paths>` | 除外するパス（カンマ区切り） |
| `--sitemap <mode>` | サイトマップの処理モード: `include`、`skip`、`only` |
| `--allow-subdomains` | サブドメインも対象に含める |
| `--allow-external-links` | 外部リンクをたどる |
| `--crawl-entire-domain` | ドメイン全体をクロール |
| `--ignore-query-parameters` | クエリパラメーターが異なるURLを同一として扱う |
| `--delay <ms>` | リクエスト間の遅延時間 |
| `--max-concurrency <n>` | 最大同時リクエスト数 |
| `--output <path>` | 出力をファイルに保存 |
| `--pretty` | JSON出力を整形して表示 |

---

<div id="agent">
  ### エージェント
</div>

自然言語プロンプトを使用して、Web上からデータを検索・収集します。

<AgentBasic />

<div id="agent-options">
  #### エージェントオプション
</div>

<AgentOptions />

**利用可能なオプション:**

| Option | Description |
|--------|-------------|
| `--urls <urls>` | エージェントが対象とするURLの任意のリスト（カンマ区切り） |
| `--model <model>` | 使用するモデル: `spark-1-mini`（デフォルト、60%安価）または `spark-1-pro`（高精度） |
| `--schema <json>` | 構造化出力用のJSONスキーマ（インラインJSON文字列） |
| `--schema-file <path>` | 構造化出力用のJSONスキーマファイルへのパス |
| `--max-credits <number>` | 消費するクレジットの上限（上限に達するとジョブは失敗） |
| `--status` | 既存のエージェントジョブのステータスを確認 |
| `--wait` | 結果を返す前にエージェントの完了を待つ |
| `--poll-interval <seconds>` | 待機中のポーリング間隔（デフォルト: 5） |
| `--timeout <seconds>` | 待機時のタイムアウト（デフォルト: タイムアウトなし） |
| `--output <path>` | 出力をファイルに保存 |
| `--json` | JSON形式で出力 |

---

<div id="browser">
  ### Browser
</div>

クラウド上のブラウザーセッションを起動し、Python、JavaScript、または bash のコードをリモートで実行します。各セッションでは完全な Chromium インスタンスが動作しており、ローカルにブラウザーをインストールする必要はありません。コードはサーバー側で実行され、あらかじめ設定された [Playwright](https://playwright.dev/) の `page` オブジェクトをすぐに利用できます。

<BrowserBasic />

<div id="browser-options">
  #### ブラウザオプション
</div>

<BrowserOptions />

**サブコマンド:**

| Subcommand | Description |
|------------|-------------|
| `launch-session` | 新しいクラウドブラウザセッションを起動します（セッション ID、CDP URL、ライブビュー URL を返します） |
| `execute <code>` | セッション内で Playwright の Python/JS コードまたは bash コマンドを実行します |
| `list [status]` | ブラウザセッションを一覧表示します（`active` または `destroyed` でフィルタ可能） |
| `close` | ブラウザセッションを閉じます |

**Execute オプション:**

| Option | Description |
|--------|-------------|
| `--bash` | サンドボックス内で bash コマンドをリモート実行します（デフォルト）。[agent-browser](https://github.com/vercel-labs/agent-browser)（40 以上のコマンド）がプリインストールされており、コマンドに自動でプレフィックスされます。`CDP_URL` は自動で設定されるため、agent-browser はセッションに自動的に接続します。AI エージェント向けの推奨手段です。 |
| `--python` | Playwright の Python コードとして実行します。Playwright の `page` オブジェクトが利用可能で、`await page.goto()`, `await page.title()` などを使用できます。 |
| `--node` | Playwright の JavaScript コードとして実行します。同じ `page` オブジェクトが利用可能です。 |
| `--session <id>` | 対象とする特定のセッションを指定します（デフォルト: アクティブなセッション） |

**Launch オプション:**

| Option | Description |
|--------|-------------|
| `--ttl <seconds>` | セッション全体の TTL（デフォルト: 300、範囲: 30–3600） |
| `--ttl-inactivity <seconds>` | 一定時間操作がない場合に自動終了します（範囲: 10–3600） |
| `--stream` | ライブビューのストリーミングを有効化します |

**共通オプション:**

| Option | Description |
|--------|-------------|
| `--output <path>` | 出力をファイルに保存します |
| `--json` | JSON 形式で出力します |

---

<div id="credit-usage">
  ### クレジット使用状況
</div>

チームのクレジット残高と利用状況を確認できます。

```bash CLI
# クレジット使用量を確認
firecrawl credit-usage

# JSON形式で出力
firecrawl credit-usage --json --pretty
```

***


<div id="version">
  ### Version
</div>

CLIのバージョンを表示します。

```bash CLI
firecrawl version
# または
firecrawl --version
```


<div id="global-options">
  ## グローバルオプション
</div>

これらのオプションはすべてのコマンドで利用できます。

| オプション | 短縮形 | 説明 |
|--------|-------|-------------|
| `--status` | | バージョン、認証情報、同時実行数、クレジット残高を表示する |
| `--api-key <key>` | `-k` | このコマンドで使用する API キーを、保存されているキーより優先して指定する |
| `--api-url <url>` | | カスタム API URL を使用する（セルフホスト環境／ローカル開発向け） |
| `--help` | `-h` | コマンドのヘルプを表示する |
| `--version` | `-V` | CLI のバージョン情報を表示する |

<div id="output-handling">
  ## 出力の処理
</div>

CLI はデフォルトで標準出力 (stdout) に出力するため、パイプやリダイレクトが容易です。

```bash CLI
# マークダウンを別のコマンドにパイプする
firecrawl https://example.com | head -50

# ファイルにリダイレクトする
firecrawl https://example.com > output.md

# 整形されたJSON形式で保存する
firecrawl https://example.com --format markdown,links --pretty -o data.json
```


<div id="format-behavior">
  ### フォーマットの挙動
</div>

* **単一フォーマット**: 生のコンテンツを出力します（markdown テキスト、HTML など）
* **複数フォーマット**: 要求されたすべてのデータを含む JSON を出力します

```bash CLI
# Raw markdown output
firecrawl https://example.com --format markdown

# 複数フォーマットを使用したJSON出力
firecrawl https://example.com --format markdown,links
```


<div id="examples">
  ## 使用例
</div>

<div id="quick-scrape">
  ### クイックスクレイプ
</div>

```bash CLI
# URLからMarkdownコンテンツを取得(クリーンな出力には --only-main-content を使用)
firecrawl https://docs.firecrawl.dev --only-main-content

# Get HTML content
firecrawl https://example.com --html -o page.html
```


<div id="full-site-crawl">
  ### サイト全体クロール
</div>

```bash CLI
# 制限付きでドキュメントサイトをクロールする
firecrawl crawl https://docs.example.com --limit 50 --max-depth 2 --wait --progress -o docs.json
```


<div id="site-discovery">
  ### サイトの発見
</div>

```bash CLI
# すべてのブログ投稿を検索
firecrawl map https://example.com --search "blog" -o blog-urls.txt
```


<div id="research-workflow">
  ### 調査ワークフロー
</div>

```bash CLI
# リサーチ用の検索とスクレイピング結果
firecrawl search "machine learning best practices 2024" --scrape --scrape-formats markdown --pretty
```


<div id="agent">
  ### エージェント
</div>

```bash CLI
# URLs are optional
firecrawl agent "Find the top 5 AI startups and their funding amounts" --wait

# 特定のURLを対象にする
firecrawl agent "Compare pricing plans" --urls https://slack.com/pricing,https://teams.microsoft.com/pricing --wait
```


<div id="browser-automation">
  ### ブラウザ自動化
</div>

```bash CLI
# Launch a session, scrape a page, and close
firecrawl browser launch-session
firecrawl browser execute "open https://news.ycombinator.com"
firecrawl browser execute "snapshot"
firecrawl browser execute "scrape"
firecrawl browser close

# bashモード経由でagent-browserを使用(デフォルト — AIエージェント向けに推奨)
firecrawl browser launch-session
firecrawl browser execute "open https://example.com"
firecrawl browser execute "snapshot"
# snapshot returns @ref IDs — use them to interact
firecrawl browser execute "click @e5"
firecrawl browser execute "fill @e3 'search query'"
firecrawl browser execute "scrape"
# Run --help to see all 40+ commands
firecrawl browser execute --bash "agent-browser --help"
firecrawl browser close
```


<div id="combine-with-other-tools">
  ### 他のツールとの連携
</div>

```bash CLI
# Extract URLs from search results
jq -r '.data.web[].url' search-results.json

# Get titles from search results
jq -r '.data.web[] | "\(.title): \(.url)"' search-results.json

# リンクを抽出してjqで処理
firecrawl https://example.com --format links | jq '.links[].url'

# Count URLs from map
firecrawl map https://example.com | wc -l
```


<div id="telemetry">
  ## テレメトリー
</div>

CLI は、製品の改善のために認証時に匿名の利用状況データを収集します：

* CLI バージョン、OS、Node.js バージョン
* 開発ツールの検出（例：Cursor、VS Code、Claude Code）

**CLI を通じてコマンド内容、URL、ファイル内容が収集されることは一切ありません。**

テレメトリーを無効にするには、次の環境変数を設定します：

```bash CLI
export FIRECRAWL_NO_TELEMETRY=1
```


<div id="open-source">
  ## オープンソース
</div>

Firecrawl CLI と Skill はオープンソースとして GitHub で公開されています: [firecrawl/cli](https://github.com/firecrawl/cli)