---
title: V1へようこそ
description: "Firecrawlは、ウェブサイト全体をLLM対応のMarkdownに変換できます"
og:title: "V1へようこそ | Firecrawl"
og:description: "Firecrawlは、ウェブサイト全体をLLM対応のMarkdownに変換できます"
---

import InstallationPython from "/snippets/ja/v1/installation/python.mdx";
import InstallationNode from "/snippets/ja/v1/installation/js.mdx";
import InstallationGo from "/snippets/ja/v1/installation/go.mdx";
import InstallationRust from "/snippets/ja/v1/installation/rust.mdx";
import ScrapePython from "/snippets/ja/v1/scrape/base/python.mdx";
import ScrapeNode from "/snippets/ja/v1/scrape/base/js.mdx";
import ScrapeGo from "/snippets/ja/v1/scrape/base/go.mdx";
import ScrapeRust from "/snippets/ja/v1/scrape/base/rust.mdx";
import ScrapeCURL from "/snippets/ja/v1/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/ja/v1/scrape/base/output.mdx";
import MapPython from "/snippets/ja/v1/map/base/python.mdx";
import MapJavaScript from "/snippets/ja/v1/map/base/js.mdx";
import MapGo from "/snippets/ja/v1/map/base/go.mdx";
import MapRust from "/snippets/ja/v1/map/base/rust.mdx";
import MapCURL from "/snippets/ja/v1/map/base/curl.mdx";
import MapResponse from "/snippets/ja/v1/map/base/output.mdx";
import CrawlWebSocketPythonBase from "/snippets/ja/v1/crawl-websocket/base/python.mdx";
import CrawlWebSocketNodeBase from "/snippets/ja/v1/crawl-websocket/base/js.mdx";
import ExtractCURL from "/snippets/ja/v1/llm-extract/base/curl.mdx";
import ExtractPython from "/snippets/ja/v1/llm-extract/base/python.mdx";
import ExtractNode from "/snippets/ja/v1/llm-extract/base/js.mdx";
import ExtractOutput from "/snippets/ja/v1/llm-extract/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/ja/v1/llm-extract/no-schema/curl.mdx";
import ExtractNoSchemaOutput from "/snippets/ja/v1/llm-extract/no-schema/output.mdx";
import CrawlWebhookCURL from "/snippets/ja/v1/crawl-webhook/base/curl.mdx";

Firecrawl V1 が登場しました！これにより、より信頼性が高く、開発者に優しい API を提供します。

新機能は次のとおりです:

* `/scrape` の出力フォーマット。出力したいフォーマットを選べます。
* ウェブページに関連する大半の URL を取得できる新しい [`/map` エンドポイント](/ja/features/map)。
* `/crawl/{id}` のステータス取得用、開発者に優しい API。
* すべてのプランでレート制限を2倍に増加。
* [Go SDK](/ja/sdks/go) と [Rust SDK](/ja/sdks/rust)
* Teams サポート
* ダッシュボードでの API キー管理。
* `onlyMainContent` はデフォルトで `true` になりました。
* `/crawl` のウェブフックおよび WebSocket サポート。

<div id="scrape-formats">
  ## スクレイプのフォーマット
</div>

出力のフォーマットを選択できます。複数の出力フォーマットを指定可能です。サポートされているフォーマットは次のとおりです:

* Markdown（markdown）
* HTML（html）
* 生のHTML（rawHtml）（変更なし）
* スクリーンショット（screenshot または screenshot@fullPage）
* リンク（links）
* JSON（json）- 構造化出力

出力キーは、選択したフォーマットに対応します。

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeGo />

  <ScrapeRust />

  <ScrapeCURL />
</CodeGroup>

<div id="response">
  ### レスポンス
</div>

SDKはデータオブジェクトを直接返します。cURLは下記のとおり、ペイロードをそのまま返します。

<ScrapeResponse />

<div id="introducing-map-alpha">
  ## /map（アルファ）の紹介
</div>

単一のURLからサイト全体のマップを最短ルートで作成するいちばん簡単な方法です。

<div id="usage">
  ### 使い方
</div>

<CodeGroup>
  <MapPython />

  <MapJavaScript />

  <MapGo />

  <MapRust />

  <MapCURL />
</CodeGroup>

<div id="response">
  ### レスポンス
</div>

SDKはデータオブジェクトをそのまま返します。cURLは下記のとおり、ペイロードをそのまま返します。

<MapResponse />

<div id="websockets">
  ## WebSockets
</div>

WebSockets を使ってウェブサイトをクロールするには、`Crawl URL and Watch` メソッドを利用します。

<CodeGroup>
  <CrawlWebSocketPythonBase />

  <CrawlWebSocketNodeBase />
</CodeGroup>

<div id="json-format">
  ## JSON フォーマット
</div>

LLM による抽出は、現在 v1 で `json` フォーマットとして利用できます。ページから構造化データを抽出するには、エンドポイントにスキーマを渡すか、プロンプトを指定するだけでも実行できます。

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

出力:

<ExtractOutput />

<div id="extracting-without-schema-new">
  ### スキーマなしでの抽出（新機能）
</div>

エンドポイントに `prompt` を渡すだけで、スキーマを指定せずに抽出できます。データの構造は LLM が決定します。

<CodeGroup>
  <ExtractNoSchemaCURL />
</CodeGroup>

出力:

<ExtractNoSchemaOutput />

<div id="new-crawl-webhook">
  ## 新しいクロール用Webhook
</div>

`/crawl` エンドポイントに `webhook` パラメータを指定できるようになりました。これにより、クロールの開始・更新・完了時に、指定した URL へ POST リクエストが送信されます。

Webhook は、最後の全体結果だけでなく、クロールされた各ページごとにもトリガーされるようになりました。

<CrawlWebhookCURL />

<div id="webhook-events">
  ### Webhook イベント
</div>

現在、イベントは4種類あります:

* `crawl.started` - クロールの開始時にトリガーされます。
* `crawl.page` - クロールした各ページごとにトリガーされます。
* `crawl.completed` - クロールの完了時にトリガーされ、処理が終了したことを知らせます。
* `crawl.failed` - クロールが失敗したときにトリガーされます。

<div id="webhook-response">
  ### Webhook レスポンス
</div>

* `success` - Webhook がページのクロールを正しく実行できたかどうか。
* `type` - 発生したイベントの種類。
* `id` - クロールの ID。
* `data` - スクレイプされたデータ（配列）。`crawl.page` の場合にのみ空ではなくなり、ページのスクレイプが成功したときはアイテムが 1 件含まれます。レスポンスは `/scrape` エンドポイントと同じです。
* `error` - Webhook が失敗した場合、ここにエラーメッセージが入ります。

<div id="migrating-from-v0">
  ## V0 からの移行
</div>

> ⚠️ **非推奨化のお知らせ**: V0 エンドポイントは 2025年4月1日に非推奨となります。それまでに V1 エンドポイントへ移行し、サービス中断を防いでください。

<div id="scrape-endpoint">
  ## /scrape エンドポイント
</div>

更新された `/scrape` エンドポイントは、信頼性と使いやすさを高めるために再設計されました。新しい `/scrape` のリクエストボディの構造は次のとおりです。

```json
{
  "url": "<string>",
  "formats": ["markdown", "html", "rawHtml", "links", "screenshot", "json"],
  "includeTags": ["<string>"],
  "excludeTags": ["<string>"],
  "headers": { "<key>": "<value>" }
  "waitFor": 123,
  "timeout": 123
}
```

<div id="formats">
  ### フォーマット
</div>

出力のフォーマットを選択できます。複数の出力フォーマットを指定可能です。サポートされているフォーマットは次のとおりです：

* Markdown（markdown）
* HTML（html）
* Raw HTML（rawHtml）（無加工）
* Screenshot（screenshot または screenshot@fullPage）
* Links（links）
* JSON（json）

デフォルトでは、出力は markdown フォーマットのみです。

<div id="details-on-the-new-request-body">
  ### 新しいリクエストボディの詳細
</div>

以下の表は、V1 の `/scrape` エンドポイントにおけるリクエストボディのパラメータ変更点を示します。

| Parameter | Change | Description |
| --------- | ------ | ----------- |
| `onlyIncludeTags` | 移動および名称変更 | ルートレベルに移動し、`includeTags` に名称変更。 |
| `removeTags` | 移動および名称変更 | ルートレベルに移動し、`excludeTags` に名称変更。 |
| `onlyMainContent`| 移動 | ルートレベルに移動。デフォルトは `true`。 |
| `waitFor`| 移動 | ルートレベルに移動。 |
| `headers`| 移動 | ルートレベルに移動。 |
| `parsePDF`| 移動 | ルートレベルに移動。 |
| `extractorOptions`| 変更なし ||
| `timeout`| 変更なし ||
| `pageOptions` | 削除 | `pageOptions` パラメータは不要。スクレイプのオプションはルートレベルに移動。 |
| `replaceAllPathsWithAbsolutePaths` | 削除 | `replaceAllPathsWithAbsolutePaths` は不要。すべてのパスはデフォルトで絶対パス。 |
| `includeHtml`| 削除 | 代わりに `formats` に `"html"` を追加。 |
| `includeRawHtml`| 削除 | 代わりに `formats` に `"rawHtml"` を追加。 |
| `screenshot`| 削除 | 代わりに `formats` に `"screenshot"` を追加。 |
| `fullPageScreenshot`| 削除 | 代わりに `formats` に `"screenshot@fullPage"` を追加。 |
| `extractorOptions` | 削除 | 代わりに `"json"` フォーマットを使用し、`jsonOptions` オブジェクトを指定。 |

新しい `json` フォーマットの詳細は [llm-extract](/ja/features/extract) セクションを参照してください。

<div id="crawl-endpoint">
  ## /crawl エンドポイント
</div>

`v1` の `/crawl` エンドポイントも更新しました。以下の改善版のリクエストボディをご確認ください:

```json
{
  "url": "<string>",
  "excludePaths": ["<string>"],
  "includePaths": ["<string>"],
  "maxDepth": 2,
  "ignoreSitemap": true,
  "limit": 10,
  "allowBackwardLinks": true,
  "allowExternalLinks": true,
  "scrapeOptions": {
    // /scrape と同一のオプション
    "formats": ["markdown", "html", "rawHtml", "screenshot", "links"],
    "headers": { "<key>": "<value>" },
    "includeTags": ["<string>"],
    "excludeTags": ["<string>"],
    "onlyMainContent": true,
    "waitFor": 123
  }
}
```

<div id="details-on-the-new-request-body">
  ### 新しいリクエストボディの詳細
</div>

以下の表は、V1 の `/crawl` エンドポイントにおけるリクエストボディのパラメータ変更点を示します。

| Parameter | 変更 | 説明 |
| --------- | ------ | ----------- |
| `pageOptions` | 名前変更 | `scrapeOptions` に名称変更。 |
| `includes` | 移動と名前変更 | ルートレベルへ移動し、`includePaths` に名称変更。 |
| `excludes` | 移動と名前変更 | ルートレベルへ移動し、`excludePaths` に名称変更。 |
| `allowBackwardCrawling` | 移動と名前変更 | ルートレベルへ移動し、`allowBackwardLinks` に名称変更。 |
| `allowExternalLinks` | 移動 | ルートレベルへ移動。 |
| `maxDepth` | 移動 | ルートレベルへ移動。 |
| `ignoreSitemap` | 移動 | ルートレベルへ移動。 |
| `limit` | 移動 | ルートレベルへ移動。 |
| `crawlerOptions` | 削除 | `crawlerOptions` パラメータは不要になりました。クロールのオプションはルートレベルへ移動しました。 |
| `timeout` | 削除 | 代わりに `scrapeOptions` の `timeout` を使用してください。 |