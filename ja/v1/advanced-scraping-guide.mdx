---
title: "高度なスクレイピングガイド"
description: "高度なオプションで Firecrawl のスクレイピングを強化する方法を学びましょう。"
og:title: "高度なスクレイピングガイド | Firecrawl"
og:description: "高度なオプションで Firecrawl のスクレイピングを強化する方法を学びましょう。"
---

このガイドでは、Firecrawl の各エンドポイントと、そのパラメータを最大限に活用する方法を解説します。

<div id="basic-scraping-with-firecrawl-scrape">
  ## Firecrawl（/scrape）による基本的なスクレイピング
</div>

単一ページをスクレイピングしてクリーンなMarkdownコンテンツを取得するには、`/scrape` エンドポイントを使用します。

<CodeGroup>

```python Python
# pip install firecrawl-py

from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR_API_KEY")

content = app.scrape_url("https://docs.firecrawl.dev")
```

```JavaScript JavaScript
// npm install @mendable/firecrawl-js

import { FirecrawlApp } from 'firecrawl-js';

const app = new FirecrawlApp({ apiKey: 'YOUR_API_KEY' });

const content = await app.scrapeUrl('https://docs.firecrawl.dev');
```

```go Go
// go get github.com/mendableai/firecrawl-go

import (
  "fmt"
  "log"

  "github.com/mendableai/firecrawl-go"
)

func main() {
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
    log.Fatalf("Failed to initialize FirecrawlApp: %v", err)
  }

  content, err := app.ScrapeURL("docs.firecrawl.dev", nil)
  if err != nil {
    log.Fatalf("Failed)
  }
}
```

```rust Rust
// Install the firecrawl_rs crate with Cargo

use firecrawl_rs::FirecrawlApp;
#[tokio::main]
async fn main() {
  // Initialize the FirecrawlApp with the API key
  let api_key = "YOUR_API_KEY";
  let api_url = "https://api.firecrawl.dev";
  let app = FirecrawlApp::new(api_key, api_url).expect("Failed to initialize FirecrawlApp");

  let scrape_result = app.scrape_url("https://docs.firecrawl.dev", None).await;
  match scrape_result {
    Ok(data) => println!("Scrape Result:\n{}", data["markdown"]),
    Err(e) => eprintln!("Scrape failed: {}", e),
  }
}
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

</CodeGroup>

<div id="scraping-pdfs">
  ## PDFのスクレイピング
</div>

**Firecrawlは標準でPDFのスクレイピングに対応しています。** `/scrape` エンドポイントを使ってPDFのリンクを指定すると、PDFのテキスト内容を取得できます。無効化するには、`parsePDF` を `false` に設定してください。

<div id="scrape-options">
  ## スクレイプのオプション
</div>

`/scrape` エンドポイントでは、多数のパラメータでスクレイプの動作をカスタマイズできます。利用可能なオプションは次のとおりです:

<div id="setting-the-content-formats-on-response-with-formats">
  ### レスポンスのコンテンツフォーマットを `formats` で設定する
</div>

- **Type**: `array`
- **Enum**: `["markdown", "links", "html", "rawHtml", "screenshot", "json"]`
- **Description**: レスポンスに含めるフォーマットを指定します。選択肢は次のとおりです：
  - `markdown`: 取得したコンテンツをMarkdown形式で返します。
  - `links`: ページ上で見つかったすべてのハイパーリンクを含めます。
  - `html`: コンテンツをHTML形式で返します。
  - `rawHtml`: 処理を行っていない生のHTMLを返します。
  - `screenshot`: ブラウザ表示そのままのページのスクリーンショットを含めます。
  - `json`: LLMを使用してページから構造化データを抽出します。
- **Default**: `["markdown"]`

<div id="getting-the-full-page-content-as-markdown-with-onlymaincontent">
  ### `onlyMainContent` を使ってページ全体のコンテンツを Markdown で取得する
</div>

- **Type**: `boolean`
- **Description**: 既定では、スクレイパーはヘッダー、ナビゲーションバー、フッターなどを除いたページの主要コンテンツのみを返します。ページ全体のコンテンツを返すには、これを `false` に設定します。
- **Default**: `true`

<div id="setting-the-tags-to-include-with-includetags">
  ### `includeTags` で含めるタグを設定する
</div>

- **Type**: `array`
- **Description**: レスポンスに含める HTML のタグ、クラス、ID を指定します。
- **Default**: undefined

<div id="setting-the-tags-to-exclude-with-excludetags">
  ### `excludeTags` で除外するタグを設定する
</div>

- **Type**: `array`
- **Description**: レスポンスから除外する HTML のタグ、クラス、ID を指定します。
- **Default**: 未設定

<div id="waiting-for-the-page-to-load-with-waitfor">
  ### `waitFor` でページ読み込みを待つ
</div>

- **タイプ**: `integer`
- **説明**: 最終手段としてのみ使用してください。コンテンツを取得する前に、指定したミリ秒数だけページの読み込みを待機します。
- **デフォルト**: `0`

<div id="setting-the-maximum-timeout">
  ### 最大の`timeout`を設定
</div>

- **型**: `integer`
- **説明**: 操作を中止する前に、スクレイパーがページの応答を待つ最大時間（ミリ秒）を設定します。
- **デフォルト**: `30000`（30秒）

<div id="parsing-pdf-files-with-parsepdf">
  ### `parsePDF` を使った PDF ファイルの解析
</div>

- **Type**: `boolean`
- **Description**: スクレイピング時の PDF ファイルの処理方法を制御します。`true` の場合、PDF の内容を抽出して Markdown フォーマットに変換し、課金はページ数に基づきます（1ページあたり1クレジット）。`false` の場合、PDF ファイルを base64 エンコードで返し、合計で1クレジットの定額です。
- **Default**: `true`

### 使用例

```bash
curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "formats": ["markdown", "links", "html", "rawHtml", "screenshot"],
      "includeTags": ["h1", "p", "a", ".main-content"],
      "excludeTags": ["#ad", "#footer"],
      "onlyMainContent": false,
      "waitFor": 1000,
      "timeout": 15000,
      "parsePDF": false
    }'
```

この例では、スクレイパーは次の処理を行います:

* ページ全体のコンテンツをMarkdownで返します。
* レスポンスにMarkdown、raw HTML、HTML、links、screenshotを含めます。
* レスポンスには、HTMLタグの `<h1>`、`<p>`、`<a>` と、クラス `.main-content` の要素のみを含め、ID `#ad` と `#footer` の要素は除外します。
* コンテンツ取得前に、ページの読み込み完了を1000ミリ秒（1秒）待ちます。
* スクレイプリクエストの最大所要時間を15000ミリ秒（15秒）に設定します。
* PDFファイルはMarkdownへ変換せず、base64形式で返します。

こちらがAPIリファレンスです: [Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)


<div id="extractor-options">
  ## 抽出オプション
</div>

`/scrape` エンドポイントを使用する際は、ページコンテンツから**構造化データを抽出**するためのオプションを `extract` パラメータで指定できます。利用可能なオプションは次のとおりです。

<div id="using-the-llm-extraction">
  ### LLM抽出の利用
</div>

<div id="schema">
  ### schema
</div>

- **Type**: `object`
- **Required**: プロンプトが指定されている場合は False
- **Description**: 抽出対象データのスキーマ。抽出結果の構造を定義します。

<div id="system-prompt">
  ### system prompt
</div>

- **Type**: `string`
- **Required**: False
- **Description**: LLM用のシステムプロンプト。

<div id="prompt">
  ### prompt
</div>

- **Type**: `string`
- **Required**: スキーマが指定されている場合は不要
- **Description**: LLMが所定の構造でデータを抽出できるよう指示するプロンプト。
- **Example**: `"製品の特徴を抽出してください"`

<div id="example-usage">
  ### 使い方の例
</div>

```bash
curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://firecrawl.dev",
      "formats": ["markdown", "json"],
      "json": {
        "prompt": "製品の機能を抽出してください"
      }
    }'
```

```json
{
  "success": true,
  "data": {
    "content": "Raw Content",
    "metadata": {
      "title": "Mendable",
      "description": "Mendable を使えば、AI チャットアプリを簡単に構築できます。取り込んで、カスタマイズして、あとはコード1行で好きな場所にデプロイできます。SideGuide 提供",
      "robots": "follow, index",
      "ogTitle": "Mendable",
      "ogDescription": "Mendable を使えば、AI チャットアプリを簡単に構築できます。取り込んで、カスタマイズして、あとはコード1行で好きな場所にデプロイできます。SideGuide 提供",
      "ogUrl": "https://docs.firecrawl.dev/",
      "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",
      "ogLocaleAlternate": [],
      "ogSiteName": "Mendable",
      "sourceURL": "https://docs.firecrawl.dev/",
      "statusCode": 200
    },
    "extract": {
      "product": "Firecrawl",
      "features": {
        "general": {
          "description": "Webサイトを LLM 向けデータに変換します。",
          "openSource": true,
          "freeCredits": 500,
          "useCases": [
            "AI applications",
            "データサイエンス",
            "市場調査",
            "コンテンツ集約",
          ]
        },
        "crawlingAndScraping": {
          "crawlAllAccessiblePages": true,
          "noSitemapRequired": true,
          "dynamicContentHandling": true,
          "dataCleanliness": {
            "process": "高度なアルゴリズム",
            "outputFormat": "Markdown"
          }
        },
        ...
      }
    }
  }
}
```


<div id="actions">
  ## アクション
</div>

`/scrape` エンドポイントでは、スクレイピング前に Web ページ上でさまざまなアクションを実行できます。これは、動的コンテンツとのインタラクション、ページ遷移の操作、ユーザー操作が必要なコンテンツへのアクセスに特に有効です。

<div id="available-actions">
  ### 利用できるアクション
</div>

<div id="wait">
  #### wait
</div>

- **Type**: `object`
- **Description**: 指定したミリ秒間待機します。
- **Properties**:
  - `type`: `"wait"`
  - `milliseconds`: 待機するミリ秒数。
- **Example**:
  ```json
  {
    "type": "wait",
    "milliseconds": 2000
  }
  ```

<div id="screenshot">
  #### screenshot
</div>

- **Type**: `object`
- **Description**: スクリーンショットを撮ります。
- **Properties**:
  - `type`: `"screenshot"`
  - `fullPage`: スクリーンショットをページ全体にするか、ビューポートサイズにするかを指定します（既定: `false`）
- **Example**:
  ```json
  {
    "type": "screenshot",
    "fullPage": true
  }
  ```

<div id="click">
  #### click
</div>

- **Type**: `object`
- **Description**: 要素をクリックします。
- **Properties**:
  - `type`: `"click"`
  - `selector`: 要素を特定するためのクエリセレクター。
- **Example**:
  ```json
  {
    "type": "click",
    "selector": "#load-more-button"
  }
  ```

<div id="write">
  #### write
</div>

- **Type**: `object`
- **Description**: 入力フィールドにテキストを入力します。
- **Properties**:
  - `type`: `"write"`
  - `text`: 入力するテキスト
  - `selector`: 入力フィールドのクエリセレクター
- **Example**:
  ```json
  {
    "type": "write",
    "text": "Hello, world!",
    "selector": "#search-input"
  }
  ```

<div id="press">
  #### press
</div>

- **タイプ**: `object`
- **説明**: ページ上でキーを押す。
- **プロパティ**:
  - `type`: `"press"`
  - `key`: 押すキー。
- **例**:
  ```json
  {
    "type": "press",
    "key": "Enter"
  }
  ```

<div id="scroll">
  #### scroll
</div>

- **タイプ**: `object`
- **説明**: ページをスクロールします。
- **プロパティ**:
  - `type`: `"scroll"`
  - `direction`: スクロール方向（`"up"` または `"down"`）。
  - `amount`: スクロール量（ピクセル単位）。
- **例**:
  ```json
  {
    "type": "scroll",
    "direction": "down",
    "amount": 500
  }
  ```

<div id="scrape">
  #### scrape
</div>

- **タイプ**: `object`
- **説明**: 現在のページのコンテンツをスクレイプし、URL と HTML を返します。スクレイプしたコンテンツは、レスポンスの `actions.scrapes` 配列に含まれます。
- **プロパティ**:
  - `type`: `"scrape"`
- **例**:
  ```json
  {
    "type": "scrape"
  }
  ```

<div id="pdf">
  #### pdf
</div>

- **タイプ**: `object`
- **説明**: 現在のページのPDFを生成します。PDFはレスポンスの `actions.pdfs` 配列で返されます。
- **プロパティ**:
  - `type`: `"pdf"`
  - `format`: 生成されるPDFの用紙サイズ（既定: `"Letter"`）
  - `landscape`: PDFを横向きで生成するかどうか（既定: `false`）
  - `scale`: 生成されるPDFのスケール倍率（既定: `1`）
- **例**:
  ```json
  {
    "type": "pdf",
    "format": "A4",
    "landscape": true,
    "scale": 0.8
  }
  ```

<div id="executejavascript">
  #### executeJavascript
</div>

- **タイプ**: `object`
- **説明**: ページ上で JavaScript コードを実行します。戻り値はレスポンスの `actions.javascriptReturns` 配列に格納されます。
- **プロパティ**:
  - `type`: `"executeJavascript"`
  - `script`: 実行する JavaScript コード。
- **例**:
  ```json
  {
    "type": "executeJavascript",
    "script": "document.querySelector('.button').click();"
  }
  ```

アクションのパラメータの詳細は、[API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape)を参照してください。

<div id="crawling-multiple-pages">
  ## 複数ページのクロール
</div>

複数ページをクロールするには、`/crawl` エンドポイントを使用します。このエンドポイントでは、基点となるURLを指定すると、アクセス可能なサブページがすべてクロールされます。

```bash
curl -X POST https://api.firecrawl.dev/v1/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

IDを返します

```json
{ "id": "1234-5678-9101" }
```


<div id="check-crawl-job">
  ### クロールジョブの確認
</div>

クロールジョブのステータスを確認し、結果を取得するために使用します。

```bash
curl -X GET https://api.firecrawl.dev/v1/crawl/1234-5678-9101 \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_API_KEY'
```


<div id="paginationnext-url">
  #### ページネーション/次のURL
</div>

コンテンツが10MBを超える場合、またはクロールジョブがまだ実行中の場合、レスポンスには `next` パラメータが含まれます。これは次の結果ページへのURLで、このパラメータを使って次ページの結果を取得できます。

<div id="crawler-options">
  ### クローラーのオプション
</div>

`/crawl` エンドポイントを使用する際は、リクエストボディのパラメータでクロールの挙動をカスタマイズできます。利用可能なオプションは次のとおりです:

<div id="includepaths">
  #### `includePaths`
</div>

- **Type**: `array`
- **Description**: クロール対象に含める正規表現パターン。これらのパターンに一致するURLのみクロールされます。たとえば、`^/blog/.*` は `/blog/` で始まるあらゆるURLに一致します。
- **Example**: `["^/blog/.*$", "^/docs/.*$"]`

<div id="excludepaths">
  #### `excludePaths`
</div>

- **Type**: `array`
- **Description**: クロール対象から除外するための正規表現パターン。これらのパターンに一致するURLはスキップされます。例: `^/admin/.*` は `/admin/` で始まるURLを除外します。
- **Example**: `["^/admin/.*$", "^/private/.*$"]`

<div id="maxdepth">
  #### `maxDepth`
</div>

- **Type**: `integer`
- **Description**: 入力したURLの基点からクロールする最大の絶対深さ。たとえば、入力URLのパスが `/features/feature-1` の場合、`maxDepth` が少なくとも 2 でなければ結果は返されません。
- **Example**: `2`

<div id="limit">
  #### `limit`
</div>

- **型**: `integer`
- **説明**: クロールするページ数の上限。
- **デフォルト**: `10000`

<div id="allowbackwardlinks">
  #### `allowBackwardLinks`
</div>

- **Type**: `boolean`
- **Description**: クローラーが子パスだけでなく、同階層や親の内部リンクもたどれるようにします。
  - **false**: より深い（子）URL のみをクロールします。
    - 例: /features/feature-1 → /features/feature-1/tips ✅
    - /pricing や / には進みません ❌
  - **true**: 兄弟および親を含むあらゆる内部リンクをクロールします。
    - 例: /features/feature-1 → /pricing、/ など ✅
  - ネストされたパスに限らず、より広く内部を網羅したい場合は true を使用します。
- **Default**: `false`

<div id="allowexternallinks">
  ### `allowExternalLinks`
</div>

- **Type**: `boolean`
- **Description**: クローラーが外部ドメインへのリンクをたどれるようにするオプションです。有効化すると、クロールの停止条件が `limit` と `maxDepth` の値のみに依存する場合があるため注意してください。
- **Default**: `false`

<div id="allowsubdomains">
  ### `allowSubdomains`
</div>

- **Type**: `boolean`
- **Description**: クローラーがメインドメインのサブドメインへのリンクを追跡することを許可します。例：`example.com` をクロールしている場合、`blog.example.com` や `api.example.com` へのリンクもたどれます。
- **Default**: `false`

<div id="delay">
  ### `delay`
</div>

- **Type**: `number`
- **Description**: スクレイピング間の待機時間（秒）。ウェブサイトのレート制限を尊重し、対象サイトへの過負荷を防ぐために有効です。未指定の場合、利用可能であれば robots.txt の crawl-delay が使用されることがあります。
- **Default**: `undefined`

<div id="scrapeoptions">
  #### scrapeOptions
</div>

クローラーのオプションの一部として、`scrapeOptions` パラメータを指定できます。各ページごとのスクレイピング動作をカスタマイズできます。

- **Type**: `object`
- **Description**: スクレイパーのオプション。
- **Example**: `{"formats": ["markdown", "links", "html", "rawHtml", "screenshot"], "includeTags": ["h1", "p", "a", ".main-content"], "excludeTags": ["#ad", "#footer"], "onlyMainContent": false, "waitFor": 1000, "timeout": 15000}`
- **Default**: `{ "formats": ["markdown"] }`
- **See**: [Scrape Options](#setting-the-content-formats-on-response-with-formats)

<div id="example-usage">
  ### 使い方の例
</div>

```bash
curl -X POST https://api.firecrawl.dev/v1/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "includePaths": ["^/blog/.*$", "^/docs/.*$"],
      "excludePaths": ["^/admin/.*$", "^/private/.*$"],
      "maxDepth": 2,
      "limit": 1000
    }'
```

この例では、クローラーは次のように動作します:

* パターン `^/blog/.*$` と `^/docs/.*$` に一致する URL のみをクロールします。
* パターン `^/admin/.*$` と `^/private/.*$` に一致する URL はスキップします。
* 各ページのドキュメント全体を返します。
* 最大深さは 2 です。
* 最大 1000 ページまでクロールします。


<div id="mapping-website-links-with-map">
  ## `/map` を使ったウェブサイトリンクのマッピング
</div>

`/map` エンドポイントは、指定したウェブサイトに文脈的に関連するURLを的確に抽出できます。この機能は、サイトのコンテキストに基づくリンク環境を把握するうえで重要で、戦略的なサイト分析やナビゲーション設計に大いに役立ちます。

<div id="usage">
  ### 使い方
</div>

`/map` エンドポイントを利用するには、対象ページの URL を指定して GET リクエストを送信します。以下は `curl` を使った例です：

```bash
curl -X POST https://api.firecrawl.dev/v1/map \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'
```

これは、URLにコンテキスト的に関連するリンクを含むJSONオブジェクトを返します。


<div id="example-response">
  ### 例のレスポンス
</div>

```json
  {
    "success": true,
    "links": [
      "https://docs.firecrawl.dev",
      "https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete",
      "https://docs.firecrawl.dev/api-reference/endpoint/crawl-get",
      "https://docs.firecrawl.dev/api-reference/endpoint/crawl-post",
      "https://docs.firecrawl.dev/api-reference/endpoint/map",
      "https://docs.firecrawl.dev/api-reference/endpoint/scrape",
      "https://docs.firecrawl.dev/api-reference/introduction",
      "https://docs.firecrawl.dev/articles/search-announcement",
      ...
    ]
  }
```


<div id="map-options">
  ### マップのオプション
</div>

<div id="search">
  #### `search`
</div>

- **Type**: `string`
- **Description**: 指定したテキストを含むリンクを検索します。
- **Example**: `"blog"`

<div id="limit">
  #### `limit`
</div>

- **Type**: `integer`
- **Description**: 返却するリンク数の上限。
- **Default**: `100`

<div id="ignoresitemap">
  #### `ignoreSitemap`
</div>

- **Type**: `boolean`
- **Description**: クロール時にサイトのサイトマップを無視する
- **Default**: `true`

<div id="includesubdomains">
  #### `includeSubdomains`
</div>

- **Type**: `boolean`
- **Description**: ウェブサイトのサブドメインを含めるかどうか
- **Default**: `true`

該当するAPIリファレンス: [Map Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/map)

お読みいただきありがとうございます。