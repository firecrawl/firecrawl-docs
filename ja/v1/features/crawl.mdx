---
title: 'クロール'
description: 'Firecrawl は URL のサブドメインを再帰的に探索し、コンテンツを収集できます'
og:title: 'クロール | Firecrawl'
og:description: 'Firecrawl は URL のサブドメインを再帰的に探索し、コンテンツを収集できます'
---

import InstallationPython from '/snippets/ja/v1/installation/python.mdx';
import InstallationNode from '/snippets/ja/v1/installation/js.mdx';
import InstallationGo from '/snippets/ja/v1/installation/go.mdx';
import InstallationRust from '/snippets/ja/v1/installation/rust.mdx';
import CrawlPython from '/snippets/ja/v1/crawl/base/python.mdx';
import CrawlNode from '/snippets/ja/v1/crawl/base/js.mdx';
import CrawlGo from '/snippets/ja/v1/crawl/base/go.mdx';
import CrawlRust from '/snippets/ja/v1/crawl/base/rust.mdx';
import CrawlCURL from '/snippets/ja/v1/crawl/base/curl.mdx';
import AsyncCrawlOutput from '/snippets/ja/v1/crawl-async/base/output.mdx';
import CheckCrawlJobPython from '/snippets/ja/v1/crawl-status/short/python.mdx';
import CheckCrawlJobNode from '/snippets/ja/v1/crawl-status/short/js.mdx';
import CheckCrawlJobGo from '/snippets/ja/v1/crawl-status/short/go.mdx';
import CheckCrawlJobRust from '/snippets/ja/v1/crawl-status/short/rust.mdx';
import CheckCrawlJobCURL from '/snippets/ja/v1/crawl-status/short/curl.mdx';
import CheckCrawlJobOutputScraping from '/snippets/ja/v1/crawl-status/base/output-scraping.mdx';
import CheckCrawlJobOutputCompleted from '/snippets/ja/v1/crawl-status/base/output-completed.mdx';
import CrawlWebSocketPython from '/snippets/ja/v1/crawl-websocket/base/python.mdx';
import CrawlWebSocketNode from '/snippets/ja/v1/crawl-websocket/base/js.mdx';
import CrawlWebhookCURL from '/snippets/ja/v1/crawl-webhook/base/curl.mdx';
import PythonCrawlExample from '/snippets/ja/v1/crawl/sdk-example/python.mdx';
import NodeCrawlExample from '/snippets/ja/v1/crawl/sdk-example/js.mdx';
import PythonCrawlExampleResponse from '/snippets/ja/v1/crawl/sdk-example/python-response.mdx';
import NodeCrawlExampleResponse from '/snippets/ja/v1/crawl/sdk-example/js-response.mdx';
import FastCrawlPython from '/snippets/ja/v1/crawl/fast/python.mdx';
import FastCrawlNode from '/snippets/ja/v1/crawl/fast/js.mdx';
import FastCrawlGo from '/snippets/ja/v1/crawl/fast/go.mdx';
import FastCrawlRust from '/snippets/ja/v1/crawl/fast/rust.mdx';
import FastCrawlCURL from '/snippets/ja/v1/crawl/fast/curl.mdx';

Firecrawl はブロッカーを回避しつつ、ウェブサイトを効率的にクロールして網羅的なデータを抽出します。プロセスは以下のとおりです:

1. **URL 解析:** サイトマップをスキャンし、サイト全体をクロールしてリンクを特定
2. **トラバーサル:** すべてのサブページを見つけるためにリンクを再帰的にたどる
3. **スクレイピング:** 各ページからコンテンツを抽出し、JS やレート制限に対応
4. **出力:** データをクリーンな Markdown または構造化フォーマットに変換

これにより、任意の開始 URL から抜け漏れのないデータ収集を実現します。

<div id='crawling'>## クローリング</div>

<div id='crawl-endpoint'>### /crawl エンドポイント</div>

指定した URL と、そこからアクセス可能なすべてのサブページをクロールします。クロールジョブを送信し、進行状況の確認に使うジョブ ID を返します。

<Warning>
  デフォルトでは、指定した URL
  の配下（子階層）でないページへのサブリンクは無視されます。したがって、website.com/blogs/
  をクロールしても website.com/other-parent/blog-1
  は返りません。website.com/other-parent/blog-1
  も対象にしたい場合は、`crawlEntireDomain`
  パラメータを使用してください。website.com をクロールする際に blog.website.com
  のようなサブドメインもクロールするには、`allowSubdomains`
  パラメータを使用してください。
</Warning>

<div id='installation'>### インストール</div>

<CodeGroup>
  <InstallationPython />

{' '}
<InstallationNode />

{' '}
<InstallationGo />

  <InstallationRust />
</CodeGroup>

<div id='usage'>### 使い方</div>

<CodeGroup>
  <CrawlPython />

{' '}
<CrawlNode />

{' '}
<CrawlGo />

{' '}
<CrawlRust />

  <CrawlCURL />
</CodeGroup>

<div id='api-response'>### API レスポンス</div>

cURL または SDK の `async crawl` 関数を使用している場合は、クロールのステータス確認に使用できる `ID` が返されます。

<Note>
  SDK を使用している場合は、[下記](#sdk-response)の「SDK
  レスポンス」セクションを確認してください。
</Note>

<AsyncCrawlOutput />

<div id='check-crawl-job'>### クローリングジョブの確認</div>

クローリングジョブのステータスを確認し、結果を取得します。

<Note>
  このエンドポイントは、進行中のクローリング、または最近完了したクローリングにのみ対応します。
</Note>

<CodeGroup>
  <CheckCrawlJobPython />

{' '}
<CheckCrawlJobNode />

{' '}
<CheckCrawlJobGo />

{' '}
<CheckCrawlJobRust />

  <CheckCrawlJobCURL />
</CodeGroup>

<div id='response-handling'>#### レスポンスの処理</div>

レスポンスはクロールのステータスによって変わります。

完了していない場合、またはサイズが 10MB を超える場合は、`next` URL パラメータが返されます。次の 10MB のデータを取得するには、この URL にリクエストしてください。`next` パラメータがない場合は、クロールデータが最後まで到達したことを示します。

skip パラメータは、返される各チャンクに含まれる結果の最大件数を設定します。

<Info>
  skip と next パラメータが関係するのは、API を直接叩く場合のみです。SDK
  を使用している場合は、こちらで処理し、すべての結果を一度に返します。
</Info>

<CodeGroup>
  <CheckCrawlJobOutputScraping />

  <CheckCrawlJobOutputCompleted />
</CodeGroup>

<div id='sdk-response'>### SDK レスポンス</div>

SDK では URL をクロールする方法が 2 つあります：

1. **同期クロール**（`crawl_url`/`crawlUrl`）:
   - クロールの完了まで待機し、完全なレスポンスを返します
   - ページネーションを自動的に処理します
   - ほとんどのユースケースで推奨されます

<CodeGroup>
  <PythonCrawlExample />

  <NodeCrawlExample />
</CodeGroup>

レスポンスには、クロールのステータスとスクレイプされたすべてのデータが含まれます：

<CodeGroup>
  <PythonCrawlExampleResponse />

  <NodeCrawlExampleResponse />
</CodeGroup>

2. **非同期クロール**（`async_crawl_url`/`asyncCrawlUrl`）:
   - すぐにクロール ID を返します
   - ステータスを手動で確認できます
   - 長時間実行のクロールやカスタムポーリングロジックに有用です

<CodeGroup>
  <AsyncCrawlPython />

  <AsyncCrawlNode />
</CodeGroup>

<div id='faster-crawling'>## より高速なクロール</div>

最新データが不要な場合、クロールを最大 500%高速化できます。利用可能なときはキャッシュ済みページデータを使うよう、`scrapeOptions` に `maxAge` を追加してください。

<CodeGroup>
  <FastCrawlPython />

{' '}
<FastCrawlNode />

{' '}
<FastCrawlGo />

{' '}
<FastCrawlRust />

  <FastCrawlCURL />
</CodeGroup>

**動作概要:**

- クロール対象の各ページで、`maxAge` より新しいキャッシュがあるかを確認
- あればキャッシュから即時に返却（最大 500%高速）
- なければページを新規にスクレイプして結果をキャッシュ
- ドキュメントサイト、製品カタログ、その他比較的静的なコンテンツのクロールに最適

`maxAge` の使い方の詳細は、[Faster Scraping](/ja/features/fast-scraping) のドキュメントをご覧ください。

<div id='crawl-websocket'>## Crawl WebSocket</div>

Firecrawl の WebSocket ベースのメソッド「Crawl URL and Watch」は、リアルタイムでのデータ抽出と監視を可能にします。URL を指定してクロールを開始し、ページ上限、許可ドメイン、出力フォーマットなどのオプションでカスタマイズできます。即時のデータ処理に最適です。

<CodeGroup>
  <CrawlWebSocketPython />

  <CrawlWebSocketNode />
</CodeGroup>

<div id='crawl-webhook'>## クロール用 Webhook</div>

クロールの進行に合わせてリアルタイム通知を受け取るために Webhook を設定できます。これにより、クロール全体の完了を待たずに、ページがスクレイプされ次第処理できます。

<CrawlWebhookCURL />

イベントタイプ、ペイロード構造、実装例など、詳細は [Webhooks ドキュメント](/ja/webhooks/overview)を参照してください。

<div id='quick-reference'>### クイックリファレンス</div>

**イベントタイプ:**

- `crawl.started` - クローリングの開始時
- `crawl.page` - 各ページのスクレイピング成功時
- `crawl.completed` - クローリングの完了時
- `crawl.failed` - クローリング中にエラーが発生した場合

**基本ペイロード:**

```json
{
  "success": true,
  "type": "crawl.page",
  "id": "crawl-job-id",
  "data": [...], // 「page」イベント用のページデータ
  "metadata": {}, // 任意のメタデータ
  "error": null
}
```

<Note>
  Webhook
  の詳細な設定方法、セキュリティのベストプラクティス、トラブルシューティングは
  [Webhooks ドキュメント](/ja/webhooks/overview)をご参照ください。
</Note>
