---
title: "スクレイプ"
description: "あらゆるURLをクリーンなデータに変換"
og:title: "スクレイプ | Firecrawl"
og:description: "あらゆるURLをクリーンなデータに変換"
---

import InstallationPython from "/snippets/ja/v1/installation/python.mdx";
import InstallationNode from "/snippets/ja/v1/installation/js.mdx";
import InstallationGo from "/snippets/ja/v1/installation/go.mdx";
import InstallationRust from "/snippets/ja/v1/installation/rust.mdx";
import ScrapePython from "/snippets/ja/v1/scrape/base/python.mdx";
import ScrapeNode from "/snippets/ja/v1/scrape/base/js.mdx";
import ScrapeGo from "/snippets/ja/v1/scrape/base/go.mdx";
import ScrapeRust from "/snippets/ja/v1/scrape/base/rust.mdx";
import ScrapeCURL from "/snippets/ja/v1/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/ja/v1/scrape/base/output.mdx";
import ExtractCURL from "/snippets/ja/v1/llm-extract/base/curl.mdx";
import ExtractPython from "/snippets/ja/v1/llm-extract/base/python.mdx";
import ExtractNode from "/snippets/ja/v1/llm-extract/base/js.mdx";
import ExtractOutput from "/snippets/ja/v1/llm-extract/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/ja/v1/llm-extract/no-schema/curl.mdx";
import ExtractNoSchemaOutput from "/snippets/ja/v1/llm-extract/no-schema/output.mdx";
import ScrapeActionsPython from "/snippets/ja/v1/scrape/actions/python.mdx";
import ScrapeActionsNode from "/snippets/ja/v1/scrape/actions/js.mdx";
import ScrapeActionsCURL from "/snippets/ja/v1/scrape/actions/curl.mdx";
import ScrapeActionsOutput from "/snippets/ja/v1/scrape/actions/output.mdx";
import BatchScrapePython from "/snippets/ja/v1/batch-scrape/base/python.mdx";
import BatchScrapeNode from "/snippets/ja/v1/batch-scrape/base/js.mdx";
import BatchScrapeCURL from "/snippets/ja/v1/batch-scrape/base/curl.mdx";
import BatchScrapeOutput from "/snippets/ja/v1/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/ja/v1/batch-scrape/base/async-output.mdx";
import ScrapeLocationPython from "/snippets/ja/v1/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/ja/v1/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/ja/v1/scrape/location/curl.mdx";

FirecrawlはウェブページをMarkdownに変換し、LLMアプリケーションに最適です。

* 複雑な処理を引き受けます：プロキシ、キャッシュ、レート制限、JavaScriptでブロックされたコンテンツ
* 動的コンテンツに対応：動的サイト、JavaScriptレンダリングのサイト、PDF、画像
* クリーンなMarkdown、構造化データ、スクリーンショット、またはHTMLを出力します。

詳細は、[Scrape Endpoint API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape)をご覧ください。

<div id="scraping-a-url-with-firecrawl">
  ## FirecrawlでURLをスクレイピングする
</div>

<div id="scrape-endpoint">
  ### /scrape エンドポイント
</div>

URLをスクレイプして、コンテンツを取得します。

<div id="installation">
  ### インストール
</div>

<CodeGroup>
  <InstallationPython />

  <InstallationNode />

  <InstallationGo />

  <InstallationRust />
</CodeGroup>

<div id="usage">
  ### 使い方
</div>

<CodeGroup>
  <ScrapePython />

  <ScrapeNode />

  <ScrapeGo />

  <ScrapeRust />

  <ScrapeCURL />
</CodeGroup>

パラメータの詳細は、[APIリファレンス](https://docs.firecrawl.dev/api-reference/endpoint/scrape)を参照してください。

<div id="response">
  ### レスポンス
</div>

各 SDK はデータオブジェクトを直接返します。cURL は以下のとおり、そのままのペイロードを返します。

<ScrapeResponse />

<div id="scrape-formats">
  ## スクレイプのフォーマット
</div>

出力するフォーマットを選べます。複数の出力フォーマットを指定できます。サポートされているフォーマットは次のとおりです:

* Markdown (markdown)
* HTML (html)
* 生の HTML (rawHtml)（変更なし）
* スクリーンショット (screenshot または screenshot@fullPage)
* リンク (links)
* JSON (json) - 構造化出力

出力のキーは、選択したフォーマットに対応します。

<div id="extract-structured-data">
  ## 構造化データの抽出
</div>

<div id="scrape-with-json-endpoint">
  ### /scrape（json使用）エンドポイント
</div>

スクレイプしたページから構造化データを抽出します。

<CodeGroup>
  <ExtractPython />

  <ExtractNode />

  <ExtractCURL />
</CodeGroup>

出力:

<ExtractOutput />

<div id="extracting-without-schema-new">
  ### スキーマなしでの抽出（新機能）
</div>

エンドポイントに `prompt` を渡すだけで、スキーマを定義せずに抽出できます。データの構造は LLM が決定します。

<CodeGroup>
  <ExtractNoSchemaCURL />
</CodeGroup>

出力:

<ExtractNoSchemaOutput />

<div id="json-options-object">
  ### JSON オプションオブジェクト
</div>

`jsonOptions` オブジェクトでは次のパラメータを指定できます:

* `schema`: 抽出に使用するスキーマ。
* `systemPrompt`: 抽出に使用するシステムプロンプト。
* `prompt`: スキーマなしで抽出に使用するプロンプト。

<div id="interacting-with-the-page-with-actions">
  ## アクションでページを操作する
</div>

Firecrawl では、スクレイピングの前に Web ページ上でさまざまなアクションを実行できます。これは、動的コンテンツとのやり取り、ページ遷移、ユーザー操作が必要なコンテンツへのアクセスに特に役立ちます。

以下は、アクションを使って google.com に移動し、Firecrawl を検索し、最初の結果をクリックしてスクリーンショットを取得する例です。

ページの読み込み時間を確保するために、他のアクションの前後には原則として `wait` アクションを使用することが重要です。

<div id="example">
  ### 例
</div>

<CodeGroup>
  <ScrapeActionsPython />

  <ScrapeActionsNode />

  <ScrapeActionsCURL />
</CodeGroup>

<div id="output">
  ### 出力
</div>

<CodeGroup>
  <ScrapeActionsOutput />
</CodeGroup>

アクションのパラメータの詳細は、[APIリファレンス](https://docs.firecrawl.dev/api-reference/endpoint/scrape)をご参照ください。

<div id="location-and-language">
  ## ロケーションと言語
</div>

対象の地域と言語の設定に基づいて関連するコンテンツを取得できるよう、国と言語（優先言語）を指定します。

<div id="how-it-works">
  ### 仕組み
</div>

ロケーション設定を指定すると、Firecrawl は利用可能であれば適切なプロキシを使用し、対応する言語とタイムゾーン設定をエミュレートします。ロケーションを指定しない場合のデフォルトは「US」です。

<div id="usage">
  ### 使い方
</div>

ロケーションと言語の設定を利用するには、リクエストボディに `location` オブジェクトを含め、次のプロパティを指定します:

* `country`: ISO 3166-1 alpha-2 の国コード（例：&#39;US&#39;、&#39;AU&#39;、&#39;DE&#39;、&#39;JP&#39;）。デフォルトは &#39;US&#39;。
* `languages`: リクエストで優先する言語・ロケールを優先度順に並べた配列。デフォルトは指定したロケーションの言語。

<CodeGroup>
  <ScrapeLocationPython />

  <ScrapeLocationNode />

  <ScrapeLocationCURL />
</CodeGroup>

<div id="batch-scraping-multiple-urls">
  ## 複数URLのバッチスクレイピング
</div>

複数のURLを同時にバッチスクレイピングできます。開始URLと任意のパラメータを引数として受け取ります。params 引数では、出力フォーマットなど、バッチスクレイピングジョブの追加オプションを指定できます。

<div id="how-it-works">
  ### 仕組み
</div>

これは `/crawl` エンドポイントの動作と非常によく似ています。バッチスクレイプのジョブを送信し、そのステータスを確認するためのジョブIDを返します。

SDK には同期と非同期の2つのメソッドがあります。同期メソッドはバッチスクレイプジョブの結果を返し、非同期メソッドはステータス確認に使用できるジョブIDを返します。

<div id="usage">
  ### 使い方
</div>

<CodeGroup>
  <BatchScrapePython />

  <BatchScrapeNode />

  <BatchScrapeCURL />
</CodeGroup>

<div id="response">
  ### レスポンス
</div>

SDK の同期メソッドを使用している場合は、バッチスクレイプジョブの結果が返ります。そうでない場合は、バッチスクレイプのステータスを確認するために使用できるジョブ ID が返ります。

<div id="synchronous">
  #### 同期処理
</div>

<BatchScrapeOutput />

<div id="asynchronous">
  #### 非同期
</div>

その後、ジョブ ID を使って `/batch/scrape/{id}` エンドポイントを呼び出し、バッチスクレイプのステータスを確認できます。バッチスクレイプのジョブは 24 時間で失効するため、このエンドポイントはジョブの実行中または完了直後に使用することを想定しています。

<BatchScrapeAsyncOutput />

<div id="enhanced-mode">
  ## 強化モード
</div>

複雑なウェブサイト向けに、Firecrawl はプライバシーを保護しながら成功率を高める強化モードを提供しています。

詳しくは[強化モード](/ja/features/enhanced-mode)をご覧ください。

<div id="using-fire-1-with-scrape">
  ## Scrape での FIRE-1 の使用
</div>

`/scrape` エンドポイントで FIRE-1（エージェント）を使うと、最終的なコンテンツをスクレイプする前にインテリジェントなナビゲーションを実行できます。

FIRE-1 の有効化は簡単です。`/scrape` または `/extract` の API リクエストに `agent` オブジェクトを含めるだけです。

```json
"agent": {
  "model": "FIRE-1",
  "prompt": "ここに詳細なナビゲーション手順を記述してください。"
}
```

*注:* `prompt` フィールドはスクレイプリクエストで必須であり、FIRE-1 にウェブページとの具体的なやり取り方法を正確に指示します。

<div id="example-usage-with-scrape-endpoint">
  ### /scrape エンドポイントでの使用例
</div>

ここでは、FIRE-1 を /scrape エンドポイントと併用して、Y Combinator のコンシューマー分野に属する企業を取得する簡単な例を示します。

```bash
curl -X POST https://api.firecrawl.dev/v1/scrape \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_API_KEY' \
  -d '{
    "url": "https://ycombinator.com/companies",
    "formats": ["markdown"],
    "agent": {
      "model": "FIRE-1",
      "prompt": "該当のボタンをクリックして、コンシューマー分野の W22 企業を取得する"
    }
  }'
```
