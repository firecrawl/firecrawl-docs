---
title: v1 → v2 移行
description: "v2 へのアップグレードに必要な主要な変更点、マッピング、ビフォー・アフターのスニペット。"
og:title: "v1 → v2 移行 | Firecrawl"
og:description: "v2 へのアップグレードに必要な主要な変更点、マッピング、ビフォー・アフターのスニペット。"
---

<div id="overview">
  ## 概要
</div>

<div id="key-improvements">
  ### 主要な改善点
</div>

- **デフォルトで高速化**: リクエストは `maxAge` がデフォルトで2日としてキャッシュされ、`blockAds`、`skipTlsVerification`、`removeBase64Images` といった妥当なデフォルト設定が有効です。

- **新しいサマリーフォーマット**: フォーマットに `"summary"` を指定すると、ページ内容の簡潔な要約を直接受け取れます。

- **JSON 抽出の更新**: JSON 抽出と変更トラッキングは、オブジェクト形式 `{ type: "json", prompt, schema }` を使用するようになりました。従来の `"extract"` フォーマットは `"json"` に名称変更されました。

- **スクリーンショットオプションの強化**: オブジェクト形式 `{ type: "screenshot", fullPage, quality, viewport }` を使用します。

- **新しい検索ソース**: `sources` パラメータを設定することで、Web 結果に加えて `"news"` と `"images"` も横断検索できます。

- **プロンプトによるスマートクロール**: 自然言語の `prompt` をクロールに渡すと、システムがパスや制限を自動的に導出します。ジョブ開始前に導出されたオプションを確認するには、新しい /crawl/params-preview エンドポイントを使用してください。

<div id="quick-migration-checklist">
  ## クイック移行チェックリスト
</div>

- v1 クライアントの使用箇所を v2 クライアントに置き換える:
  - JS: `const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR-API-KEY' })`
  - Python: `firecrawl = Firecrawl(api_key='fc-YOUR-API-KEY')`
  - API: 新しい `https://api.firecrawl.dev/v2/` エンドポイントを使用
- フォーマットを更新:
  - 必要に応じて `"summary"` を使用
  - JSONモード: JSON 抽出には `{ type: "json", prompt, schema }` を使用
  - Screenshot および Screenshot@fullPage: オプション指定時は screenshot オブジェクトのフォーマットを使用
- SDK の標準化された非同期フローを採用:
  - クロール: `startCrawl` + `getCrawlStatus`（または `crawl` ウェイター）
  - バッチ: `startBatchScrape` + `getBatchScrapeStatus`（または `batchScrape` ウェイター）
  - 抽出: `startExtract` + `getExtractStatus`（または `extract` ウェイター）
- クロールオプションのマッピング（下記参照）
- `/crawl/params-preview` でクロールの `prompt` を確認

<div id="sdk-surface-v2">
  ## SDK サーフェス（v2）
</div>

<div id="jsts">
  ### JS/TS
</div>

<div id="method-name-changes-v1-v2">
  #### メソッド名の変更（v1 → v2）
</div>

**Scrape、Search、Map**

| v1 (FirecrawlApp)     | v2 (Firecrawl)           |
|-----------------------|--------------------------|
| `scrapeUrl(url, ...)` | `scrape(url, options?)`  |
| `search(query, ...)`  | `search(query, options?)`|
| `mapUrl(url, ...)`    | `map(url, options?)`     |

**クロール**

| v1                        | v2                        |
|---------------------------|---------------------------|
| `crawlUrl(url, ...)`      | `crawl(url, options?)`（waiter） |
| `asyncCrawlUrl(url, ...)` | `startCrawl(url, options?)`     |
| `checkCrawlStatus(id, ...)` | `getCrawlStatus(id)`          |
| `cancelCrawl(id)`         | `cancelCrawl(id)`              |
| `checkCrawlErrors(id)`    | `getCrawlErrors(id)`           |

**バッチスクレイピング**

| v1                                 | v2                                 |
|------------------------------------|------------------------------------|
| `batchScrapeUrls(urls, ...)`       | `batchScrape(urls, opts?)`（waiter）|
| `asyncBatchScrapeUrls(urls, ...)`  | `startBatchScrape(urls, opts?)`    |
| `checkBatchScrapeStatus(id, ...)`  | `getBatchScrapeStatus(id)`         |
| `checkBatchScrapeErrors(id)`       | `getBatchScrapeErrors(id)`         |

**抽出**

| v1                          | v2                  |
|-----------------------------|---------------------|
| `extract(urls?, params?)`   | `extract(args)`     |
| `asyncExtract(urls, params?)` | `startExtract(args)` |
| `getExtractStatus(id)`      | `getExtractStatus(id)` |

**その他／削除**

| v1                                 | v2                |
|------------------------------------|-------------------|
| `generateLLMsText(...)`            | （v2 SDK にはありません）   |
| `checkGenerateLLMsTextStatus(id)`  | （v2 SDK にはありません）   |
| `crawlUrlAndWatch(...)`            | `watcher(jobId, ...)` |
| `batchScrapeUrlsAndWatch(...)`     | `watcher(jobId, ...)` |

---

<div id="python-sync">
  ### Python（同期）
</div>

<div id="method-name-changes-v1-v2">
  #### メソッド名の変更（v1 → v2）
</div>

**Scrape、Search、Map**

| v1                | v2             |
|-------------------|----------------|
| `scrape_url(...)` | `scrape(...)`  |
| `search(...)`     | `search(...)`  |
| `map_url(...)`    | `map(...)`     |

**Crawling**

| v1                      | v2                    |
|-------------------------|-----------------------|
| `crawl_url(...)`        | `crawl(...)`（waiter） |
| `async_crawl_url(...)`  | `start_crawl(...)`    |
| `check_crawl_status(...)` | `get_crawl_status(...)` |
| `cancel_crawl(...)`     | `cancel_crawl(...)`   |

**バッチスクレイピング**

| v1                           | v2                           |
|------------------------------|------------------------------|
| `batch_scrape_urls(...)`     | `batch_scrape(...)`（waiter） |
| `async_batch_scrape_urls(...)` | `start_batch_scrape(...)`   |
| `get_batch_scrape_status(...)` | `get_batch_scrape_status(...)` |
| `get_batch_scrape_errors(...)` | `get_batch_scrape_errors(...)` |

**抽出**

| v1                | v2                |
|-------------------|-------------------|
| `extract(...)`    | `extract(...)`    |
| `start_extract(...)` | `start_extract(...)` |
| `get_extract_status(...)` | `get_extract_status(...)` |

**その他 / 廃止**

| v1                              | v2                |
|---------------------------------|-------------------|
| `generate_llms_text(...)`       | （v2 SDK では未対応） |
| `get_generate_llms_text_status(...)` | （v2 SDK では未対応） |
| `watch_crawl(...)`              | `watcher(job_id, ...)` |

---

<div id="python-async">
  ### Python（非同期）
</div>

- `AsyncFirecrawl` は同じメソッドを備えており（すべて await 可能）、そのまま利用できます。

<div id="formats-and-scrape-options">
  ## フォーマットとスクレイプオプション
</div>

- 基本的な用途には文字列フォーマットを使用: `"markdown"`, `"html"`, `"rawHtml"`, `"links"`, `"summary"`。
- `parsePDF` の代わりに `parsers: [ { "type": "pdf" } | "pdf" ]` を使用。
- JSON、変更トラッキング、スクリーンショットにはオブジェクトフォーマットを使用:

<div id="json-format">
  ### JSONフォーマット
</div>

<CodeGroup>
```js Node
const formats = [ {
  "type": "json",
  "prompt": "ページから企業のミッションを抽出してください。"
}];

doc = firecrawl.scrape(url, { formats });
```

```python Python
formats = [ { "type": "json", "prompt": "ページから企業のミッションを抽出してください。" } ];

doc = firecrawl.scrape(url, formats=formats);
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev/",
      "formats": [{
        "type": "json",
        "prompt": "ページから企業のミッションを抽出してください。"
      }]
    }'
```
</CodeGroup>

<div id="screenshot-format">
  ### スクリーンショットフォーマット
</div>

<CodeGroup>

```js Node
// スクリーンショットフォーマット（JS）
const formats = [ { "type": "screenshot", "fullPage": true, "quality": 80, "viewport": { "width": 1280, "height": 800 } } ];

doc = firecrawl.scrape(url, { formats });
```

```python Python
# スクリーンショットフォーマット（Python）
formats = [ { "type": "screenshot", "fullPage": true, "quality": 80, "viewport": { "width": 1280, "height": 800 } } ];
doc = firecrawl.scrape(url, formats=formats);
```

```bash cURL
# スクリーンショットフォーマット（cURL）
curl -X POST https://api.firecrawl.dev/v2/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev/",
      "formats": [{
        "type": "screenshot",
        "fullPage": true,
        "quality": 80,
        "viewport": { "width": 1280, "height": 800 }
      }]
    }'
```

</CodeGroup>

<div id="crawl-options-mapping-v1-v2">
  ## クロールオプションの対応表（v1 → v2）
</div>

| v1                     | v2                 |
|------------------------|--------------------|
| `allowBackwardCrawling`| （削除）`crawlEntireDomain` を使用 |
| `maxDepth`             | （削除）`maxDiscoveryDepth` を使用 |
| `ignoreSitemap` (bool) | `sitemap`（例：「only」「skip」「include」） |
| （なし） | `prompt` |

<div id="crawl-prompt-params-preview">
  ## クロール用プロンプトとパラメータのプレビュー
</div>

クロールパラメータのプレビュー例:

<CodeGroup>
```js Node
import Firecrawl from '@mendable/firecrawl-js';

const firecrawl = new Firecrawl({ apiKey: "fc-YOUR-API-KEY" });

const params = await firecrawl.crawlParamsPreview('https://docs.firecrawl.dev', 'Extract docs and blog');
console.log(params);
```

```python Python
from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key='fc-YOUR-API-KEY')
preview = firecrawl.crawl_params_preview(url='https://docs.firecrawl.dev', prompt='Extract docs and blog')
print(preview)
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v2/crawl/params-preview \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "prompt": "Extract docs and blog"
    }'
```
</CodeGroup>