---
title: 'Skill + CLI'
description: 'Firecrawl Skill is an easy way for AI agents such as Claude Code, Antigravity and  OpenCode to use Firecrawl through the CLI.'
icon: 'terminal'
og:title: "CLI | Firecrawl"
og:description: "Firecrawl Skills is an easy way for AI agents to use Firecrawl through the CLI. AI agents can get web data through a better and more context efficient interface"
---

import InstallationCLI from '/snippets/v2/cli/installation/bash.mdx'
import AuthLogin from '/snippets/v2/cli/auth/login.mdx'
import AuthLogout from '/snippets/v2/cli/auth/logout.mdx'
import AuthConfig from '/snippets/v2/cli/auth/config.mdx'
import AuthSelfHosted from '/snippets/v2/cli/auth/self-hosted.mdx'
import ScrapeBasic from '/snippets/v2/cli/scrape/basic.mdx'
import ScrapeFormats from '/snippets/v2/cli/scrape/formats.mdx'
import ScrapeOptions from '/snippets/v2/cli/scrape/options.mdx'
import CrawlBasic from '/snippets/v2/cli/crawl/basic.mdx'
import CrawlStatus from '/snippets/v2/cli/crawl/status.mdx'
import CrawlOptions from '/snippets/v2/cli/crawl/options.mdx'
import MapBasic from '/snippets/v2/cli/map/basic.mdx'
import MapOptions from '/snippets/v2/cli/map/options.mdx'
import SearchBasic from '/snippets/v2/cli/search/basic.mdx'
import SearchOptions from '/snippets/v2/cli/search/options.mdx'
import AgentBasic from '/snippets/v2/cli/agent/basic.mdx'
import AgentOptions from '/snippets/v2/cli/agent/options.mdx'
import BrowserBasic from '/snippets/v2/browser/cli/basic.mdx'
import BrowserOptions from '/snippets/v2/browser/cli/options.mdx'

## Installation

Install the Firecrawl CLI globally using npm:

<InstallationCLI />

If you are using any AI agent like Claude Code, you can install the Firecrawl skill below and the agent will be able to set it up for you.

```bash
npx skills add firecrawl/cli
```

<Note>
After installing the skill, restart Claude Code for it to discover the new skill.
</Note>

## Authentication

Before using the CLI, you need to authenticate with your Firecrawl API key.

### Login

<AuthLogin />

### View Configuration

<AuthConfig />

### Logout

<AuthLogout />

### Self-Hosted / Local Development

For self-hosted Firecrawl instances or local development, use the `--api-url` option:

<AuthSelfHosted />

When using a custom API URL (anything other than `https://api.firecrawl.dev`), API key authentication is automatically skipped, allowing you to use local instances without an API key.

### Check Status

Verify installation, authentication, and view rate limits:

```bash CLI
firecrawl --status
```

Output when ready:

```
  üî• firecrawl cli v1.1.1

  ‚óè Authenticated via FIRECRAWL_API_KEY
  Concurrency: 0/100 jobs (parallel scrape limit)
  Credits: 500,000 remaining
```

- **Concurrency**: Maximum parallel jobs. Run parallel operations close to this limit but not above.
- **Credits**: Remaining API credits. Each scrape/crawl consumes credits.





## Commands

### Scrape

Scrape a single URL and extract its content in various formats.

<Tip>
Use `--only-main-content` to get clean output without navigation, footers, and ads. This is recommended for most use cases where you want just the article or main page content.
</Tip>

<ScrapeBasic />

#### Output Formats

<ScrapeFormats />

#### Scrape Options

<ScrapeOptions />

**Available Options:**

| Option | Short | Description |
|--------|-------|-------------|
| `--url <url>` | `-u` | URL to scrape (alternative to positional argument) |
| `--format <formats>` | `-f` | Output formats (comma-separated): `markdown`, `html`, `rawHtml`, `links`, `screenshot`, `json`, `images`, `summary`, `changeTracking`, `attributes`, `branding` |
| `--html` | `-H` | Shortcut for `--format html` |
| `--only-main-content` | | Extract only main content |
| `--wait-for <ms>` | | Wait time in milliseconds for JS rendering |
| `--screenshot` | | Take a screenshot |
| `--include-tags <tags>` | | HTML tags to include (comma-separated) |
| `--exclude-tags <tags>` | | HTML tags to exclude (comma-separated) |
| `--output <path>` | `-o` | Save output to file |
| `--json` | | Force JSON output even with single format |
| `--pretty` | | Pretty print JSON output |
| `--timing` | | Show request timing and other useful information |

---


### Search

Search the web and optionally scrape the results.

<SearchBasic />

#### Search Options

<SearchOptions />

**Available Options:**

| Option | Description |
|--------|-------------|
| `--limit <number>` | Maximum results (default: 5, max: 100) |
| `--sources <sources>` | Sources to search: `web`, `images`, `news` (comma-separated) |
| `--categories <categories>` | Filter by category: `github`, `research`, `pdf` (comma-separated) |
| `--tbs <value>` | Time filter: `qdr:h` (hour), `qdr:d` (day), `qdr:w` (week), `qdr:m` (month), `qdr:y` (year) |
| `--location <location>` | Geo-targeting (e.g., "Berlin,Germany") |
| `--country <code>` | ISO country code (default: US) |
| `--timeout <ms>` | Timeout in milliseconds (default: 60000) |
| `--ignore-invalid-urls` | Exclude URLs invalid for other Firecrawl endpoints |
| `--scrape` | Scrape search results |
| `--scrape-formats <formats>` | Formats for scraped content (default: markdown) |
| `--only-main-content` | Include only main content when scraping (default: true) |
| `--json` | Output as JSON |
| `--output <path>` | Save output to file |
| `--pretty` | Pretty print JSON output |

---


### Map

Discover all URLs on a website quickly.

<MapBasic />

#### Map Options

<MapOptions />

**Available Options:**

| Option | Description |
|--------|-------------|
| `--url <url>` | URL to map (alternative to positional argument) |
| `--limit <number>` | Maximum URLs to discover |
| `--search <query>` | Filter URLs by search query |
| `--sitemap <mode>` | Sitemap handling: `include`, `skip`, `only` |
| `--include-subdomains` | Include subdomains |
| `--ignore-query-parameters` | Treat URLs with different params as same |
| `--wait` | Wait for map to complete |
| `--timeout <seconds>` | Timeout in seconds |
| `--json` | Output as JSON |
| `--output <path>` | Save output to file |
| `--pretty` | Pretty print JSON output |

---

### Crawl

Crawl an entire website starting from a URL.

<CrawlBasic />

#### Check Crawl Status

<CrawlStatus />

#### Crawl Options

<CrawlOptions />

**Available Options:**

| Option | Description |
|--------|-------------|
| `--url <url>` | URL to crawl (alternative to positional argument) |
| `--wait` | Wait for crawl to complete |
| `--progress` | Show progress indicator while waiting |
| `--poll-interval <seconds>` | Polling interval (default: 5) |
| `--timeout <seconds>` | Timeout when waiting |
| `--status` | Check status of existing crawl job |
| `--limit <number>` | Maximum pages to crawl |
| `--max-depth <number>` | Maximum crawl depth |
| `--include-paths <paths>` | Paths to include (comma-separated) |
| `--exclude-paths <paths>` | Paths to exclude (comma-separated) |
| `--sitemap <mode>` | Sitemap handling: `include`, `skip`, `only` |
| `--allow-subdomains` | Include subdomains |
| `--allow-external-links` | Follow external links |
| `--crawl-entire-domain` | Crawl entire domain |
| `--ignore-query-parameters` | Treat URLs with different params as same |
| `--delay <ms>` | Delay between requests |
| `--max-concurrency <n>` | Maximum concurrent requests |
| `--output <path>` | Save output to file |
| `--pretty` | Pretty print JSON output |

---

### Agent

Search and gather data from the web using natural language prompts.

<AgentBasic />

#### Agent Options

<AgentOptions />

**Available Options:**

| Option | Description |
|--------|-------------|
| `--urls <urls>` | Optional list of URLs to focus the agent on (comma-separated) |
| `--model <model>` | Model to use: `spark-1-mini` (default, 60% cheaper) or `spark-1-pro` (higher accuracy) |
| `--schema <json>` | JSON schema for structured output (inline JSON string) |
| `--schema-file <path>` | Path to JSON schema file for structured output |
| `--max-credits <number>` | Maximum credits to spend (job fails if limit reached) |
| `--status` | Check status of existing agent job |
| `--wait` | Wait for agent to complete before returning results |
| `--poll-interval <seconds>` | Polling interval when waiting (default: 5) |
| `--timeout <seconds>` | Timeout when waiting (default: no timeout) |
| `--output <path>` | Save output to file |
| `--json` | Output as JSON format |

---

### Browser

Launch cloud browser sessions and execute Python, JavaScript, or bash code remotely. Each session runs a full Chromium instance ‚Äî no local browser install required. Code runs server-side with a pre-configured [Playwright](https://playwright.dev/) `page` object ready to use.

<BrowserBasic />

#### Browser Options

<BrowserOptions />

**Subcommands:**

| Subcommand | Description |
|------------|-------------|
| `launch` | Launch a new cloud browser session (returns session ID, CDP URL, and live view URL) |
| `execute <code>` | Execute Playwright Python/JS code or bash commands in a session |
| `list [status]` | List browser sessions (filter by `active` or `destroyed`) |
| `close` | Close a browser session |

**Execute Options:**

| Option | Description |
|--------|-------------|
| `--python` | Execute as Playwright Python code (default). A Playwright `page` object is available ‚Äî use `await page.goto()`, `await page.title()`, etc. |
| `--js` | Execute as Playwright JavaScript code. Same `page` object available. |
| `--bash` | Execute bash commands remotely in the sandbox where [agent-browser](https://github.com/vercel-labs/agent-browser) (40+ commands) is pre-installed. `CDP_URL` is auto-injected so agent-browser connects to your session automatically. Best approach for AI agents. |
| `--session <id>` | Target a specific session (default: active session) |

**Launch Options:**

| Option | Description |
|--------|-------------|
| `--ttl <seconds>` | Total session TTL (default: 300, range: 30‚Äì3600) |
| `--ttl-inactivity <seconds>` | Auto-close after inactivity (range: 10‚Äì3600) |
| `--stream` | Enable live view streaming |

**Common Options:**

| Option | Description |
|--------|-------------|
| `--output <path>` | Save output to file |
| `--json` | Output as JSON format |

---

### Credit Usage

Check your team's credit balance and usage.

```bash CLI
# View credit usage
firecrawl credit-usage

# Output as JSON
firecrawl credit-usage --json --pretty
```

---

### Version

Display the CLI version.

```bash CLI
firecrawl version
# or
firecrawl --version
```

## Global Options

These options are available for all commands:

| Option | Short | Description |
|--------|-------|-------------|
| `--status` | | Show version, auth, concurrency, and credits |
| `--api-key <key>` | `-k` | Override stored API key for this command |
| `--api-url <url>` | | Use custom API URL (for self-hosted/local development) |
| `--help` | `-h` | Show help for a command |
| `--version` | `-V` | Show CLI version |

## Output Handling

The CLI outputs to stdout by default, making it easy to pipe or redirect:

```bash CLI
# Pipe markdown to another command
firecrawl https://example.com | head -50

# Redirect to a file
firecrawl https://example.com > output.md

# Save JSON with pretty formatting
firecrawl https://example.com --format markdown,links --pretty -o data.json
```

### Format Behavior

- **Single format**: Outputs raw content (markdown text, HTML, etc.)
- **Multiple formats**: Outputs JSON with all requested data

```bash CLI
# Raw markdown output
firecrawl https://example.com --format markdown

# JSON output with multiple formats
firecrawl https://example.com --format markdown,links
```

## Examples

### Quick Scrape

```bash CLI
# Get markdown content from a URL (use --only-main-content for clean output)
firecrawl https://docs.firecrawl.dev --only-main-content

# Get HTML content
firecrawl https://example.com --html -o page.html
```

### Full Site Crawl

```bash CLI
# Crawl a docs site with limits
firecrawl crawl https://docs.example.com --limit 50 --max-depth 2 --wait --progress -o docs.json
```

### Site Discovery

```bash CLI
# Find all blog posts
firecrawl map https://example.com --search "blog" -o blog-urls.txt
```

### Research Workflow

```bash CLI
# Search and scrape results for research
firecrawl search "machine learning best practices 2024" --scrape --scrape-formats markdown --pretty
```

### Agent

```bash CLI
# URLs are optional
firecrawl agent "Find the top 5 AI startups and their funding amounts" --wait

# Focus on specific URLs
firecrawl agent "Compare pricing plans" --urls https://slack.com/pricing,https://teams.microsoft.com/pricing --wait
```

### Browser Automation

```bash CLI
# Launch a session, scrape a page, and close
firecrawl browser launch
firecrawl browser execute '
await page.goto("https://news.ycombinator.com")
title = await page.title()
items = await page.query_selector_all(".titleline > a")
for item in items[:5]:
    print(await item.text_content())
'
firecrawl browser close

# Use agent-browser via bash mode (recommended for AI agents)
firecrawl browser launch
firecrawl browser execute --bash "agent-browser open https://example.com"
firecrawl browser execute --bash "agent-browser snapshot"
# snapshot returns @ref IDs ‚Äî use them to interact
firecrawl browser execute --bash "agent-browser click @e5"
firecrawl browser execute --bash "agent-browser fill @e3 'search query'"
firecrawl browser execute --bash "agent-browser scrape"
# Run --help to see all 40+ commands
firecrawl browser execute --bash "agent-browser --help"
firecrawl browser close
```

### Combine with Other Tools

```bash CLI
# Extract URLs from search results
jq -r '.data.web[].url' search-results.json

# Get titles from search results
jq -r '.data.web[] | "\(.title): \(.url)"' search-results.json

# Extract links and process with jq
firecrawl https://example.com --format links | jq '.links[].url'

# Count URLs from map
firecrawl map https://example.com | wc -l
```

## Telemetry

The CLI collects anonymous usage data during authentication to help improve the product:

- CLI version, OS, and Node.js version
- Development tool detection (e.g., Cursor, VS Code, Claude Code)

**No command data, URLs, or file contents are collected via the CLI.**

To disable telemetry, set the environment variable:

```bash CLI
export FIRECRAWL_NO_TELEMETRY=1
```

## Open Source

The Firecrawl CLI and Skill are open source and available on GitHub: [firecrawl/cli](https://github.com/firecrawl/cli)