---
title: 'Python'
description: 'Firecrawl Python SDK is a wrapper around the Firecrawl API to help you easily turn websites into markdown.'
icon: 'python'
og:title: "Python SDK | Firecrawl"
og:description: "Firecrawl Python SDK is a wrapper around the Firecrawl API to help you easily turn websites into markdown."
---

import InstallationPython from '/snippets/v2/installation/python.mdx'
import ScrapePythonShort from '/snippets/v2/scrape/short/python.mdx'
import CrawlPythonShort from '/snippets/v2/crawl/short/python.mdx'
import CrawlSitemapOnlyPython from '/snippets/v2/crawl/sitemap-only/python.mdx'
import CheckCrawlStatusPythonShort from '/snippets/v2/crawl-status/short/python.mdx'
import StartCrawlPythonShort from '/snippets/v2/start-crawl/short/python.mdx'
import CancelCrawlPythonShort from '/snippets/v2/crawl-delete/short/python.mdx'
import MapPythonShort from '/snippets/v2/map/short/python.mdx'
import ExtractPythonShort from '/snippets/v2/extract/short/python.mdx'
import ScrapeAndCrawlExamplePython from '/snippets/v2/scrape-and-crawl/python.mdx'
import CrawlWebSocketPythonBase from '/snippets/v2/crawl-websocket/base/python.mdx'
import AIOPython from '/snippets/v2/async/base/python.mdx'

## Installation

To install the Firecrawl Python SDK, you can use pip:

<InstallationPython />

## Usage

1. Get an API key from [firecrawl.dev](https://firecrawl.dev)
2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `Firecrawl` class.


Here's an example of how to use the SDK:

<ScrapeAndCrawlExamplePython />

### Scraping a URL

To scrape a single URL, use the `scrape` method. It takes the URL as a parameter and returns the scraped document.

<ScrapePythonShort />

### Crawl a Website

To crawl a website, use the `crawl` method. It takes the starting URL and optional options as arguments. The options allow you to specify additional settings for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format. See [Pagination](#pagination) for auto/manual pagination and limiting.

<CrawlPythonShort />

### Sitemap-Only Crawl

Use `sitemap="only"` to crawl sitemap URLs only (the start URL is always included, and HTML link discovery is skipped).

<CrawlSitemapOnlyPython />

### Start a Crawl

<Tip>Prefer non-blocking? Check out the [Async Class](#async-class) section below.</Tip>

Start a job without waiting using `start_crawl`. It returns a job `ID` you can use to check status. Use `crawl` when you want a waiter that blocks until completion. See [Pagination](#pagination) for paging behavior and limits.

<StartCrawlPythonShort />


### Checking Crawl Status

To check the status of a crawl job, use the `get_crawl_status` method. It takes the job ID as a parameter and returns the current status of the crawl job.

<CheckCrawlStatusPythonShort />

### Cancelling a Crawl

To cancel an crawl job, use the `cancel_crawl` method. It takes the job ID of the `start_crawl` as a parameter and returns the cancellation status.

<CancelCrawlPythonShort />

### Map a Website

Use `map` to generate a list of URLs from a website. The options let you customize the mapping process, including excluding subdomains or utilizing the sitemap.

<MapPythonShort />

{/* ### Extracting Structured Data from Websites

To extract structured data from websites, use the `extract` method. It takes the URLs to extract data from, a prompt, and a schema as arguments. The schema is a Pydantic model that defines the structure of the extracted data.

<ExtractPythonShort /> */}

### Crawling a Website with WebSockets

To crawl a website with WebSockets, start the job with `start_crawl` and subscribe using the `watcher` helper. Create a watcher with the job ID and attach handlers (e.g., for page, completed, failed) before calling `start()`.

<CrawlWebSocketPythonBase />

### Pagination

Firecrawl endpoints for crawl and batch return a `next` URL when more data is available. The Python SDK auto-paginates by default and aggregates all documents; in that case `next` will be `None`. You can disable auto-pagination or set limits.

#### Crawl

Use the waiter method `crawl` for the simplest experience, or start a job and page manually.

##### Simple crawl (auto-pagination, default)

- See the default flow in [Crawl a Website](#crawl-a-website).

##### Manual crawl with pagination control (single page)

- Start a job, then fetch one page at a time with `auto_paginate=False`.

```python Python
crawl_job = client.start_crawl("https://example.com", limit=100)

status = client.get_crawl_status(crawl_job.id, pagination_config=PaginationConfig(auto_paginate=False))
print("crawl single page:", status.status, "docs:", len(status.data), "next:", status.next)
```

##### Manual crawl with limits (auto-pagination + early stop)

- Keep auto-pagination on but stop early with `max_pages`, `max_results`, or `max_wait_time`.

```python Python
status = client.get_crawl_status(
    crawl_job.id,
    pagination_config=PaginationConfig(max_pages=2, max_results=50, max_wait_time=15),
)
print("crawl limited:", status.status, "docs:", len(status.data), "next:", status.next)
```

#### Batch Scrape

Use the waiter method `batch_scrape`, or start a job and page manually.

##### Simple batch scrape (auto-pagination, default)

- See the default flow in [Batch Scrape](/features/batch-scrape).

##### Manual batch scrape with pagination control (single page)

- Start a job, then fetch one page at a time with `auto_paginate=False`.

```python Python
batch_job = client.start_batch_scrape(urls)
status = client.get_batch_scrape_status(batch_job.id, pagination_config=PaginationConfig(auto_paginate=False))
print("batch single page:", status.status, "docs:", len(status.data), "next:", status.next)
```

##### Manual batch scrape with limits (auto-pagination + early stop)

- Keep auto-pagination on but stop early with `max_pages`, `max_results`, or `max_wait_time`.

```python Python
status = client.get_batch_scrape_status(
    batch_job.id,
    pagination_config=PaginationConfig(max_pages=2, max_results=100, max_wait_time=20),
)
print("batch limited:", status.status, "docs:", len(status.data), "next:", status.next)
```

## Error Handling

The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.

## Async Class

For async operations, use the `AsyncFirecrawl` class. Its methods mirror `Firecrawl`, but they don't block the main thread.

<AIOPython />
