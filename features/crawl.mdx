---
title: 'Crawl'
description: 'Firecrawl can recursively search through a urls subdomains, and gather the content'
og:title: 'Crawl | Firecrawl'
og:description: 'Firecrawl can recursively search through a urls subdomains, and gather the content'
icon: 'spider'
---

import InstallationPython from '/snippets/v2/installation/python.mdx';
import InstallationNode from '/snippets/v2/installation/js.mdx';
import InstallationCLI from '/snippets/v2/installation/cli.mdx';
import CrawlPython from '/snippets/v2/crawl/base/python.mdx';
import CrawlNode from '/snippets/v2/crawl/base/js.mdx';
import CrawlCURL from '/snippets/v2/crawl/base/curl.mdx';
import CrawlCLI from '/snippets/v2/crawl/base/cli.mdx';
import CheckCrawlJobPython from '/snippets/v2/crawl-status/short/python.mdx';
import CheckCrawlJobNode from '/snippets/v2/crawl-status/short/js.mdx';
import CheckCrawlJobCURL from '/snippets/v2/crawl-status/short/curl.mdx';
import CheckCrawlJobCLI from '/snippets/v2/crawl-status/short/cli.mdx';
import CheckCrawlJobOutputScraping from '/snippets/v2/crawl-status/base/output-scraping.mdx';
import CheckCrawlJobOutputCompleted from '/snippets/v2/crawl-status/base/output-completed.mdx';
import CrawlWebSocketPython from '/snippets/v2/crawl-websocket/base/python.mdx';
import CrawlWebSocketNode from '/snippets/v2/crawl-websocket/base/js.mdx';
import CrawlWebhookCURL from '/snippets/v2/crawl-webhook/base/curl.mdx';
import PythonCrawlExample from '/snippets/v2/crawl/sdk-example/python.mdx';
import NodeCrawlExample from '/snippets/v2/crawl/sdk-example/js.mdx';
import PythonCrawlExampleResponse from '/snippets/v2/crawl/sdk-example/python-response.mdx';
import NodeCrawlExampleResponse from '/snippets/v2/crawl/sdk-example/js-response.mdx';
import StartCrawlPython from '/snippets/v2/start-crawl/base/python.mdx';
import StartCrawlNode from '/snippets/v2/start-crawl/base/js.mdx';
import StartCrawlCURL from '/snippets/v2/start-crawl/base/curl.mdx';
import StartCrawlCLI from '/snippets/v2/start-crawl/base/cli.mdx';
import StartCrawlOutput from '/snippets/v2/start-crawl/base/output.mdx';

Firecrawl efficiently crawls websites to extract comprehensive data while handling complex web infrastructure. The process:

1. **URL Analysis:** Scans sitemap and crawls website to identify links
2. **Traversal:** Recursively follows links to find all subpages
3. **Scraping:** Extracts content from each page, handling JS and rate limits
4. **Output:** Converts data to clean markdown or structured format

This ensures thorough data collection from any starting URL.

## Crawling

### /crawl endpoint

Used to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.

<Warning>
  By default - Crawl will ignore sublinks of a page if they aren't children of
  the url you provide. So, the website.com/other-parent/blog-1 wouldn't be
  returned if you crawled website.com/blogs/. If you want
  website.com/other-parent/blog-1, use the `crawlEntireDomain` parameter. To
  crawl subdomains like blog.website.com when crawling website.com, use the
  `allowSubdomains` parameter.
</Warning>

### Installation

<CodeGroup>

<InstallationPython />
<InstallationNode />
<InstallationCLI />

</CodeGroup>

### Usage

<CodeGroup>

<CrawlPython />
<CrawlNode />
<CrawlCURL />
<CrawlCLI />

</CodeGroup>

### Scrape options in crawl

All options from the Scrape endpoint are available in Crawl via `scrapeOptions` (JS) / `scrape_options` (Python). These apply to every page the crawler scrapes: formats, proxy, caching, actions, location, tags, etc. See the full list in the [Scrape API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

<CodeGroup>

```js Node
import Firecrawl from '@mendable/firecrawl-js';

const firecrawl = new Firecrawl({ apiKey: 'fc-YOUR_API_KEY' });

// Crawl with scrape options
const crawlResponse = await firecrawl.crawl('https://example.com', {
  limit: 100,
  scrapeOptions: {
    formats: [
      'markdown',
      {
        type: 'json',
        schema: { type: 'object', properties: { title: { type: 'string' } } },
      },
    ],
    proxy: 'auto',
    maxAge: 600000,
    onlyMainContent: true,
  },
});
```

```python Python
from firecrawl import Firecrawl

firecrawl = Firecrawl(api_key='fc-YOUR_API_KEY')

# Crawl with scrape options
response = firecrawl.crawl('https://example.com',
    limit=100,
    scrape_options={
        'formats': [
            'markdown',
            { 'type': 'json', 'schema': { 'type': 'object', 'properties': { 'title': { 'type': 'string' } } } }
        ],
        'proxy': 'auto',
        'maxAge': 600000,
        'onlyMainContent': True
    }
)
```

</CodeGroup>

<Tip>
  **PDF Credit Usage:** When crawling pages that contain PDFs, credit consumption may exceed the `limit` parameter since PDF parsing costs 1 credit per PDF page. To control costs, use the `parsers` option in `scrapeOptions`: set `parsers: []` to skip PDF parsing (returns base64 for 1 credit), or use `parsers: [{ type: "pdf", maxPages: 10 }]` to limit parsed pages per PDF.
</Tip>

### API Response

If you're using cURL or the starter method, this will return an `ID` to check the status of the crawl.

<Note>
  If you're using the SDK, see methods below for waiter vs starter behavior.
</Note>

<StartCrawlOutput />

### Check Crawl Job

Used to check the status of a crawl job and get its result.

<Note>
  Job results are available via the API for 24 hours after completion. After this period, you can still view your crawl history and results in the [activity logs](https://www.firecrawl.dev/app/logs).
</Note>

<CodeGroup>

<CheckCrawlJobPython />
<CheckCrawlJobNode />
<CheckCrawlJobCURL />
<CheckCrawlJobCLI />

</CodeGroup>

#### Response Handling

The response varies based on the crawl's status.

For not completed or large responses exceeding 10MB, a `next` URL parameter is provided. You must request this URL to retrieve the next 10MB of data. If the `next` parameter is absent, it indicates the end of the crawl data.

The skip parameter sets the maximum number of results returned for each chunk of results returned.

<Info>
  The skip and next parameter are only relavent when hitting the api directly.
  If you're using the SDK, we handle this for you and will return all the
  results at once.
</Info>

<CodeGroup>
  <CheckCrawlJobOutputScraping />
  <CheckCrawlJobOutputCompleted />
</CodeGroup>

### SDK methods

There are two ways to use the SDK:

1. **Crawl then wait** (`crawl`):
   - Waits for the crawl to complete and returns the full response
   - Handles pagination automatically
   - Recommended for most use cases

<CodeGroup>
  <PythonCrawlExample />
  <NodeCrawlExample />
</CodeGroup>

The response includes the crawl status and all scraped data:

<CodeGroup>
  <PythonCrawlExampleResponse />
  <NodeCrawlExampleResponse />
</CodeGroup>

2. **Start then check status** (`startCrawl`/`start_crawl`):
   - Returns immediately with a crawl ID
   - Allows manual status checking
   - Useful for long-running crawls or custom polling logic

<CodeGroup>
  <StartCrawlPython />
  <StartCrawlNode />
  <StartCrawlCURL />
  <StartCrawlCLI />
</CodeGroup>

## Crawl WebSocket

Firecrawl's WebSocket-based method, `Crawl URL and Watch`, enables real-time data extraction and monitoring. Start a crawl with a URL and customize it with options like page limits, allowed domains, and output formats, ideal for immediate data processing needs.

<CodeGroup>
  <CrawlWebSocketPython />
  <CrawlWebSocketNode />
</CodeGroup>

## Crawl Webhook

You can configure webhooks to receive real-time notifications as your crawl progresses. This allows you to process pages as they're scraped instead of waiting for the entire crawl to complete.

<CrawlWebhookCURL />

### Quick Reference

**Event Types:**

- `crawl.started` - When the crawl begins
- `crawl.page` - For each page successfully scraped
- `crawl.completed` - When the crawl finishes
- `crawl.failed` - If the crawl encounters an error

**Basic Payload:**

```json
{
  "success": true,
  "type": "crawl.page",
  "id": "crawl-job-id",
  "data": [...], // Page data for 'page' events
  "metadata": {}, // Your custom metadata
  "error": null
}
```

### Security: Verifying Webhook Signatures

Every webhook request from Firecrawl includes an `X-Firecrawl-Signature` header containing an HMAC-SHA256 signature. **Always verify this signature** to ensure the webhook is authentic and hasn't been tampered with.

**How it works:**

1. Get your webhook secret from the [Advanced tab](https://www.firecrawl.dev/app/settings?tab=advanced) of your account settings
2. Extract the signature from the `X-Firecrawl-Signature` header
3. Compute HMAC-SHA256 of the raw request body using your secret
4. Compare with the signature header using a timing-safe function

<Warning>
  Never process a webhook without verifying its signature first. The `X-Firecrawl-Signature` header contains the signature in the format: `sha256=abc123def456...`
</Warning>

For complete implementation examples in JavaScript and Python, see the [Webhook Security documentation](/webhooks/security).

### Full Documentation

For comprehensive webhook documentation including detailed event payloads, payload structure, advanced configuration, and troubleshooting, see the [Webhooks documentation](/webhooks/overview).
