---
title: "Agent"
description: "Gather data wherever it lives on the web."
og:title: "Agent | Firecrawl"
og:description: "Gather data wherever it lives on the web. Describe what you want, /agent handles the rest."
sidebarTitle: "Agent"
---

import AgentPython from "/snippets/v2/agent/base/python.mdx";
import AgentJS from "/snippets/v2/agent/base/js.mdx";
import AgentCURL from "/snippets/v2/agent/base/curl.mdx";
import AgentOutput from "/snippets/v2/agent/base/output.mdx";
import AgentWithSchemaPython from "/snippets/v2/agent/with-schema/python.mdx";
import AgentWithSchemaJS from "/snippets/v2/agent/with-schema/js.mdx";
import AgentWithSchemaCURL from "/snippets/v2/agent/with-schema/curl.mdx";
import AgentWithSchemaOutput from "/snippets/v2/agent/with-schema/output.mdx";
import AgentWithURLsPython from "/snippets/v2/agent/with-urls/python.mdx";
import AgentWithURLsJS from "/snippets/v2/agent/with-urls/js.mdx";
import AgentWithURLsCURL from "/snippets/v2/agent/with-urls/curl.mdx";
import AgentStatusPython from "/snippets/v2/agent/status/python.mdx";
import AgentStatusJS from "/snippets/v2/agent/status/js.mdx";
import AgentStatusCURL from "/snippets/v2/agent/status/curl.mdx";
import AgentStatusPending from "/snippets/v2/agent/status/pending.mdx";
import AgentStatusCompleted from "/snippets/v2/agent/status/completed.mdx";
import AgentWithModelPython from "/snippets/v2/agent/with-model/python.mdx";
import AgentWithModelJS from "/snippets/v2/agent/with-model/js.mdx";
import AgentWithModelCURL from "/snippets/v2/agent/with-model/curl.mdx";


Firecrawl `/agent` is a magic API that searches, navigates, and gathers data from the widest range of websites, finding data in hard-to-reach places and uncovering data in ways no other API can. It accomplishes in a few minutes what would take a human many hours — end-to-end data collection, without scripts or manual work.
Whether you need one data point or entire datasets at scale, Firecrawl `/agent` works to get your data.

**Think of `/agent` as deep research for data, wherever it is!**

<Info>
**Research Preview**: Agent is in early access. Expect rough edges. It will get significantly better over time. [Share feedback →](mailto:product@firecrawl.com)
</Info>

Agent builds on everything great about `/extract` and takes it further:

- **No URLs Required**: Just describe what you need via `prompt` parameter. URLs are optional
- **Deep Web Search**: Autonomously searches and navigates deep into sites to find your data
- **Reliable and Accurate**: Works with a wide variety of queries and use cases
- **Faster**: Processes multiple sources in parallel for quicker results

## Using `/agent`

The only required parameter is `prompt`. Simply describe what data you want to extract. For structured output, provide a JSON schema. The SDKs support Pydantic (Python) and Zod (Node) for type-safe schema definitions:

<CodeGroup>

<AgentWithSchemaPython />
<AgentWithSchemaJS />
<AgentWithSchemaCURL />

</CodeGroup>

### Response

<AgentWithSchemaOutput />

## Providing URLs (Optional)

You can optionally provide URLs to focus the agent on specific pages:

<CodeGroup>

<AgentWithURLsPython />
<AgentWithURLsJS />
<AgentWithURLsCURL />

</CodeGroup>



## Job Status and Completion

Agent jobs run asynchronously. When you submit a job, you'll receive a Job ID that you can use to check status:

- **Default method**: `agent()` waits and returns final results
- **Start then poll**: Use `start_agent` (Python) or `startAgent` (Node) to get a Job ID immediately, then poll with `get_agent_status` / `getAgentStatus`

<Note>Job results are available via the API for 24 hours after completion. After this period, you can still view your agent history and results in the [activity logs](https://www.firecrawl.dev/app/logs).</Note>

<CodeGroup>

<AgentStatusPython />
<AgentStatusJS />
<AgentStatusCURL />

</CodeGroup>

### Possible States

| Status | Description |
|--------|-------------|
| `processing` | The agent is still working on your request |
| `completed` | Extraction finished successfully |
| `failed` | An error occurred during extraction |

#### Pending Example

<AgentStatusPending />

#### Completed Example

<AgentStatusCompleted />

## Model Selection

Firecrawl Agent offers two models. **Spark 1 Mini is 60% cheaper** and is the default — perfect for most use cases. Upgrade to Spark 1 Pro when you need maximum accuracy on complex tasks.

| Model | Cost | Accuracy | Best For |
|-------|------|----------|----------|
| `spark-1-mini` | **60% cheaper** | Standard | Most tasks (default) |
| `spark-1-pro` | Standard | Higher | Complex research, critical extraction |

<Tip>
**Start with Spark 1 Mini** (default) — it handles most extraction tasks well at 60% lower cost. Switch to Pro only for complex multi-domain research or when accuracy is critical.
</Tip>

### Spark 1 Mini (Default)

`spark-1-mini` is our efficient model, ideal for straightforward data extraction tasks.

**Use Mini when:**
- Extracting simple data points (contact info, pricing, etc.)
- Working with well-structured websites
- Cost efficiency is a priority
- Running high-volume extraction jobs

### Spark 1 Pro

`spark-1-pro` is our flagship model, designed for maximum accuracy on complex extraction tasks.

**Use Pro when:**
- Performing complex competitive analysis
- Extracting data that requires deep reasoning
- Accuracy is critical for your use case
- Dealing with ambiguous or hard-to-find data

### Specifying a Model

Pass the `model` parameter to select which model to use:

<CodeGroup>

<AgentWithModelPython />
<AgentWithModelJS />
<AgentWithModelCURL />

</CodeGroup>

## Parameters

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `prompt` | string | **Yes** | Natural language description of the data you want to extract (max 10,000 characters) |
| `model` | string | No | Model to use: `spark-1-mini` (default) or `spark-1-pro` |
| `urls` | array | No | Optional list of URLs to focus the extraction |
| `schema` | object | No | Optional JSON schema for structured output |
| `maxCredits` | number | No | Maximum number of credits to spend on this agent task. If the limit is reached, the job fails and **no data is returned**, though credits consumed for work performed are still charged. |

## Citations

Citations provide source URLs for each extracted data point, allowing you to trace where the information came from.

### Playground

When using the [Agent Playground](https://www.firecrawl.dev/app/agent), citations are **enabled by default**. The playground automatically adds `_citation` fields to your schema before sending the request. You can toggle this behavior using the "Include Citations" option in the UI.

### API / SDK

When using the API or SDK directly, you must **manually add `_citation` fields** to your schema to receive source URLs. For each field you want a citation for, add a corresponding `{field}_citation` field:

```json
{
  "schema": {
    "type": "object",
    "properties": {
      "company_name": {
        "type": "string"
      },
      "company_name_citation": {
        "type": "string",
        "description": "Source URL for company_name"
      },
      "funding_amount": {
        "type": "number"
      },
      "funding_amount_citation": {
        "type": "string",
        "description": "Source URL for funding_amount"
      }
    }
  }
}
```

For arrays of primitive values, wrap items in objects with `value` and `value_citation`:

```json
{
  "schema": {
    "type": "object",
    "properties": {
      "features": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "value": { "type": "string" },
            "value_citation": { "type": "string", "description": "Source URL for this value" }
          }
        }
      }
    }
  }
}
```

<Tip>
If you're not receiving citation URLs in your API/SDK responses, make sure you've added the `_citation` fields to your schema.
</Tip>

## Agent vs Extract: What's Improved

| Feature | Agent (New) | Extract |
|---------|-------------|---------|
| URLs Required | No | Yes |
| Speed | Faster | Standard |
| Cost | Lower | Standard |
| Reliability | Higher | Standard |
| Query Flexibility | High | Moderate |

## Example Use Cases

- **Research**: "Find the top 5 AI startups and their funding amounts"
- **Competitive Analysis**: "Compare pricing plans between Slack and Microsoft Teams"
- **Data Gathering**: "Extract contact information from company websites"
- **Content Summarization**: "Summarize the latest blog posts about web scraping"

## CSV Upload in Agent Playground

The [Agent Playground](https://www.firecrawl.dev/app/agent) supports CSV upload for batch processing. Upload a CSV containing your input data (e.g., company names, URLs, or any entities), write a prompt describing what data you want the agent to find for each row, define your output fields, and run — the agent processes each row in parallel and fills in the results.

## API Reference

Check out the [Agent API Reference](/api-reference/endpoint/agent) for more details.

Have feedback or need help? Email [help@firecrawl.com](mailto:help@firecrawl.com).

## Pricing

Firecrawl Agent uses **dynamic billing** that scales with the complexity of your data extraction request. You pay based on the actual work Agent performs, ensuring fair pricing whether you're extracting simple data points or complex structured information from multiple sources.

### How Agent pricing works

Agent pricing is **dynamic and credit-based** during Research Preview:

* **Simple extractions** (like contact info from a single page) typically use fewer credits and cost less
* **Complex research tasks** (like competitive analysis across multiple domains) use more credits but reflect the total effort involved
* **Transparent usage** shows you exactly how many credits each request consumed
* **Credit conversion** automatically converts agent credit usage to credits for easy billing

<Info>
  Credit usage varies based on the complexity of your prompt, the amount of data processed, and the structure of the output requested. As a rough guide, most agent runs consume **a few hundred credits**, though simpler single-page tasks may use less and complex multi-domain research may use more.
</Info>

### Parallel Agents Pricing

If you are running multiple agents in parallel with Spark-1 Fast, pricing is a lot more predictable at 10 credits per cell.


### Getting started

**All users** receive **5 free daily runs** to explore Agent's capabilities without any cost.

Additional usage is billed based on credit consumption and converted to credits.


### Managing costs

Agent can be expensive, but there are some ways to decrease the cost:

* **Start with free runs**: Use your 5 daily free requests to understand pricing
* **Set a `maxCredits` parameter**: Limit your spending by setting a maximum number of credits you're willing to spend
* **Optimize prompts**: More specific prompts often use fewer credits
* **Monitor usage**: Track your consumption through the dashboard
* **Set expectations**: Complex multi-domain research will use more credits than simple single-page extractions

Try Agent now at [firecrawl.dev/app/agent](https://www.firecrawl.dev/app/agent) to see how credit usage scales with your specific use cases.

<Note>
  Pricing is subject to change as we move from Research Preview to general availability. Current users will receive advance notice of any pricing updates.
</Note>
